{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBT Dynamic Loading Example\n",
    "\n",
    "This notebook demonstrates the new dynamic loading feature for dbt Python classes.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The dynamic loading feature allows you to:\n",
    "- Load and execute dbt SQL models at runtime without code generation\n",
    "- Hot-reload SQL changes without regenerating Python classes\n",
    "- Execute DAGs dynamically based on runtime manifest\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, ensure you have compiled your dbt project and generated the necessary artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Import dynamic runtime components\n",
    "from ingen_fab.packages.dbt.runtime.dynamic import (\n",
    "    DynamicModelLoader,\n",
    "    DynamicSQLExecutor,\n",
    "    DynamicDAGExecutor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DBT Dynamic Loading Example\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using DynamicModelLoader\n",
    "\n",
    "The `DynamicModelLoader` reads manifest and SQL files at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your dbt project path\n",
    "dbt_project_path = Path(\"./sample_project\")\n",
    "\n",
    "# Create a loader instance\n",
    "loader = DynamicModelLoader(dbt_project_path)\n",
    "\n",
    "# Get all nodes from the manifest\n",
    "all_nodes = loader.get_all_nodes()\n",
    "print(f\"Total nodes in manifest: {len(all_nodes)}\")\n",
    "print(f\"Sample nodes: {all_nodes[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get nodes by type\n",
    "models = loader.get_nodes_by_type(\"model\")\n",
    "tests = loader.get_nodes_by_type(\"test\")\n",
    "seeds = loader.get_nodes_by_type(\"seed\")\n",
    "\n",
    "print(f\"Models: {len(models)}\")\n",
    "print(f\"Tests: {len(tests)}\")\n",
    "print(f\"Seeds: {len(seeds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata for a specific node\n",
    "if models:\n",
    "    sample_node = models[0]\n",
    "    metadata = loader.get_node_metadata(sample_node)\n",
    "    \n",
    "    print(f\"Node: {sample_node}\")\n",
    "    print(f\"Resource Type: {metadata.get('resource_type')}\")\n",
    "    print(f\"Path: {metadata.get('path')}\")\n",
    "    print(f\"SQL Statements: {metadata.get('sql_count')}\")\n",
    "    print(f\"Dependencies: {metadata.get('dependencies', [])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using DynamicSQLExecutor\n",
    "\n",
    "The `DynamicSQLExecutor` executes SQL statements loaded dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an executor instance\n",
    "executor = DynamicSQLExecutor(spark, loader)\n",
    "\n",
    "# Validate a node before execution\n",
    "if models:\n",
    "    validation = executor.validate_node(models[0])\n",
    "    print(f\"Node: {models[0]}\")\n",
    "    print(f\"Valid: {validation['valid']}\")\n",
    "    print(f\"SQL Count: {validation.get('sql_count')}\")\n",
    "    print(f\"Dependencies: {validation.get('dependency_count')} nodes\")\n",
    "    \n",
    "    if validation['issues']:\n",
    "        print(f\"Issues: {validation['issues']}\")\n",
    "    if validation['warnings']:\n",
    "        print(f\"Warnings: {validation['warnings']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute a single node\n",
    "if models:\n",
    "    try:\n",
    "        result = executor.execute_node(models[0])\n",
    "        print(f\"Successfully executed: {models[0]}\")\n",
    "        \n",
    "        if result:\n",
    "            print(f\"Result schema: {result.schema}\")\n",
    "            print(f\"Row count: {result.count()}\")\n",
    "            result.show(5)\n",
    "    except Exception as e:\n",
    "        print(f\"Execution failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using DynamicDAGExecutor\n",
    "\n",
    "The `DynamicDAGExecutor` orchestrates execution based on dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DAG executor\n",
    "dag_executor = DynamicDAGExecutor(\n",
    "    spark=spark,\n",
    "    dbt_project_path=dbt_project_path,\n",
    "    max_workers=4\n",
    ")\n",
    "\n",
    "# Validate the DAG\n",
    "is_valid, cycles = dag_executor.validate_dag()\n",
    "print(f\"DAG is valid: {is_valid}\")\n",
    "if cycles:\n",
    "    print(f\"Cycles found: {cycles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get execution plan\n",
    "plan = dag_executor.get_execution_plan()\n",
    "print(f\"Execution plan has {len(plan)} stages\")\n",
    "\n",
    "for i, stage in enumerate(plan[:3], 1):  # Show first 3 stages\n",
    "    print(f\"\\nStage {i}: {len(stage)} nodes can run in parallel\")\n",
    "    for node in stage[:5]:  # Show first 5 nodes\n",
    "        print(f\"  - {node}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute specific resource types\n",
    "results = dag_executor.execute_dag(\n",
    "    resource_types=[\"seed\"],  # Only execute seeds\n",
    "    fail_fast=True\n",
    ")\n",
    "\n",
    "print(f\"\\nExecution Summary:\")\n",
    "print(f\"Executed: {len(results['executed'])} nodes\")\n",
    "print(f\"Failed: {len(results['failed'])} nodes\")\n",
    "print(f\"Skipped: {len(results['skipped'])} nodes\")\n",
    "print(f\"Success Rate: {results['success_rate']:.1%}\")\n",
    "print(f\"Total Time: {results['total_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hot Reload Example\n",
    "\n",
    "Demonstrate how to reload SQL changes without regenerating classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cache to reload from disk\n",
    "loader.clear_cache()\n",
    "print(\"Cache cleared - will reload from disk on next access\")\n",
    "\n",
    "# Reload manifest to pick up any changes\n",
    "loader.reload_manifest()\n",
    "print(\"Manifest reloaded\")\n",
    "\n",
    "# DAG executor can also reload\n",
    "dag_executor.reload_manifest()\n",
    "print(\"DAG executor reloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison: Static vs Dynamic Mode\n",
    "\n",
    "### Generate Classes in Both Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Generate static classes (default)\n",
    "export FABRIC_WORKSPACE_REPO_DIR=\"./sample_project\"\n",
    "export FABRIC_ENVIRONMENT=\"development\"\n",
    "\n",
    "ingen_fab dbt create-python-classes sample_project --execution-mode static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Generate dynamic wrappers\n",
    "export FABRIC_WORKSPACE_REPO_DIR=\"./sample_project\"\n",
    "export FABRIC_ENVIRONMENT=\"development\"\n",
    "\n",
    "ingen_fab dbt create-python-classes sample_project --execution-mode dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Generated Dynamic Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a generated dynamic wrapper class\n",
    "# (This assumes you've generated classes for your project)\n",
    "# from ingen_fab.packages.dbt.runtime.projects.sample_project.models import YourModel\n",
    "\n",
    "# model = YourModel(spark)\n",
    "# result = model.execute()\n",
    "# model.reload()  # Hot reload SQL changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get execution history\n",
    "history = executor.get_execution_history()\n",
    "\n",
    "if history:\n",
    "    print(\"Execution History:\")\n",
    "    for entry in history[-5:]:  # Last 5 executions\n",
    "        print(f\"\\nNode: {entry['node_id']}\")\n",
    "        print(f\"Status: {entry['status']}\")\n",
    "        print(f\"Time: {entry['execution_time']:.2f}s\")\n",
    "        print(f\"Statements: {entry.get('statement_count', 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DAG execution status summary\n",
    "status_summary = dag_executor.get_status_summary()\n",
    "print(\"Execution Status Summary:\")\n",
    "for status, count in status_summary.items():\n",
    "    print(f\"  {status}: {count} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Practices\n",
    "\n",
    "### When to Use Dynamic Mode\n",
    "\n",
    "**Use Dynamic Mode when:**\n",
    "- Developing and iterating on SQL models\n",
    "- Need hot-reload capability\n",
    "- Want smaller generated code footprint\n",
    "- Working with frequently changing SQL\n",
    "\n",
    "**Use Static Mode when:**\n",
    "- Deploying to production\n",
    "- Need maximum performance\n",
    "- Want self-contained Python packages\n",
    "- Working in environments without access to dbt artifacts\n",
    "\n",
    "### Tips\n",
    "\n",
    "1. **Cache Management**: Use `cache_sql=True` for better performance in production\n",
    "2. **Parallel Execution**: Adjust `max_workers` based on your Spark cluster size\n",
    "3. **Error Handling**: Use `fail_fast=False` to continue execution after failures\n",
    "4. **Monitoring**: Check execution history regularly for performance insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}