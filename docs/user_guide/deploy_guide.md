# Deploy Guide

[Home](../index.md) > [User Guide](index.md) > Deploy Guide

Practical guide to deploy changes, upload Python libraries with variable injection, and extract metadata from Fabric assets.

## Summary

| Action | Command | Common flags |
|--------|---------|--------------|
| Deploy items | `ingen_fab deploy deploy` | Uses `FABRIC_WORKSPACE_REPO_DIR`, `FABRIC_ENVIRONMENT` |
| Upload `python_libs` | `ingen_fab deploy upload-python-libs` | Injects variables during upload |
| Upload dbt project to config lakehouse| `ingen_fab deploy upload-dbt-project` | `--dbt-project` (required)|
| Extract metadata | `ingen_fab deploy get-metadata` | `--target`, `--schema`, `--table`, `--format`, `--output` |
| Compare metadata | `ingen_fab deploy compare-metadata` | `--file1`, `--file2`, `--format`, `--output` |
| Download artefact | `ingen_fab deploy download-artefact` | `--artifact-type`, `--output-dir`, `--workspace-id`, `--include-content`, `--overwrite` |
| Delete all items | `ingen_fab deploy delete-all` | `--force` |

## Prerequisites

--8<-- "_includes/environment_setup.md"

## Deploy artifacts

Deploy all items under `fabric_workspace_items` to the selected environment.

```bash
ingen_fab deploy deploy
```

Tips:
- Use semantic, ordered DDL under `ddl_scripts` and generate notebooks with `ingen_fab ddl compile ...` before deploying.
- Validate your variable library value set for the target environment.

## Upload Python libraries to OneLake

Upload `python_libs` to the config lakehouse with variable injection during upload.

```bash
ingen_fab deploy upload-python-libs
```

Notes:
- Code between injection markers is resolved using the active value set.
- Upload progress and failures are shown in the console.

## Upload dbt project to OneLake/Config Lakehouse

Upload dbt project to the /Files section of the Config Lakehouse. Useful for Fabric Warehouse based projects to mount to Fabric Python notebooks

```bash
ingen_fab deploy upload-dbt-project --dbt-project <project_name>
```

## Extract metadata (Lakehouse/Warehouse)

Discover schemas, tables, and columns via Fabric SQL endpoints.

```bash
# Lakehouse example (write CSV)
ingen_fab deploy get-metadata \
  --workspace-name "Analytics Workspace" \
  --lakehouse-name "Config Lakehouse" \
  --schema config --table meta \
  --format csv --output ./artifacts/lakehouse_metadata.csv

# Warehouse example (pretty table)
ingen_fab deploy get-metadata \
  --workspace-name "Analytics Workspace" \
  --warehouse-name "EDW" \
  --target warehouse --format table

# Both lakehouse and warehouse (filter by schema)
ingen_fab deploy get-metadata --workspace-name "Analytics Workspace" --schema sales --target both
```

Common flags:
- `--target / -tgt`: `lakehouse` (default), `warehouse`, `both`
- `--schema / -s`, `--table / -t`: filters
- `--format / -f`: `csv`, `json`, `table`
- `--output / -o`: write to file

## Compare metadata files

Analyze differences between two metadata CSV files generated by `get-metadata`. Perfect for schema change tracking, environment comparisons, and migration validation.

```bash
# Basic comparison with table output
ingen_fab deploy compare-metadata \
  --file1 production_metadata.csv \
  --file2 staging_metadata.csv

# Generate JSON report for automated processing
ingen_fab deploy compare-metadata \
  -f1 before_migration.csv \
  -f2 after_migration.csv \
  -o migration_differences.json --format json

# CSV output for Excel analysis
ingen_fab deploy compare-metadata \
  --file1 lakehouse_v1.csv \
  --file2 lakehouse_v2.csv \
  --format csv --output schema_changes.csv
```

**What it detects:**
- **Missing tables**: Tables added or removed between versions
- **Missing columns**: Columns added or removed from existing tables
- **Data type changes**: Column data type modifications (e.g., `varchar(50)` → `varchar(100)`)
- **Nullable changes**: Column nullability modifications (`NULL` → `NOT NULL`)

**Output features:**
- **Logical ordering**: Results sorted by asset → schema → table → column
- **Multiple formats**: Human-readable tables, structured JSON, or CSV for analysis
- **Rich context**: Each difference includes detailed descriptions and location information
- **Summary statistics**: Count of differences by type for quick assessment

**Common use cases:**
```bash
# Track schema evolution over time
ingen_fab deploy get-metadata --target lakehouse -o schema_v1.csv
# ... make changes ...
ingen_fab deploy get-metadata --target lakehouse -o schema_v2.csv
ingen_fab deploy compare-metadata -f1 schema_v1.csv -f2 schema_v2.csv

# Compare environments before migration
ingen_fab deploy compare-metadata -f1 prod_schema.csv -f2 test_schema.csv

# Validate post-deployment changes
ingen_fab deploy compare-metadata \
  --file1 pre_deploy.csv --file2 post_deploy.csv \
  -o deployment_validation.json --format json
```

## Download artifacts from workspace

Download artifacts from the Fabric workspace to local directory for backup, analysis, or version control.

```bash

# Download an artefact from the existing environment's workspace
ingen_fab deploy download-artefact --artefact-name "rp_test" --artefact-type Report

```

Common flags:
- `--artifact-type / -t`: `notebook`, `lakehouse`, `warehouse`, `all`
- `--output-dir / -o`: Local directory destination (default: `./downloaded_artifacts`)
- `--workspace-id`: Specific workspace ID (uses environment config if not specified)
- `--include-content`: Include notebook content files (default: false)
- `--overwrite`: Overwrite existing files (default: false)

**Use cases:**
- **Backup**: Create local backups of Fabric workspace artifacts
- **Version Control**: Download artifacts for Git repository storage
- **Migration**: Export artifacts from one workspace for import to another
- **Analysis**: Download notebooks for local code review and analysis

## Clean up

Delete all workspace items in an environment (use with care):

```bash
ingen_fab deploy delete-all --force
```

!!! warning
    `deploy delete-all` removes all items in the resolved workspace for the active environment. Use the `--force` flag only when you are certain.

## Troubleshooting

- Ensure `FABRIC_WORKSPACE_REPO_DIR` points to your workspace repo root.
- `FABRIC_ENVIRONMENT` must map to a value set in your variable library.
- Validate Azure authentication and permissions for the target workspace.
