{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f54f3fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NotebookUtils not available, assumed running in local mode.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "# Check if running in Fabric environment\n",
    "if \"notebookutils\" in sys.modules:\n",
    "    import sys\n",
    "    \n",
    "    notebookutils.fs.mount(\"abfss://{{varlib:config_workspace_name}}@onelake.dfs.fabric.microsoft.com/{{varlib:config_lakehouse_name}}.Lakehouse/Files/\", \"/config_files\")  # type: ignore # noqa: F821\n",
    "    mount_path = notebookutils.fs.getMountPath(\"/config_files\")  # type: ignore # noqa: F821\n",
    "    \n",
    "    run_mode = \"fabric\"\n",
    "    sys.path.insert(0, mount_path)\n",
    "\n",
    "    \n",
    "    # PySpark environment - spark session should be available\n",
    "    \n",
    "else:\n",
    "    print(\"NotebookUtils not available, assumed running in local mode.\")\n",
    "    from ingen_fab.python_libs.pyspark.notebook_utils_abstraction import (\n",
    "        NotebookUtilsFactory,\n",
    "    )\n",
    "    notebookutils = NotebookUtilsFactory.create_instance()\n",
    "        \n",
    "    spark = None\n",
    "    \n",
    "    mount_path = None\n",
    "    run_mode = \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d662654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting...ingen_fab.python_libs\n",
      "deleting...ingen_fab.python_libs.interfaces\n",
      "deleting...ingen_fab.python_libs.interfaces.ddl_utils_interface\n",
      "deleting...ingen_fab.python_libs.common\n",
      "deleting...ingen_fab.python_libs.common.config_utils\n",
      "deleting...ingen_fab.python_libs.interfaces.data_store_interface\n",
      "deleting...ingen_fab.python_libs.pyspark.lakehouse_utils\n",
      "deleting...ingen_fab.python_libs.pyspark.ddl_utils\n",
      "deleting...ingen_fab.python_libs.interfaces.notebook_utils_interface\n",
      "deleting...ingen_fab.python_libs.common.utils\n",
      "deleting...ingen_fab.python_libs.common.utils.path_utils\n",
      "deleting...ingen_fab.python_libs.common.notebook_utils_base\n",
      "deleting...ingen_fab.python_libs.pyspark.notebook_utils_abstraction\n",
      "deleting...ingen_fab.python_libs.pyspark.parquet_load_utils\n",
      "deleting...ingen_fab.python_libs.pyspark\n",
      "deleting...ingen_fab\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "\n",
    "def load_python_modules_from_path(base_path: str, relative_files: list[str], max_chars: int = 1_000_000_000):\n",
    "    \"\"\"\n",
    "    Executes Python files from a Fabric-mounted file path using notebookutils.fs.head.\n",
    "    \n",
    "    Args:\n",
    "        base_path (str): The root directory where modules are located.\n",
    "        relative_files (list[str]): List of relative paths to Python files (from base_path).\n",
    "        max_chars (int): Max characters to read from each file (default: 1,000,000).\n",
    "    \"\"\"\n",
    "    success_files = []\n",
    "    failed_files = []\n",
    "\n",
    "    for relative_path in relative_files:\n",
    "        full_path = f\"file:{base_path}/{relative_path}\"\n",
    "        try:\n",
    "            print(f\"üîÑ Loading: {full_path}\")\n",
    "            code = notebookutils.fs.head(full_path, max_chars)\n",
    "            exec(code, globals())  # Use globals() to share context across modules\n",
    "            success_files.append(relative_path)\n",
    "        except Exception as e:\n",
    "            failed_files.append(relative_path)\n",
    "            print(f\"‚ùå Error loading {relative_path}\")\n",
    "\n",
    "    print(\"\\n‚úÖ Successfully loaded:\")\n",
    "    for f in success_files:\n",
    "        print(f\" - {f}\")\n",
    "\n",
    "    if failed_files:\n",
    "        print(\"\\n‚ö†Ô∏è Failed to load:\")\n",
    "        for f in failed_files:\n",
    "            print(f\" - {f}\")\n",
    "\n",
    "def clear_module_cache(prefix: str):\n",
    "    \"\"\"Clear module cache for specified prefix\"\"\"\n",
    "    for mod in list(sys.modules):\n",
    "        if mod.startswith(prefix):\n",
    "            print(\"deleting...\" + mod)\n",
    "            del sys.modules[mod]\n",
    "\n",
    "# Always clear the module cache - We may remove this once the libs are stable\n",
    "clear_module_cache(\"ingen_fab.python_libs\")\n",
    "clear_module_cache(\"ingen_fab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9f94d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if run_mode == \"local\":\n",
    "    from ingen_fab.python_libs.common.config_utils import get_configs_as_object\n",
    "    from ingen_fab.python_libs.pyspark.ddl_utils import ddl_utils\n",
    "    from ingen_fab.python_libs.pyspark.lakehouse_utils import lakehouse_utils\n",
    "    from ingen_fab.python_libs.pyspark.notebook_utils_abstraction import (\n",
    "        NotebookUtilsFactory,\n",
    "    )\n",
    "    notebookutils = NotebookUtilsFactory.create_instance() \n",
    "else:\n",
    "    files_to_load = [\n",
    "        \"ingen_fab/python_libs/common/config_utils.py\",\n",
    "        \"ingen_fab/python_libs/pyspark/lakehouse_utils.py\",\n",
    "        \"ingen_fab/python_libs/pyspark/ddl_utils.py\",\n",
    "        \"ingen_fab/python_libs/pyspark/notebook_utils_abstraction.py\"\n",
    "    ]\n",
    "\n",
    "    load_python_modules_from_path(mount_path, files_to_load)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "223d1dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No active Spark session found, creating a new one with Delta support.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /root/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /root/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5d0c614f-bf8f-4b35-a56f-a34879de76b8;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      ":: resolution report :: resolve 66ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5d0c614f-bf8f-4b35-a56f-a34879de76b8\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n",
      "25/08/05 20:18:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "target_lakehouse = lakehouse_utils(\n",
    "    target_workspace_id=get_configs_as_object().config_workspace_id,\n",
    "    target_lakehouse_id=get_configs_as_object().config_lakehouse_id    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e4b5c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing Spark session, reusing it.\n",
      "Path file:////workspaces/ingen_fab/tmp/spark/Tables/ddl_script_executions is not a Delta table.\n",
      "Creating execution log table at file:////workspaces/ingen_fab/tmp/spark/Tables/ddl_script_executions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö† Alert: Registering table 'ddl_script_executions' in the Hive catalog for local Spark.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/14 13:47:13 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`default`.`ddl_script_executions` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "25/07/14 13:47:13 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/07/14 13:47:13 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "du = ddl_utils(\n",
    "    target_workspace_id=get_configs_as_object().config_workspace_id,\n",
    "    target_lakehouse_id=get_configs_as_object().config_lakehouse_id    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98163c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "53\n",
      "‚ö† Alert: Registering table 'config_flat_file_ingestion' in the Hive catalog for local Spark.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 09:58:05 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`default`.`config_flat_file_ingestion` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    }
   ],
   "source": [
    "# Sample configuration data for flat file ingestion testing - Universal schema (Lakehouse version)\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Import the universal schema definition\n",
    "from pyspark.sql.types import (\n",
    "    BooleanType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"config_id\", StringType(), nullable=False),\n",
    "    StructField(\"config_name\", StringType(), nullable=False),\n",
    "    StructField(\"source_file_path\", StringType(), nullable=False),\n",
    "    StructField(\"source_file_format\", StringType(), nullable=False),  # csv, json, parquet, avro, xml\n",
    "    # Source location fields (optional - defaults to target or raw workspace)\n",
    "    StructField(\"source_workspace_id\", StringType(), nullable=True),  # Source workspace (defaults to target if null)\n",
    "    StructField(\"source_datastore_id\", StringType(), nullable=True),  # Source lakehouse/warehouse (defaults to raw if null)\n",
    "    StructField(\"source_datastore_type\", StringType(), nullable=True),  # 'lakehouse' or 'warehouse' (defaults to lakehouse)\n",
    "    StructField(\"source_file_root_path\", StringType(), nullable=True),  # Root path override (e.g., \"Files\", \"Tables\")\n",
    "    # Target location fields\n",
    "    StructField(\"target_workspace_id\", StringType(), nullable=False),  # Universal field for workspace\n",
    "    StructField(\"target_datastore_id\", StringType(), nullable=False),  # Universal field for lakehouse/warehouse\n",
    "    StructField(\"target_datastore_type\", StringType(), nullable=False),  # 'lakehouse' or 'warehouse'\n",
    "    StructField(\"target_schema_name\", StringType(), nullable=False),\n",
    "    StructField(\"target_table_name\", StringType(), nullable=False),\n",
    "    StructField(\"staging_table_name\", StringType(), nullable=True),  # For warehouse COPY INTO staging\n",
    "    StructField(\"file_delimiter\", StringType(), nullable=True),  # for CSV files\n",
    "    StructField(\"has_header\", BooleanType(), nullable=True),  # for CSV files\n",
    "    StructField(\"encoding\", StringType(), nullable=True),  # utf-8, latin-1, etc.\n",
    "    StructField(\"date_format\", StringType(), nullable=True),  # for date columns\n",
    "    StructField(\"timestamp_format\", StringType(), nullable=True),  # for timestamp columns\n",
    "    StructField(\"schema_inference\", BooleanType(), nullable=False),  # whether to infer schema\n",
    "    StructField(\"custom_schema_json\", StringType(), nullable=True),  # custom schema definition\n",
    "    StructField(\"partition_columns\", StringType(), nullable=True),  # comma-separated list\n",
    "    StructField(\"sort_columns\", StringType(), nullable=True),  # comma-separated list\n",
    "    StructField(\"write_mode\", StringType(), nullable=False),  # overwrite, append, merge\n",
    "    StructField(\"merge_keys\", StringType(), nullable=True),  # for merge operations\n",
    "    StructField(\"data_validation_rules\", StringType(), nullable=True),  # JSON validation rules\n",
    "    StructField(\"error_handling_strategy\", StringType(), nullable=False),  # fail, skip, log\n",
    "    StructField(\"execution_group\", IntegerType(), nullable=False),\n",
    "    StructField(\"active_yn\", StringType(), nullable=False),\n",
    "    StructField(\"created_date\", StringType(), nullable=False),\n",
    "    StructField(\"modified_date\", StringType(), nullable=True),\n",
    "    StructField(\"created_by\", StringType(), nullable=False),\n",
    "    StructField(\"modified_by\", StringType(), nullable=True),\n",
    "    # Advanced CSV configuration fields\n",
    "    StructField(\"quote_character\", StringType(), nullable=True),  # Default: '\"'\n",
    "    StructField(\"escape_character\", StringType(), nullable=True),  # Default: '\"' (Excel style)\n",
    "    StructField(\"multiline_values\", BooleanType(), nullable=True),  # Default: True\n",
    "    StructField(\"ignore_leading_whitespace\", BooleanType(), nullable=True),  # Default: False\n",
    "    StructField(\"ignore_trailing_whitespace\", BooleanType(), nullable=True),  # Default: False\n",
    "    StructField(\"null_value\", StringType(), nullable=True),  # Default: \"\"\n",
    "    StructField(\"empty_value\", StringType(), nullable=True),  # Default: \"\"\n",
    "    StructField(\"comment_character\", StringType(), nullable=True),  # Default: None\n",
    "    StructField(\"max_columns\", IntegerType(), nullable=True),  # Default: 100\n",
    "    StructField(\"max_chars_per_column\", IntegerType(), nullable=True),  # Default: 50000\n",
    "    # New fields for incremental synthetic data import support\n",
    "    StructField(\"import_pattern\", StringType(), nullable=True),  # 'single_file', 'date_partitioned', 'wildcard_pattern'\n",
    "    StructField(\"date_partition_format\", StringType(), nullable=True),  # Date partition format (e.g., 'YYYY/MM/DD')\n",
    "    StructField(\"table_relationship_group\", StringType(), nullable=True),  # Group for related table imports\n",
    "    StructField(\"batch_import_enabled\", BooleanType(), nullable=True),  # Enable batch processing\n",
    "    StructField(\"file_discovery_pattern\", StringType(), nullable=True),  # Pattern for automatic file discovery\n",
    "    StructField(\"import_sequence_order\", IntegerType(), nullable=True),  # Order for related table imports\n",
    "    StructField(\"date_range_start\", StringType(), nullable=True),  # Start date for batch import\n",
    "    StructField(\"date_range_end\", StringType(), nullable=True),  # End date for batch import\n",
    "    StructField(\"skip_existing_dates\", BooleanType(), nullable=True),  # Skip already imported dates\n",
    "    StructField(\"source_is_folder\", BooleanType(), nullable=True)  # True for folder with part files, False for single file\n",
    "])\n",
    "\n",
    "# Sample configuration records for testing - Using synthetic data generator files\n",
    "sample_configs = [\n",
    "    Row(\n",
    "        config_id=\"synthetic_customers_folder_001\",\n",
    "        config_name=\"Synthetic Data - Customers Folder (Retail OLTP Small)\",\n",
    "        source_file_path=\"synthetic_data/csv/series/retail_oltp_small/test_data_feb_small/flat/snapshot_customers/snapshot_customers_20240201.csv\",\n",
    "        source_file_format=\"csv\",\n",
    "        source_workspace_id=\"{{varlib:sample_lh_workspace_id}}\",\n",
    "        source_datastore_id=\"{{varlib:sample_lh_lakehouse_id}}\",\n",
    "        source_datastore_type=\"lakehouse\",\n",
    "        source_file_root_path=None,  # No root path override needed\n",
    "        target_workspace_id=\"{{varlib:sample_lh_workspace_id}}\",\n",
    "        target_datastore_id=\"{{varlib:sample_lh_lakehouse_id}}\",\n",
    "        target_datastore_type=\"lakehouse\",\n",
    "        target_schema_name=\"raw\",\n",
    "        target_table_name=\"synthetic_customers\",\n",
    "        staging_table_name=None,\n",
    "        file_delimiter=\",\",\n",
    "        has_header=True,\n",
    "        encoding=\"utf-8\",\n",
    "        date_format=\"yyyy-MM-dd\",\n",
    "        timestamp_format=\"yyyy-MM-dd HH:mm:ss\",\n",
    "        schema_inference=True,\n",
    "        custom_schema_json=None,\n",
    "        partition_columns=\"\",\n",
    "        sort_columns=\"customer_id\",\n",
    "        write_mode=\"overwrite\",\n",
    "        merge_keys=\"\",\n",
    "        data_validation_rules=None,\n",
    "        error_handling_strategy=\"log\",\n",
    "        execution_group=1,  # Folder-based snapshot processing (Group 1)\n",
    "        active_yn=\"Y\",\n",
    "        created_date=\"2024-01-15\",\n",
    "        modified_date=None,\n",
    "        created_by=\"system\",\n",
    "        modified_by=None,\n",
    "        quote_character='\"',\n",
    "        escape_character='\"',\n",
    "        multiline_values=True,\n",
    "        ignore_leading_whitespace=False,\n",
    "        ignore_trailing_whitespace=False,\n",
    "        null_value=\"\",\n",
    "        empty_value=\"\",\n",
    "        comment_character=None,\n",
    "        max_columns=100,\n",
    "        max_chars_per_column=50000,\n",
    "        # New fields for incremental synthetic data import support\n",
    "        import_pattern=\"single_file\",\n",
    "        date_partition_format=None,\n",
    "        table_relationship_group=\"retail_oltp_single\",\n",
    "        batch_import_enabled=False,\n",
    "        file_discovery_pattern=None,\n",
    "        import_sequence_order=1,\n",
    "        date_range_start=None,\n",
    "        date_range_end=None,\n",
    "        skip_existing_dates=None,\n",
    "        source_is_folder=True  # Read all part files from the folder\n",
    "    ),\n",
    "    Row(\n",
    "        config_id=\"synthetic_orders_folder_002\",\n",
    "        config_name=\"Synthetic Data - Orders Folder (Retail OLTP Small)\",\n",
    "        source_file_path=\"synthetic_data/parquet/series/retail_oltp_small/high_volume_summer_data/flat/orders/\",\n",
    "        source_file_format=\"parquet\",\n",
    "        source_workspace_id=\"{{varlib:sample_lh_workspace_id}}\",\n",
    "        source_datastore_id=\"{{varlib:sample_lh_lakehouse_id}}\",\n",
    "        source_datastore_type=\"lakehouse\",\n",
    "        source_file_root_path=None,  # No root path override needed\n",
    "        target_workspace_id=\"{{varlib:sample_lh_workspace_id}}\",\n",
    "        target_datastore_id=\"{{varlib:sample_lh_lakehouse_id}}\",\n",
    "        target_datastore_type=\"lakehouse\",\n",
    "        target_schema_name=\"raw\",\n",
    "        target_table_name=\"synthetic_orders\",\n",
    "        staging_table_name=None,\n",
    "        file_delimiter=\",\",\n",
    "        has_header=True,\n",
    "        encoding=\"utf-8\",\n",
    "        date_format=\"yyyyMMdd\",\n",
    "        timestamp_format=\"yyyy-MM-dd HH:mm:ss\",\n",
    "        schema_inference=True,\n",
    "        custom_schema_json=None,\n",
    "        partition_columns=\"\",\n",
    "        sort_columns=\"order_id\",\n",
    "        write_mode=\"overwrite\",\n",
    "        merge_keys=\"\",\n",
    "        data_validation_rules=None,\n",
    "        error_handling_strategy=\"log\",\n",
    "        execution_group=1,  \n",
    "        active_yn=\"Y\",\n",
    "        created_date=\"2024-01-15\",\n",
    "        modified_date=None,\n",
    "        created_by=\"system\",\n",
    "        modified_by=None,\n",
    "        quote_character='\"',\n",
    "        escape_character='\"',\n",
    "        multiline_values=True,\n",
    "        ignore_leading_whitespace=False,\n",
    "        ignore_trailing_whitespace=False,\n",
    "        null_value=\"\",\n",
    "        empty_value=\"\",\n",
    "        comment_character=None,\n",
    "        max_columns=100,\n",
    "        max_chars_per_column=50000,\n",
    "        # New fields for incremental synthetic data import support\n",
    "        import_pattern=\"date_partitioned\",\n",
    "        date_partition_format=\"yyyyMMdd\",\n",
    "        table_relationship_group=\"retail_oltp_single\",\n",
    "        batch_import_enabled=False,\n",
    "        file_discovery_pattern=\"orders_*.parquet\",\n",
    "        import_sequence_order=1,\n",
    "        date_range_start=None,\n",
    "        date_range_end=None,\n",
    "        skip_existing_dates=None,\n",
    "        source_is_folder=True  # Read all part files from the folder\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create DataFrame and insert records\n",
    "print(len(schema.fields))\n",
    "print(len(sample_configs[0].asDict()))\n",
    "df = target_lakehouse.get_connection.createDataFrame(sample_configs, schema)\n",
    "target_lakehouse.drop_table(\"config_flat_file_ingestion\")\n",
    "target_lakehouse.write_to_table(\n",
    "    df=df,\n",
    "    table_name=\"config_flat_file_ingestion\",\n",
    "    mode=\"overwrite\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0335314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+----------+--------------+-----------+-------------+---------------+------------+--------------+\n",
      "|order_id|customer_id|order_date|    status|payment_method|order_total|shipping_cost|discount_amount|shipped_date|delivered_date|\n",
      "+--------+-----------+----------+----------+--------------+-----------+-------------+---------------+------------+--------------+\n",
      "|  100001|          2|2023-12-23|   Pending|    Debit Card|      110.1|          0.1|            0.1|        NULL|          NULL|\n",
      "|  100002|          3|2023-12-24|   Pending|        PayPal|      110.2|          0.2|            0.2|        NULL|          NULL|\n",
      "|  100003|          4|2023-12-25|   Pending| Bank Transfer|      110.3|          0.3|            0.0|        NULL|          NULL|\n",
      "|  100004|          5|2023-12-26|   Pending|          Cash|      110.4|          0.4|            0.0|        NULL|          NULL|\n",
      "|  100005|          6|2023-12-27|   Pending|   Credit Card|      110.5|          0.5|            0.0|        NULL|          NULL|\n",
      "|  100006|          7|2023-12-28|   Pending|    Debit Card|      110.6|          0.6|            0.0|        NULL|          NULL|\n",
      "|  100007|          8|2023-12-29|   Pending|        PayPal|      110.7|          0.7|            0.0|        NULL|          NULL|\n",
      "|  100008|          9|2023-12-30|   Pending| Bank Transfer|      110.8|          0.8|            0.0|        NULL|          NULL|\n",
      "|  100009|         10|2023-12-31|   Pending|          Cash|      110.9|          0.9|            0.0|        NULL|          NULL|\n",
      "|  100010|         11|2022-01-01|   Pending|   Credit Card|      111.0|          1.0|            1.0|        NULL|          NULL|\n",
      "|  100011|         12|2022-01-02|   Pending|    Debit Card|      111.1|          1.1|            1.1|        NULL|          NULL|\n",
      "|  100012|         13|2022-01-03|   Pending|        PayPal|      111.2|          1.2|            1.2|        NULL|          NULL|\n",
      "|  100013|         14|2022-01-04|   Pending| Bank Transfer|      111.3|          1.3|            0.0|        NULL|          NULL|\n",
      "|  100014|         15|2022-01-05|   Pending|          Cash|      111.4|          1.4|            0.0|        NULL|          NULL|\n",
      "|  100015|         16|2022-01-06|Processing|   Credit Card|      111.5|          1.5|            0.0|        NULL|          NULL|\n",
      "|  100016|         17|2022-01-07|Processing|    Debit Card|      111.6|          1.6|            0.0|        NULL|          NULL|\n",
      "|  100017|         18|2022-01-08|Processing|        PayPal|      111.7|          1.7|            0.0|        NULL|          NULL|\n",
      "|  100018|         19|2022-01-09|Processing| Bank Transfer|      111.8|          1.8|            0.0|        NULL|          NULL|\n",
      "|  100019|         20|2022-01-10|Processing|          Cash|      111.9|          1.9|            0.0|        NULL|          NULL|\n",
      "|  100020|         21|2022-01-11|Processing|   Credit Card|      112.0|          2.0|            2.0|        NULL|          NULL|\n",
      "+--------+-----------+----------+----------+--------------+-----------+-------------+---------------+------------+--------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "target_lakehouse.execute_query(\"Select * from synthetic_orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37e6fa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0592eb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+----------+--------------+-----------+-------------+---------------+------------+--------------+\n",
      "|order_id|customer_id|order_date|    status|payment_method|order_total|shipping_cost|discount_amount|shipped_date|delivered_date|\n",
      "+--------+-----------+----------+----------+--------------+-----------+-------------+---------------+------------+--------------+\n",
      "| 1150001|          2|2022-09-09|   Pending|    Debit Card|      170.1|          0.1|            0.1|        NULL|          NULL|\n",
      "| 1150002|          3|2022-09-10|   Pending|        PayPal|      170.2|          0.2|            0.2|        NULL|          NULL|\n",
      "| 1150003|          4|2022-09-11|   Pending| Bank Transfer|      170.3|          0.3|            0.0|        NULL|          NULL|\n",
      "| 1150004|          5|2022-09-12|   Pending|          Cash|      170.4|          0.4|            0.0|        NULL|          NULL|\n",
      "| 1150005|          6|2022-09-13|   Pending|   Credit Card|      170.5|          0.5|            0.0|        NULL|          NULL|\n",
      "| 1150006|          7|2022-09-14|   Pending|    Debit Card|      170.6|          0.6|            0.0|        NULL|          NULL|\n",
      "| 1150007|          8|2022-09-15|   Pending|        PayPal|      170.7|          0.7|            0.0|        NULL|          NULL|\n",
      "| 1150008|          9|2022-09-16|   Pending| Bank Transfer|      170.8|          0.8|            0.0|        NULL|          NULL|\n",
      "| 1150009|         10|2022-09-17|   Pending|          Cash|      170.9|          0.9|            0.0|        NULL|          NULL|\n",
      "| 1150010|         11|2022-09-18|   Pending|   Credit Card|      171.0|          1.0|            1.0|        NULL|          NULL|\n",
      "| 1150011|         12|2022-09-19|   Pending|    Debit Card|      171.1|          1.1|            1.1|        NULL|          NULL|\n",
      "| 1150012|         13|2022-09-20|   Pending|        PayPal|      171.2|          1.2|            1.2|        NULL|          NULL|\n",
      "| 1150013|         14|2022-09-21|   Pending| Bank Transfer|      171.3|          1.3|            0.0|        NULL|          NULL|\n",
      "| 1150014|         15|2022-09-22|   Pending|          Cash|      171.4|          1.4|            0.0|        NULL|          NULL|\n",
      "| 1150015|         16|2022-09-23|Processing|   Credit Card|      171.5|          1.5|            0.0|        NULL|          NULL|\n",
      "| 1150016|         17|2022-09-24|Processing|    Debit Card|      171.6|          1.6|            0.0|        NULL|          NULL|\n",
      "| 1150017|         18|2022-09-25|Processing|        PayPal|      171.7|          1.7|            0.0|        NULL|          NULL|\n",
      "| 1150018|         19|2022-09-26|Processing| Bank Transfer|      171.8|          1.8|            0.0|        NULL|          NULL|\n",
      "| 1150019|         20|2022-09-27|Processing|          Cash|      171.9|          1.9|            0.0|        NULL|          NULL|\n",
      "| 1150020|         21|2022-09-28|Processing|   Credit Card|      172.0|          2.0|            2.0|        NULL|          NULL|\n",
      "+--------+-----------+----------+----------+--------------+-----------+-------------+---------------+------------+--------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "target_lakehouse.spark.read.format(\"parquet\").load(\"tmp/spark/Files/synthetic_data/parquet/series/retail_oltp_small/high_volume_summer_data/flat/orders/orders_20240601.parquet/part-00023-a53a5973-8248-405a-9982-e270544646be-c000.snappy.parquet\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e9631e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9afcadc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62359b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/18 08:49:12 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+---------------+---------------+---------------------------+------------+------------------------------+------------------+-----------------+------------+----------------+--------------+----------------+-----------+-----------------+--------------+---------------+-------------+------------+------+--------------+-----------------+\n",
      "|master_execution_id|execution_id|pipeline_job_id|execution_group|master_execution_parameters|trigger_type|config_synapse_connection_name|source_schema_name|source_table_name|extract_mode|extract_start_dt|extract_end_dt|partition_clause|output_path|extract_file_name|external_table|start_timestamp|end_timestamp|duration_sec|status|error_messages|end_timestamp_int|\n",
      "+-------------------+------------+---------------+---------------+---------------------------+------------+------------------------------+------------------+-----------------+------------+----------------+--------------+----------------+-----------+-----------------+--------------+---------------+-------------+------------+------+--------------+-----------------+\n",
      "+-------------------+------------+---------------+---------------+---------------------------+------------+------------------------------+------------------+-----------------+------------+----------------+--------------+----------------+-----------+-----------------+--------------+---------------+-------------+------------+------+--------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lu.execute_query(\"SELECT * from log_synapse_extract_run_log\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "971b15fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/31 08:23:47 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------------+---------------------+------------------+--------------------+-----------------+-------------------+------------+------------------+------------+----------+-------------+-----------+\n",
      "|        extract_name|is_active|trigger_name|extract_pipeline_name|extract_table_name|extract_table_schema|extract_view_name|extract_view_schema|is_full_load|   execution_group|created_date|created_by|modified_date|modified_by|\n",
      "+--------------------+---------+------------+---------------------+------------------+--------------------+-----------------+-------------------+------------+------------------+------------+----------+-------------+-----------+\n",
      "|SAMPLE_CUSTOMERS_...|     true|        NULL|                 NULL|         customers|             default|             NULL|               NULL|        true|LAKEHOUSE_EXTRACTS|  2024-01-15|    system|         NULL|       NULL|\n",
      "|SAMPLE_PRODUCTS_L...|     true|        NULL|                 NULL|          products|             default|             NULL|               NULL|        true|LAKEHOUSE_EXTRACTS|  2024-01-15|    system|         NULL|       NULL|\n",
      "|SAMPLE_ORDERS_LAK...|     true|        NULL|                 NULL|            orders|             default|             NULL|               NULL|        true|LAKEHOUSE_EXTRACTS|  2024-01-15|    system|         NULL|       NULL|\n",
      "+--------------------+---------+------------+---------------------+------------------+--------------------+-----------------+-------------------+------------+------------------+------------+----------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lu.execute_query(\"SELECT * from config_extract_generation\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb6f03c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/31 08:51:08 WARN DeltaLog: Change in the table id detected while updating snapshot. \n",
      "Previous snapshot = Snapshot(path=file:/workspaces/ingen_fab/tmp/spark/Tables/config_extract_generation_details/_delta_log, version=1, metadata=Metadata(82c2237b-0ce7-44e7-9f94-c5248df68ee6,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"extract_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_generation_group\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_container\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_directory\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_timestamp_format\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_period_end_day\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_ordering\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_column_delimiter\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_row_delimiter\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_encoding\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_quote_character\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_escape_character\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_header\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_null_value\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_max_rows_per_file\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"output_format\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"is_trigger_file\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"trigger_file_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"is_compressed\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_type\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_level\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_file_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"fabric_lakehouse_path\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"created_date\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"created_by\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"modified_date\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"modified_by\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1753950105913)), logSegment=LogSegment(file:/workspaces/ingen_fab/tmp/spark/Tables/config_extract_generation_details/_delta_log,1,ArraySeq(DeprecatedRawLocalFileStatus{path=file:/workspaces/ingen_fab/tmp/spark/Tables/config_extract_generation_details/_delta_log/00000000000000000000.json; isDirectory=false; length=3422; replication=1; blocksize=33554432; modification_time=1753950115809; access_time=1753950116336; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}, DeprecatedRawLocalFileStatus{path=file:/workspaces/ingen_fab/tmp/spark/Tables/config_extract_generation_details/_delta_log/00000000000000000001.json; isDirectory=false; length=7967; replication=1; blocksize=33554432; modification_time=1753950116919; access_time=1753950117120; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}),org.apache.spark.sql.delta.EmptyCheckpointProvider$@4f46dba,1753950116919), checksumOpt=Some(VersionChecksum(Some(fe581cc3-2dc9-43a6-beb9-b591a365db5f),27871,3,None,None,1,1,None,Some(List()),Some(List()),Metadata(82c2237b-0ce7-44e7-9f94-c5248df68ee6,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"extract_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_generation_group\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_container\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_directory\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_timestamp_format\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_period_end_day\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_ordering\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_column_delimiter\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_row_delimiter\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_encoding\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_quote_character\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_escape_character\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_header\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_null_value\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_max_rows_per_file\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"output_format\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"is_trigger_file\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"trigger_file_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"is_compressed\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_type\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_level\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_file_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"fabric_lakehouse_path\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"created_date\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"created_by\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"modified_date\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"modified_by\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1753950105913)),Protocol(1,2),None,None,Some(List(AddFile(part-00004-d5a5e933-6242-4ffc-a338-2e5823b5b26d-c000.snappy.parquet,Map(),9294,1753950116900,false,{\"numRecords\":1,\"minValues\":{\"extract_name\":\"SAMPLE_PRODUCTS_LAKEHOUSE\",\"file_generation_group\":\"PRODUCT_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"products\",\"extract_file_name\":\"products_catalog\",\"extract_file_name_timestamp_format\":\"yyyyMMdd\",\"extract_file_name_extension\":\"parquet\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"parquet\",\"trigger_file_extension\":\".done\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"maxValues\":{\"extract_name\":\"SAMPLE_PRODUCTS_LAKEHOUSE\",\"file_generation_group\":\"PRODUCT_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"products\",\"extract_file_name\":\"products_catalog\",\"extract_file_name_timestamp_format\":\"yyyyMMdd\",\"extract_file_name_extension\":\"parquet\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"parquet\",\"trigger_file_extension\":\".done\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"nullCount\":{\"extract_name\":0,\"file_generation_group\":0,\"extract_container\":0,\"extract_directory\":0,\"extract_file_name\":0,\"extract_file_name_timestamp_format\":0,\"extract_file_name_period_end_day\":1,\"extract_file_name_extension\":0,\"extract_file_name_ordering\":0,\"file_properties_column_delimiter\":0,\"file_properties_row_delimiter\":0,\"file_properties_encoding\":0,\"file_properties_quote_character\":0,\"file_properties_escape_character\":0,\"file_properties_header\":0,\"file_properties_null_value\":0,\"file_properties_max_rows_per_file\":1,\"output_format\":0,\"is_trigger_file\":0,\"trigger_file_extension\":0,\"is_compressed\":0,\"compressed_type\":1,\"compressed_level\":1,\"compressed_file_name\":1,\"compressed_extension\":1,\"fabric_lakehouse_path\":1,\"created_date\":0,\"created_by\":0,\"modified_date\":1,\"modified_by\":1}},null,null,None,None,None), AddFile(part-00002-d4ae1eda-a5bc-489b-a25a-17da7c72aa6e-c000.snappy.parquet,Map(),9282,1753950116892,false,{\"numRecords\":1,\"minValues\":{\"extract_name\":\"SAMPLE_CUSTOMERS_LAKEHOUSE\",\"file_generation_group\":\"CUSTOMER_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"customers\",\"extract_file_name\":\"customers_lakehouse\",\"extract_file_name_timestamp_format\":\"yyyyMMdd_HHmmss\",\"extract_file_name_extension\":\"csv\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"csv\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"maxValues\":{\"extract_name\":\"SAMPLE_CUSTOMERS_LAKEHOUSE\",\"file_generation_group\":\"CUSTOMER_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"customers\",\"extract_file_name\":\"customers_lakehouse\",\"extract_file_name_timestamp_format\":\"yyyyMMdd_HHmmss\",\"extract_file_name_extension\":\"csv\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"csv\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"nullCount\":{\"extract_name\":0,\"file_generation_group\":0,\"extract_container\":0,\"extract_directory\":0,\"extract_file_name\":0,\"extract_file_name_timestamp_format\":0,\"extract_file_name_period_end_day\":1,\"extract_file_name_extension\":0,\"extract_file_name_ordering\":0,\"file_properties_column_delimiter\":0,\"file_properties_row_delimiter\":0,\"file_properties_encoding\":0,\"file_properties_quote_character\":0,\"file_properties_escape_character\":0,\"file_properties_header\":0,\"file_properties_null_value\":0,\"file_properties_max_rows_per_file\":1,\"output_format\":0,\"is_trigger_file\":0,\"trigger_file_extension\":1,\"is_compressed\":0,\"compressed_type\":1,\"compressed_level\":1,\"compressed_file_name\":1,\"compressed_extension\":1,\"fabric_lakehouse_path\":1,\"created_date\":0,\"created_by\":0,\"modified_date\":1,\"modified_by\":1}},null,null,None,None,None), AddFile(part-00006-35ee8dda-5590-414a-a49d-1c2e027e9caa-c000.snappy.parquet,Map(),9295,1753950116908,false,{\"numRecords\":1,\"minValues\":{\"extract_name\":\"SAMPLE_ORDERS_LAKEHOUSE\",\"file_generation_group\":\"ORDER_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"orders\",\"extract_file_name\":\"orders_daily\",\"extract_file_name_timestamp_format\":\"yyyyMMdd_HHmmss\",\"extract_file_name_extension\":\"csv\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"csv\",\"compressed_type\":\"GZIP\",\"compressed_level\":\"NORMAL\",\"compressed_extension\":\".gz\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"maxValues\":{\"extract_name\":\"SAMPLE_ORDERS_LAKEHOUSE\",\"file_generation_group\":\"ORDER_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"orders\",\"extract_file_name\":\"orders_daily\",\"extract_file_name_timestamp_format\":\"yyyyMMdd_HHmmss\",\"extract_file_name_extension\":\"csv\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"csv\",\"compressed_type\":\"GZIP\",\"compressed_level\":\"NORMAL\",\"compressed_extension\":\".gz\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"nullCount\":{\"extract_name\":0,\"file_generation_group\":0,\"extract_container\":0,\"extract_directory\":0,\"extract_file_name\":0,\"extract_file_name_timestamp_format\":0,\"extract_file_name_period_end_day\":1,\"extract_file_name_extension\":0,\"extract_file_name_ordering\":0,\"file_properties_column_delimiter\":0,\"file_properties_row_delimiter\":0,\"file_properties_encoding\":0,\"file_properties_quote_character\":0,\"file_properties_escape_character\":0,\"file_properties_header\":0,\"file_properties_null_value\":0,\"file_properties_max_rows_per_file\":1,\"output_format\":0,\"is_trigger_file\":0,\"trigger_file_extension\":1,\"is_compressed\":0,\"compressed_type\":0,\"compressed_level\":0,\"compressed_file_name\":1,\"compressed_extension\":0,\"fabric_lakehouse_path\":1,\"created_date\":0,\"created_by\":0,\"modified_date\":1,\"modified_by\":1}},null,null,None,None,None))))))\n",
      "New snapshot = Snapshot(path=file:/workspaces/ingen_fab/tmp/spark/Tables/config_extract_generation_details/_delta_log, version=1, metadata=Metadata(4e1d4d7e-21bb-4a7c-97ac-09fc5cd9b2aa,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"extract_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_generation_group\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_container\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_directory\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_timestamp_format\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_period_end_day\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_ordering\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_column_delimiter\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_row_delimiter\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_encoding\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_quote_character\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_escape_character\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_header\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_null_value\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_max_rows_per_file\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"output_format\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"is_trigger_file\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"trigger_file_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"is_compressed\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_type\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_level\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_file_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"fabric_lakehouse_path\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"force_single_file\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"created_date\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"created_by\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"modified_date\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"modified_by\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1753951721528)), logSegment=LogSegment(file:/workspaces/ingen_fab/tmp/spark/Tables/config_extract_generation_details/_delta_log,1,ArraySeq(DeprecatedRawLocalFileStatus{path=file:/workspaces/ingen_fab/tmp/spark/Tables/config_extract_generation_details/_delta_log/00000000000000000000.json; isDirectory=false; length=3510; replication=1; blocksize=33554432; modification_time=1753951731435; access_time=1753951731643; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}, DeprecatedRawLocalFileStatus{path=file:/workspaces/ingen_fab/tmp/spark/Tables/config_extract_generation_details/_delta_log/00000000000000000001.json; isDirectory=false; length=8039; replication=1; blocksize=33554432; modification_time=1753951732549; access_time=1753951732595; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}),org.apache.spark.sql.delta.EmptyCheckpointProvider$@4f46dba,1753951732549), checksumOpt=Some(VersionChecksum(Some(6708e170-b0ff-49e6-8b79-7b0603150ff2),28642,3,None,None,1,1,None,Some(List()),Some(List()),Metadata(4e1d4d7e-21bb-4a7c-97ac-09fc5cd9b2aa,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"extract_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_generation_group\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_container\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_directory\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_timestamp_format\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_period_end_day\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_ordering\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_column_delimiter\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_row_delimiter\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_encoding\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_quote_character\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_escape_character\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_header\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_null_value\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_max_rows_per_file\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"output_format\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"is_trigger_file\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"trigger_file_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"is_compressed\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_type\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_level\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_file_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"fabric_lakehouse_path\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"force_single_file\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"created_date\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"created_by\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"modified_date\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"modified_by\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1753951721528)),Protocol(1,2),None,None,Some(List(AddFile(part-00006-4572af7d-520a-4ed9-bdac-47f070fa2301-c000.snappy.parquet,Map(),9552,1753951732522,false,{\"numRecords\":1,\"minValues\":{\"extract_name\":\"SAMPLE_ORDERS_LAKEHOUSE\",\"file_generation_group\":\"ORDER_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"orders\",\"extract_file_name\":\"orders_daily\",\"extract_file_name_timestamp_format\":\"yyyyMMdd_HHmmss\",\"extract_file_name_extension\":\"csv\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"csv\",\"compressed_type\":\"GZIP\",\"compressed_level\":\"NORMAL\",\"compressed_extension\":\".gz\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"maxValues\":{\"extract_name\":\"SAMPLE_ORDERS_LAKEHOUSE\",\"file_generation_group\":\"ORDER_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"orders\",\"extract_file_name\":\"orders_daily\",\"extract_file_name_timestamp_format\":\"yyyyMMdd_HHmmss\",\"extract_file_name_extension\":\"csv\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"csv\",\"compressed_type\":\"GZIP\",\"compressed_level\":\"NORMAL\",\"compressed_extension\":\".gz\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"nullCount\":{\"extract_name\":0,\"file_generation_group\":0,\"extract_container\":0,\"extract_directory\":0,\"extract_file_name\":0,\"extract_file_name_timestamp_format\":0,\"extract_file_name_period_end_day\":1,\"extract_file_name_extension\":0,\"extract_file_name_ordering\":0,\"file_properties_column_delimiter\":0,\"file_properties_row_delimiter\":0,\"file_properties_encoding\":0,\"file_properties_quote_character\":0,\"file_properties_escape_character\":0,\"file_properties_header\":0,\"file_properties_null_value\":0,\"file_properties_max_rows_per_file\":1,\"output_format\":0,\"is_trigger_file\":0,\"trigger_file_extension\":1,\"is_compressed\":0,\"compressed_type\":0,\"compressed_level\":0,\"compressed_file_name\":1,\"compressed_extension\":0,\"fabric_lakehouse_path\":1,\"force_single_file\":0,\"created_date\":0,\"created_by\":0,\"modified_date\":1,\"modified_by\":1}},null,null,None,None,None), AddFile(part-00004-3f3237d5-0e63-4103-834d-eff10648adee-c000.snappy.parquet,Map(),9551,1753951732512,false,{\"numRecords\":1,\"minValues\":{\"extract_name\":\"SAMPLE_PRODUCTS_LAKEHOUSE\",\"file_generation_group\":\"PRODUCT_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"products\",\"extract_file_name\":\"products_catalog\",\"extract_file_name_timestamp_format\":\"yyyyMMdd\",\"extract_file_name_extension\":\"parquet\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"parquet\",\"trigger_file_extension\":\".done\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"maxValues\":{\"extract_name\":\"SAMPLE_PRODUCTS_LAKEHOUSE\",\"file_generation_group\":\"PRODUCT_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"products\",\"extract_file_name\":\"products_catalog\",\"extract_file_name_timestamp_format\":\"yyyyMMdd\",\"extract_file_name_extension\":\"parquet\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"parquet\",\"trigger_file_extension\":\".done\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"nullCount\":{\"extract_name\":0,\"file_generation_group\":0,\"extract_container\":0,\"extract_directory\":0,\"extract_file_name\":0,\"extract_file_name_timestamp_format\":0,\"extract_file_name_period_end_day\":1,\"extract_file_name_extension\":0,\"extract_file_name_ordering\":0,\"file_properties_column_delimiter\":0,\"file_properties_row_delimiter\":0,\"file_properties_encoding\":0,\"file_properties_quote_character\":0,\"file_properties_escape_character\":0,\"file_properties_header\":0,\"file_properties_null_value\":0,\"file_properties_max_rows_per_file\":1,\"output_format\":0,\"is_trigger_file\":0,\"trigger_file_extension\":0,\"is_compressed\":0,\"compressed_type\":1,\"compressed_level\":1,\"compressed_file_name\":1,\"compressed_extension\":1,\"fabric_lakehouse_path\":1,\"force_single_file\":0,\"created_date\":0,\"created_by\":0,\"modified_date\":1,\"modified_by\":1}},null,null,None,None,None), AddFile(part-00002-150e6eab-24cd-4c2c-8ff4-ae1f25ca2c1b-c000.snappy.parquet,Map(),9539,1753951732537,false,{\"numRecords\":1,\"minValues\":{\"extract_name\":\"SAMPLE_CUSTOMERS_LAKEHOUSE\",\"file_generation_group\":\"CUSTOMER_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"customers\",\"extract_file_name\":\"customers_lakehouse\",\"extract_file_name_timestamp_format\":\"yyyyMMdd_HHmmss\",\"extract_file_name_extension\":\"csv\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"csv\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"maxValues\":{\"extract_name\":\"SAMPLE_CUSTOMERS_LAKEHOUSE\",\"file_generation_group\":\"CUSTOMER_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"customers\",\"extract_file_name\":\"customers_lakehouse\",\"extract_file_name_timestamp_format\":\"yyyyMMdd_HHmmss\",\"extract_file_name_extension\":\"csv\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"csv\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"nullCount\":{\"extract_name\":0,\"file_generation_group\":0,\"extract_container\":0,\"extract_directory\":0,\"extract_file_name\":0,\"extract_file_name_timestamp_format\":0,\"extract_file_name_period_end_day\":1,\"extract_file_name_extension\":0,\"extract_file_name_ordering\":0,\"file_properties_column_delimiter\":0,\"file_properties_row_delimiter\":0,\"file_properties_encoding\":0,\"file_properties_quote_character\":0,\"file_properties_escape_character\":0,\"file_properties_header\":0,\"file_properties_null_value\":0,\"file_properties_max_rows_per_file\":1,\"output_format\":0,\"is_trigger_file\":0,\"trigger_file_extension\":1,\"is_compressed\":0,\"compressed_type\":1,\"compressed_level\":1,\"compressed_file_name\":1,\"compressed_extension\":1,\"fabric_lakehouse_path\":1,\"force_single_file\":0,\"created_date\":0,\"created_by\":0,\"modified_date\":1,\"modified_by\":1}},null,null,None,None,None)))))).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+-----------------+-----------------+-------------------+----------------------------------+--------------------------------+---------------------------+--------------------------+--------------------------------+-----------------------------+------------------------+-------------------------------+--------------------------------+----------------------+--------------------------+---------------------------------+-------------+---------------+----------------------+-------------+---------------+----------------+--------------------+--------------------+---------------------+-----------------+------------+----------+-------------+-----------+\n",
      "|        extract_name|file_generation_group|extract_container|extract_directory|  extract_file_name|extract_file_name_timestamp_format|extract_file_name_period_end_day|extract_file_name_extension|extract_file_name_ordering|file_properties_column_delimiter|file_properties_row_delimiter|file_properties_encoding|file_properties_quote_character|file_properties_escape_character|file_properties_header|file_properties_null_value|file_properties_max_rows_per_file|output_format|is_trigger_file|trigger_file_extension|is_compressed|compressed_type|compressed_level|compressed_file_name|compressed_extension|fabric_lakehouse_path|force_single_file|created_date|created_by|modified_date|modified_by|\n",
      "+--------------------+---------------------+-----------------+-----------------+-------------------+----------------------------------+--------------------------------+---------------------------+--------------------------+--------------------------------+-----------------------------+------------------------+-------------------------------+--------------------------------+----------------------+--------------------------+---------------------------------+-------------+---------------+----------------------+-------------+---------------+----------------+--------------------+--------------------+---------------------+-----------------+------------+----------+-------------+-----------+\n",
      "|SAMPLE_ORDERS_LAK...|           ORDER_DATA|   Files/extracts|           orders|       orders_daily|                   yyyyMMdd_HHmmss|                            NULL|                        csv|                         1|                               ,|                           \\n|                   UTF-8|                              \"|                               \\|                  true|                          |                             NULL|          csv|          false|                  NULL|         true|           GZIP|          NORMAL|                NULL|                 .gz|                 NULL|             true|  2024-01-15|    system|         NULL|       NULL|\n",
      "|SAMPLE_PRODUCTS_L...|         PRODUCT_DATA|   Files/extracts|         products|   products_catalog|                          yyyyMMdd|                            NULL|                    parquet|                         1|                               ,|                           \\n|                   UTF-8|                              \"|                               \\|                  true|                          |                             NULL|      parquet|           true|                 .done|        false|           NULL|            NULL|                NULL|                NULL|                 NULL|             true|  2024-01-15|    system|         NULL|       NULL|\n",
      "|SAMPLE_CUSTOMERS_...|        CUSTOMER_DATA|   Files/extracts|        customers|customers_lakehouse|                   yyyyMMdd_HHmmss|                            NULL|                        csv|                         1|                               ,|                           \\n|                   UTF-8|                              \"|                               \\|                  true|                          |                             NULL|          csv|          false|                  NULL|        false|           NULL|            NULL|                NULL|                NULL|                 NULL|             true|  2024-01-15|    system|         NULL|       NULL|\n",
      "+--------------------+---------------------+-----------------+-----------------+-------------------+----------------------------------+--------------------------------+---------------------------+--------------------------+--------------------------------+-----------------------------+------------------------+-------------------------------+--------------------------------+----------------------+--------------------------+---------------------------------+-------------+---------------+----------------------+-------------+---------------+----------------+--------------------+--------------------+---------------------+-----------------+------------+----------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lu.execute_query(\"SELECT * from config_extract_generation_details\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7e15957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------------+--------------+------------+------+----------------+----------------------+-------------------------+-----------------+-----------------+----------------+---------------+---------------+--------------+----------------+-----------------+-----------------------+----------------------+-------------------------------+--------------------+---------------------+-------------------------+-----------------+-----------------+-------------------+------------+------------------------+-------------+-------------+--------------------------+--------------------+------------+----------+\n",
      "|log_id|config_id|execution_id|job_start_time|job_end_time|status|source_file_path|source_file_size_bytes|source_file_modified_time|target_table_name|records_processed|records_inserted|records_updated|records_deleted|records_failed|source_row_count|staging_row_count|target_row_count_before|target_row_count_after|row_count_reconciliation_status|row_count_difference|data_read_duration_ms|staging_write_duration_ms|merge_duration_ms|total_duration_ms|avg_rows_per_second|data_size_mb|throughput_mb_per_second|error_message|error_details|execution_duration_seconds|spark_application_id|created_date|created_by|\n",
      "+------+---------+------------+--------------+------------+------+----------------+----------------------+-------------------------+-----------------+-----------------+----------------+---------------+---------------+--------------+----------------+-----------------+-----------------------+----------------------+-------------------------------+--------------------+---------------------+-------------------------+-----------------+-----------------+-------------------+------------+------------------------+-------------+-------------+--------------------------+--------------------+------------+----------+\n",
      "+------+---------+------------+--------------+------------+------+----------------+----------------------+-------------------------+-----------------+-----------------+----------------+---------------+---------------+--------------+----------------+-----------------+-----------------------+----------------------+-------------------------------+--------------------+---------------------+-------------------------+-----------------+-----------------+-------------------+------------+------------------------+-------------+-------------+--------------------------+--------------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lu.execute_query(\"SELECT * FROM log_flat_file_ingestion\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92053fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/14 06:03:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_HOME: /opt/bitnami/spark\n",
      "Expected config file: /opt/bitnami/spark/conf/spark-defaults.conf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-bf33e5a4-c086-4102-a9b8-7e3faf685181\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-bf33e5a4-c086-4102-a9b8-7e3faf685181\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-9723fbe2-13b2-4fd9-bda6-7c511cb1b88f\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-9723fbe2-13b2-4fd9-bda6-7c511cb1b88f\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-0aadf707-5118-4dbf-8dc8-6a4acdb9ce1a\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-0aadf707-5118-4dbf-8dc8-6a4acdb9ce1a\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-9f347010-be87-476f-a558-c28da585f837\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-9f347010-be87-476f-a558-c28da585f837\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-d02cf7ca-50f4-4047-b004-8a03a92911d2\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-d02cf7ca-50f4-4047-b004-8a03a92911d2\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-6d73eb59-91f0-4532-99aa-6b490d08e60e\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-6d73eb59-91f0-4532-99aa-6b490d08e60e\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-a86f5bd9-19ef-4a7e-8467-64b29d50badd\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-a86f5bd9-19ef-4a7e-8467-64b29d50badd\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-c5fd85d7-a06c-4010-a6e4-4a265de68245\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-c5fd85d7-a06c-4010-a6e4-4a265de68245\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-e9cd36ce-e32e-4973-be7e-040f48e3220c\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-e9cd36ce-e32e-4973-be7e-040f48e3220c\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-d66d3cc2-0a20-4d4d-b1d3-09656ab6f511\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-d66d3cc2-0a20-4d4d-b1d3-09656ab6f511\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o35.showString.\n: java.io.IOException: Failed to create a temp directory (under artifacts) after 10 attempts!\n\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:411)\n\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected config file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.environ.get(\u001b[33m'\u001b[39m\u001b[33mSPARK_HOME\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/conf/spark-defaults.conf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m df = spark.sql(\u001b[33m\"\u001b[39m\u001b[33mSELECT 1 as test\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/sql/classic/dataframe.py:285\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/sql/classic/dataframe.py:303\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    298\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    299\u001b[39m         messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvertical\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical).\u001b[34m__name__\u001b[39m},\n\u001b[32m    300\u001b[39m     )\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/lib/py4j.zip/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/errors/exceptions/captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/lib/py4j.zip/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o35.showString.\n: java.io.IOException: Failed to create a temp directory (under artifacts) after 10 attempts!\n\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:411)\n\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ConfigCheck\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Method 1: Check SPARK_HOME and config file locations\n",
    "print(f\"SPARK_HOME: {os.environ.get('SPARK_HOME', 'Not set')}\")\n",
    "print(f\"Expected config file: {os.environ.get('SPARK_HOME', '')}/conf/spark-defaults.conf\")\n",
    "df = spark.sql(\"SELECT 1 as test\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20d901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = (\n",
    "    SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5788119e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file from: synthetic_data/single/retail_oltp_small/customers.csv/\n",
      "First part of path: synthetic_data\n",
      "Reading file from: file:////workspaces/ingen_fab/tmp/spark/Files/synthetic_data/single/retail_oltp_small/customers.csv/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lu.read_file(\n",
    "    file_path=\"synthetic_data/single/retail_oltp_small/customers.csv/\",\n",
    "    file_format=\"csv\"\n",
    ").count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ingen_fab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
