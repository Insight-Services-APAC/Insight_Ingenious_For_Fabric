{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f54f3fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NotebookUtils not available, assumed running in local mode.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Check if running in Fabric environment\n",
    "if \"notebookutils\" in sys.modules:\n",
    "    import sys\n",
    "\n",
    "    notebookutils.fs.mount(\n",
    "        \"abfss://{{varlib:config_workspace_name}}@onelake.dfs.fabric.microsoft.com/{{varlib:config_lakehouse_name}}.Lakehouse/Files/\",\n",
    "        \"/config_files\",\n",
    "    )  # type: ignore # noqa: F821\n",
    "    mount_path = notebookutils.fs.getMountPath(\"/config_files\")  # type: ignore # noqa: F821\n",
    "\n",
    "    run_mode = \"fabric\"\n",
    "    sys.path.insert(0, mount_path)\n",
    "\n",
    "    # PySpark environment - spark session should be available\n",
    "\n",
    "else:\n",
    "    print(\"NotebookUtils not available, assumed running in local mode.\")\n",
    "    from ingen_fab.python_libs.pyspark.notebook_utils_abstraction import (\n",
    "        NotebookUtilsFactory,\n",
    "    )\n",
    "\n",
    "    notebookutils = NotebookUtilsFactory.create_instance()\n",
    "\n",
    "    spark = None\n",
    "\n",
    "    mount_path = None\n",
    "    run_mode = \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d662654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting...ingen_fab.python_libs\n",
      "deleting...ingen_fab.python_libs.common\n",
      "deleting...ingen_fab.python_libs.common.config_utils\n",
      "deleting...ingen_fab.python_libs.interfaces\n",
      "deleting...ingen_fab.python_libs.interfaces.ddl_utils_interface\n",
      "deleting...ingen_fab.python_libs.interfaces.data_store_interface\n",
      "deleting...ingen_fab.python_libs.pyspark.lakehouse_utils\n",
      "deleting...ingen_fab.python_libs.pyspark.ddl_utils\n",
      "deleting...ingen_fab.python_libs.common.utils\n",
      "deleting...ingen_fab.python_libs.common.utils.path_utils\n",
      "deleting...ingen_fab.python_libs.interfaces.notebook_utils_interface\n",
      "deleting...ingen_fab.python_libs.common.notebook_utils_base\n",
      "deleting...ingen_fab.python_libs.pyspark.notebook_utils_abstraction\n",
      "deleting...ingen_fab.python_libs.pyspark.parquet_load_utils\n",
      "deleting...ingen_fab.python_libs.pyspark\n",
      "deleting...ingen_fab.python_libs.common.spark_session_factory\n",
      "deleting...ingen_fab.python_libs.common.path_configuration\n",
      "deleting...ingen_fab\n",
      "deleting...ingen_fab.notebook_utils.notebook_utils\n",
      "deleting...ingen_fab.notebook_utils.base_notebook_compiler\n",
      "deleting...ingen_fab.notebook_utils.fabric_cli_notebook\n",
      "deleting...ingen_fab.notebook_utils.notebook_finder\n",
      "deleting...ingen_fab.notebook_utils\n",
      "deleting...ingen_fab.packages.extract_generation.extract_generation\n",
      "deleting...ingen_fab.packages.extract_generation\n",
      "deleting...ingen_fab.packages.flat_file_ingestion.flat_file_ingestion\n",
      "deleting...ingen_fab.packages.flat_file_ingestion\n",
      "deleting...ingen_fab.packages.synapse_sync\n",
      "deleting...ingen_fab.packages.synapse_sync.synapse_sync\n",
      "deleting...ingen_fab.packages\n",
      "deleting...ingen_fab.packages.data_profiling.compilation\n",
      "deleting...ingen_fab.packages.data_profiling.libs\n",
      "deleting...ingen_fab.packages.data_profiling.libs.interfaces\n",
      "deleting...ingen_fab.packages.data_profiling.libs.interfaces.data_profiling_interface\n",
      "deleting...ingen_fab.packages.data_profiling.libs.interfaces.profiler_registry\n",
      "deleting...ingen_fab.packages.data_profiling.compilation.configuration_builder\n",
      "deleting...ingen_fab.packages.data_profiling.data_profiling\n",
      "deleting...ingen_fab.packages.data_profiling.compilation.ddl_manager\n",
      "deleting...ingen_fab.packages.data_profiling.compilation.compiler\n",
      "deleting...ingen_fab.packages.data_profiling.compilation.modular_compiler\n",
      "deleting...ingen_fab.packages.data_profiling\n",
      "deleting...ingen_fab.packages.data_profiling.runtime\n",
      "deleting...ingen_fab.packages.data_profiling.runtime.core.enums.profile_types\n",
      "deleting...ingen_fab.packages.data_profiling.runtime.core.enums\n",
      "deleting...ingen_fab.packages.data_profiling.runtime.core.models.metadata\n",
      "deleting...ingen_fab.packages.data_profiling.runtime.core.models.statistics\n",
      "deleting...ingen_fab.packages.data_profiling.runtime.core.models.relationships\n",
      "deleting...ingen_fab.packages.data_profiling.runtime.core.models.profile_models\n",
      "deleting...ingen_fab.packages.data_profiling.runtime.core.models\n",
      "deleting...ingen_fab.packages.data_profiling.runtime.core.interfaces.profiling_interface\n",
      "deleting...ingen_fab.packages.data_profiling.runtime.core.interfaces.persistence_interface\n",
      "deleting...ingen_fab.packages.data_profiling.runtime.core.interfaces\n",
      "deleting...ingen_fab.packages.data_profiling.runtime.core\n",
      "deleting...ingen_fab.packages.data_profiling.runtime.persistence.base_persistence\n",
      "deleting...ingen_fab.packages.data_profiling.runtime.persistence.lakehouse_persistence\n",
      "deleting...ingen_fab.packages.data_profiling.runtime.persistence.memory_persistence\n",
      "deleting...ingen_fab.packages.data_profiling.runtime.persistence.enhanced_lakehouse_persistence\n",
      "deleting...ingen_fab.packages.data_profiling.runtime.persistence.factory\n",
      "deleting...ingen_fab.packages.data_profiling.runtime.persistence\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def load_python_modules_from_path(\n",
    "    base_path: str, relative_files: list[str], max_chars: int = 1_000_000_000\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes Python files from a Fabric-mounted file path using notebookutils.fs.head.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): The root directory where modules are located.\n",
    "        relative_files (list[str]): List of relative paths to Python files (from base_path).\n",
    "        max_chars (int): Max characters to read from each file (default: 1,000,000).\n",
    "    \"\"\"\n",
    "    success_files = []\n",
    "    failed_files = []\n",
    "\n",
    "    for relative_path in relative_files:\n",
    "        full_path = f\"file:{base_path}/{relative_path}\"\n",
    "        try:\n",
    "            print(f\"ðŸ”„ Loading: {full_path}\")\n",
    "            code = notebookutils.fs.head(full_path, max_chars)\n",
    "            exec(code, globals())  # Use globals() to share context across modules\n",
    "            success_files.append(relative_path)\n",
    "        except Exception:\n",
    "            failed_files.append(relative_path)\n",
    "            print(f\"âŒ Error loading {relative_path}\")\n",
    "\n",
    "    print(\"\\nâœ… Successfully loaded:\")\n",
    "    for f in success_files:\n",
    "        print(f\" - {f}\")\n",
    "\n",
    "    if failed_files:\n",
    "        print(\"\\nâš ï¸ Failed to load:\")\n",
    "        for f in failed_files:\n",
    "            print(f\" - {f}\")\n",
    "\n",
    "\n",
    "def clear_module_cache(prefix: str):\n",
    "    \"\"\"Clear module cache for specified prefix\"\"\"\n",
    "    for mod in list(sys.modules):\n",
    "        if mod.startswith(prefix):\n",
    "            print(\"deleting...\" + mod)\n",
    "            del sys.modules[mod]\n",
    "\n",
    "\n",
    "# Always clear the module cache - We may remove this once the libs are stable\n",
    "clear_module_cache(\"ingen_fab.python_libs\")\n",
    "clear_module_cache(\"ingen_fab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9f94d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_mode == \"local\":\n",
    "    from ingen_fab.python_libs.common.config_utils import get_configs_as_object\n",
    "    from ingen_fab.python_libs.pyspark.ddl_utils import ddl_utils\n",
    "    from ingen_fab.python_libs.pyspark.lakehouse_utils import lakehouse_utils\n",
    "    from ingen_fab.python_libs.pyspark.notebook_utils_abstraction import (\n",
    "        NotebookUtilsFactory,\n",
    "    )\n",
    "\n",
    "    notebookutils = NotebookUtilsFactory.create_instance()\n",
    "else:\n",
    "    files_to_load = [\n",
    "        \"ingen_fab/python_libs/common/config_utils.py\",\n",
    "        \"ingen_fab/python_libs/pyspark/lakehouse_utils.py\",\n",
    "        \"ingen_fab/python_libs/pyspark/ddl_utils.py\",\n",
    "        \"ingen_fab/python_libs/pyspark/notebook_utils_abstraction.py\",\n",
    "    ]\n",
    "\n",
    "    load_python_modules_from_path(mount_path, files_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "223d1dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating local Spark session with provider: native\n"
     ]
    }
   ],
   "source": [
    "target_lakehouse = lakehouse_utils(\n",
    "    target_workspace_id=get_configs_as_object().config_workspace_id,\n",
    "    target_lakehouse_id=get_configs_as_object().config_lakehouse_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7395683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config_data_profiling',\n",
       " 'config_flat_file_ingestion',\n",
       " 'config_synthetic_data_datasets',\n",
       " 'config_synthetic_data_generation_jobs',\n",
       " 'customers',\n",
       " 'data_quality_rules',\n",
       " 'ddl_script_executions',\n",
       " 'log_flat_file_ingestion',\n",
       " 'log_synthetic_data_generation',\n",
       " 'orders',\n",
       " 'products',\n",
       " 'profile_history',\n",
       " 'profile_results',\n",
       " 'sample_customers',\n",
       " 'sample_store',\n",
       " 'synthetic_customers',\n",
       " 'synthetic_data',\n",
       " 'synthetic_orders',\n",
       " 'test_complete_fix_metadata',\n",
       " 'test_complete_fix_profiles',\n",
       " 'test_complete_fix_progress',\n",
       " 'test_complete_fix_schemas',\n",
       " 'test_error_reporting_metadata',\n",
       " 'test_error_reporting_progress',\n",
       " 'test_error_reporting_schemas',\n",
       " 'test_final_l3_metadata',\n",
       " 'test_final_l3_progress',\n",
       " 'test_final_l3_schemas',\n",
       " 'test_fix_metadata',\n",
       " 'test_fix_progress',\n",
       " 'test_fix_schemas',\n",
       " 'test_l3_customers',\n",
       " 'test_l3_edge_metadata',\n",
       " 'test_l3_edge_progress',\n",
       " 'test_l3_edge_schemas',\n",
       " 'test_l3_metrics',\n",
       " 'test_l3_orders',\n",
       " 'test_l3_profile_metadata',\n",
       " 'test_l3_profile_profiles',\n",
       " 'test_l3_profile_progress',\n",
       " 'test_l3_profile_schemas',\n",
       " 'test_l3_verify_metadata',\n",
       " 'test_l3_verify_progress',\n",
       " 'test_l3_verify_schemas',\n",
       " 'test_optimized_l3_metadata',\n",
       " 'test_optimized_l3_profiles',\n",
       " 'test_optimized_l3_progress',\n",
       " 'test_optimized_l3_schemas',\n",
       " 'tiered_profile_bridge_column_values',\n",
       " 'tiered_profile_bridge_percentiles',\n",
       " 'tiered_profile_dim_columns',\n",
       " 'tiered_profile__dim_data_types',\n",
       " 'tiered_profile_dim_data_types',\n",
       " 'tiered_profile__dim_scan_levels',\n",
       " 'tiered_profile_dim_scan_levels',\n",
       " 'tiered_profile__dim_semantic_types',\n",
       " 'tiered_profile_dim_semantic_types',\n",
       " 'tiered_profile_dim_tables',\n",
       " 'tiered_profile_fact_column_profiles',\n",
       " 'tiered_profile_fact_relationships',\n",
       " 'tiered_profile_fact_scan_progress',\n",
       " 'tiered_profile_fact_table_profiles',\n",
       " 'tiered_profile_profiles',\n",
       " 'tiered_profile_test_metadata',\n",
       " 'tiered_profile_test_progress',\n",
       " 'tiered_profile_test_schemas']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_lakehouse.list_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de8fe56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingen_fab.packages.data_profiling.runtime.persistence.enhanced_lakehouse_persistence import (\n",
    "    EnhancedLakehousePersistence,\n",
    ")\n",
    "\n",
    "enhanced_lakehouse_persistence = EnhancedLakehousePersistence(\n",
    "    lakehouse=target_lakehouse, spark=target_lakehouse.spark, table_prefix=\"tiered_profile\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b993f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_facts_df.filter(f\"table_name = '{table_name}'\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed774811",
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_lakehouse_persistence.list_all_profiles()\n",
    "\n",
    "enhanced_lakehouse_persistence.load_profile(\"synthetic_orders\")  # type: ignore # noqa: F821\n",
    "table_facts_df = enhanced_lakehouse_persistence.lakehouse.read_table(enhanced_lakehouse_persistence.fact_table_profiles_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfded2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_facts_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5308880a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|num_affected_rows|\n",
      "+-----------------+\n",
      "|                1|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/05 08:07:46 WARN UpdateCommand: Could not validate number of records due to missing statistics.\n"
     ]
    }
   ],
   "source": [
    "target_lakehouse.execute_query(\"Update config_data_profiling set active_yn = 'Y' where table_name = 'synthetic_orders'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e764d44",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1319013832.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtarget_lakehouse.spark.catalog.getTable(\"tiered_profile_profiles\").\u001b[39m\n                                                                       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "target_lakehouse.spark.catalog.(\"tiered_profile_profiles\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22e73d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_lakehouse.spark.sql(\"drop table tiered_profile_schemas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af809a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/24 06:51:33 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pathlib import Path\n",
    "delta_table = DeltaTable.forPath(target_lakehouse.spark, f\"file:///{Path.cwd()}/tmp/spark/Tables/config_data_profiling\")\n",
    "details = delta_table.detail().collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfc2cb5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'obj'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlazy_import\u001b[39;00m\n\u001b[32m      2\u001b[39m deploy_commands = lazy_import.lazy_module(\u001b[33m\"\u001b[39m\u001b[33mingen_fab.cli_utils.deploy_commands\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mdeploy_commands\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeploy_to_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~workspaces/i4f/ingen_fab/cli_utils/deploy_commands.py:13\u001b[39m, in \u001b[36mdeploy_to_environment\u001b[39m\u001b[34m(ctx)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeploy_to_environment\u001b[39m(ctx):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobj\u001b[49m.get(\u001b[33m\"\u001b[39m\u001b[33mfabric_workspace_repo_dir\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     14\u001b[39m         ConsoleStyles.print_error(\n\u001b[32m     15\u001b[39m             Console(),\n\u001b[32m     16\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFabric workspace repository directory not set. Use --fabric-workspace-repo-dir directly after ingen_fab to specify it.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m         )\n\u001b[32m     18\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSystemExit\u001b[39;00m(\u001b[32m1\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'obj'"
     ]
    }
   ],
   "source": [
    "import lazy_import\n",
    "deploy_commands = lazy_import.lazy_module(\"ingen_fab.cli_utils.deploy_commands\")\n",
    "deploy_commands.deploy_to_environment(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "127ead07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating local Spark session with provider: native\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /root/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /root/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-613e822e-d9d5-491f-9a0d-a0c97c6581b2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      ":: resolution report :: resolve 82ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-613e822e-d9d5-491f-9a0d-a0c97c6581b2\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n",
      "25/09/05 06:55:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/05 06:55:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Œ Initializing Tiered Profiler with lakehouse_utils...\n",
      "Delta table already exists at path: file:////workspaces/i4f/tmp/spark/Tables/test_l3_profile_metadata, skipping creation.\n",
      "Delta table already exists at path: file:////workspaces/i4f/tmp/spark/Tables/test_l3_profile_schemas, skipping creation.\n",
      "Delta table already exists at path: file:////workspaces/i4f/tmp/spark/Tables/test_l3_profile_progress, skipping creation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/05 06:55:49 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'table_name': 'test_fix_progress',\n",
       "  'row_count': 0,\n",
       "  'column_count': 12,\n",
       "  'scan_timestamp': '2025-08-24 10:24:03'},\n",
       " {'table_name': 'test_fix_metadata',\n",
       "  'row_count': 0,\n",
       "  'column_count': 11,\n",
       "  'scan_timestamp': '2025-08-24 10:24:01'},\n",
       " {'table_name': 'test_optimized_l3_schemas',\n",
       "  'row_count': 2,\n",
       "  'column_count': 6,\n",
       "  'scan_timestamp': '2025-08-24 10:23:59'},\n",
       " {'table_name': 'test_optimized_l3_profiles',\n",
       "  'row_count': 2,\n",
       "  'column_count': 5,\n",
       "  'scan_timestamp': '2025-08-24 10:23:56'},\n",
       " {'table_name': 'test_optimized_l3_progress',\n",
       "  'row_count': 48,\n",
       "  'column_count': 12,\n",
       "  'scan_timestamp': '2025-08-24 10:23:54'},\n",
       " {'table_name': 'test_optimized_l3_metadata',\n",
       "  'row_count': 48,\n",
       "  'column_count': 11,\n",
       "  'scan_timestamp': '2025-08-24 10:23:52'},\n",
       " {'table_name': 'test_complete_fix_profiles',\n",
       "  'row_count': 1,\n",
       "  'column_count': 5,\n",
       "  'scan_timestamp': '2025-08-24 10:23:50'},\n",
       " {'table_name': 'test_complete_fix_progress',\n",
       "  'row_count': 43,\n",
       "  'column_count': 12,\n",
       "  'scan_timestamp': '2025-08-24 10:23:48'},\n",
       " {'table_name': 'test_complete_fix_schemas',\n",
       "  'row_count': 1,\n",
       "  'column_count': 6,\n",
       "  'scan_timestamp': '2025-08-24 10:23:46'},\n",
       " {'table_name': 'test_complete_fix_metadata',\n",
       "  'row_count': 43,\n",
       "  'column_count': 11,\n",
       "  'scan_timestamp': '2025-08-24 10:23:44'},\n",
       " {'table_name': 'profile_history',\n",
       "  'row_count': 0,\n",
       "  'column_count': 20,\n",
       "  'scan_timestamp': '2025-08-24 10:23:41'},\n",
       " {'table_name': 'test_l3_profile_profiles',\n",
       "  'row_count': 36,\n",
       "  'column_count': 5,\n",
       "  'scan_timestamp': '2025-08-24 10:23:39'},\n",
       " {'table_name': 'test_fix_schemas',\n",
       "  'row_count': 0,\n",
       "  'column_count': 6,\n",
       "  'scan_timestamp': '2025-08-24 10:23:37'},\n",
       " {'table_name': 'products',\n",
       "  'row_count': 6,\n",
       "  'column_count': 6,\n",
       "  'scan_timestamp': '2025-08-24 10:23:35'},\n",
       " {'table_name': 'customers',\n",
       "  'row_count': 5,\n",
       "  'column_count': 5,\n",
       "  'scan_timestamp': '2025-08-24 10:23:33'},\n",
       " {'table_name': 'synthetic_data',\n",
       "  'row_count': 10000,\n",
       "  'column_count': 5,\n",
       "  'scan_timestamp': '2025-08-24 10:23:30'},\n",
       " {'table_name': 'test_error_reporting_schemas',\n",
       "  'row_count': 0,\n",
       "  'column_count': 6,\n",
       "  'scan_timestamp': '2025-08-24 10:23:28'},\n",
       " {'table_name': 'test_error_reporting_metadata',\n",
       "  'row_count': 1,\n",
       "  'column_count': 11,\n",
       "  'scan_timestamp': '2025-08-24 10:23:26'},\n",
       " {'table_name': 'tiered_profile_test_schemas',\n",
       "  'row_count': 4,\n",
       "  'column_count': 6,\n",
       "  'scan_timestamp': '2025-08-24 10:23:24'},\n",
       " {'table_name': 'synthetic_customers',\n",
       "  'row_count': 10000,\n",
       "  'column_count': 5,\n",
       "  'scan_timestamp': '2025-08-24 10:23:22'},\n",
       " {'table_name': 'config_synthetic_data_datasets',\n",
       "  'row_count': 8,\n",
       "  'column_count': 11,\n",
       "  'scan_timestamp': '2025-08-24 10:23:19'},\n",
       " {'table_name': 'test_error_reporting_progress',\n",
       "  'row_count': 1,\n",
       "  'column_count': 12,\n",
       "  'scan_timestamp': '2025-08-24 10:23:17'},\n",
       " {'table_name': 'test_l3_edge_metadata',\n",
       "  'row_count': 31,\n",
       "  'column_count': 11,\n",
       "  'scan_timestamp': '2025-08-24 10:23:15'},\n",
       " {'table_name': 'test_l3_edge_progress',\n",
       "  'row_count': 30,\n",
       "  'column_count': 12,\n",
       "  'scan_timestamp': '2025-08-24 10:23:12'},\n",
       " {'table_name': 'test_l3_customers',\n",
       "  'row_count': 15,\n",
       "  'column_count': 8,\n",
       "  'scan_timestamp': '2025-08-24 10:23:10'},\n",
       " {'table_name': 'test_l3_verify_schemas',\n",
       "  'row_count': 0,\n",
       "  'column_count': 6,\n",
       "  'scan_timestamp': '2025-08-24 10:23:08'},\n",
       " {'table_name': 'test_l3_verify_progress',\n",
       "  'row_count': 0,\n",
       "  'column_count': 12,\n",
       "  'scan_timestamp': '2025-08-24 10:23:06'},\n",
       " {'table_name': 'sample_customers',\n",
       "  'row_count': 3,\n",
       "  'column_count': 6,\n",
       "  'scan_timestamp': '2025-08-24 10:23:04'},\n",
       " {'table_name': 'tiered_profile_test_progress',\n",
       "  'row_count': 21,\n",
       "  'column_count': 12,\n",
       "  'scan_timestamp': '2025-08-24 10:23:02'},\n",
       " {'table_name': 'tiered_profile_test_metadata',\n",
       "  'row_count': 21,\n",
       "  'column_count': 11,\n",
       "  'scan_timestamp': '2025-08-24 10:23:00'},\n",
       " {'table_name': 'test_l3_profile_schemas',\n",
       "  'row_count': 48,\n",
       "  'column_count': 6,\n",
       "  'scan_timestamp': '2025-08-24 10:22:57'},\n",
       " {'table_name': 'ddl_script_executions',\n",
       "  'row_count': 15,\n",
       "  'column_count': 4,\n",
       "  'scan_timestamp': '2025-08-24 10:22:55'},\n",
       " {'table_name': 'log_flat_file_ingestion',\n",
       "  'row_count': 96,\n",
       "  'column_count': 37,\n",
       "  'scan_timestamp': '2025-08-24 10:22:53'},\n",
       " {'table_name': 'orders',\n",
       "  'row_count': 8,\n",
       "  'column_count': 5,\n",
       "  'scan_timestamp': '2025-08-24 10:22:49'},\n",
       " {'table_name': 'sample_store',\n",
       "  'row_count': 0,\n",
       "  'column_count': 2,\n",
       "  'scan_timestamp': '2025-08-24 10:22:47'},\n",
       " {'table_name': 'test_l3_metrics',\n",
       "  'row_count': 100,\n",
       "  'column_count': 6,\n",
       "  'scan_timestamp': '2025-08-24 10:22:45'},\n",
       " {'table_name': 'test_l3_profile_progress',\n",
       "  'row_count': 48,\n",
       "  'column_count': 12,\n",
       "  'scan_timestamp': '2025-08-24 10:22:43'},\n",
       " {'table_name': 'test_l3_edge_schemas',\n",
       "  'row_count': 0,\n",
       "  'column_count': 6,\n",
       "  'scan_timestamp': '2025-08-24 10:22:41'},\n",
       " {'table_name': 'test_l3_verify_metadata',\n",
       "  'row_count': 0,\n",
       "  'column_count': 11,\n",
       "  'scan_timestamp': '2025-08-24 10:22:38'},\n",
       " {'table_name': 'data_quality_rules',\n",
       "  'row_count': 0,\n",
       "  'column_count': 16,\n",
       "  'scan_timestamp': '2025-08-24 10:22:35'},\n",
       " {'table_name': 'test_l3_profile_metadata',\n",
       "  'row_count': 48,\n",
       "  'column_count': 11,\n",
       "  'scan_timestamp': '2025-08-24 10:22:31'},\n",
       " {'table_name': 'test_l3_orders',\n",
       "  'row_count': 20,\n",
       "  'column_count': 6,\n",
       "  'scan_timestamp': '2025-08-24 10:22:29'},\n",
       " {'table_name': 'synthetic_orders',\n",
       "  'row_count': 1200000,\n",
       "  'column_count': 11,\n",
       "  'scan_timestamp': '2025-08-24 10:22:26'},\n",
       " {'table_name': 'config_flat_file_ingestion',\n",
       "  'row_count': 2,\n",
       "  'column_count': 53,\n",
       "  'scan_timestamp': '2025-08-24 10:22:20'},\n",
       " {'table_name': 'log_synthetic_data_generation',\n",
       "  'row_count': 0,\n",
       "  'column_count': 12,\n",
       "  'scan_timestamp': '2025-08-24 10:22:16'},\n",
       " {'table_name': 'config_synthetic_data_generation_jobs',\n",
       "  'row_count': 0,\n",
       "  'column_count': 16,\n",
       "  'scan_timestamp': '2025-08-24 10:22:13'},\n",
       " {'table_name': 'profile_results',\n",
       "  'row_count': 0,\n",
       "  'column_count': 28,\n",
       "  'scan_timestamp': '2025-08-24 10:22:09'},\n",
       " {'table_name': 'config_data_profiling',\n",
       "  'row_count': 0,\n",
       "  'column_count': 23,\n",
       "  'scan_timestamp': '2025-08-24 10:22:06'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ingen_fab.python_libs.pyspark.tiered_profiler import TieredProfiler\n",
    "\n",
    "\n",
    "configs = get_configs_as_object()\n",
    "lakehouse = lakehouse_utils(\n",
    "    target_workspace_id=configs.config_workspace_id,\n",
    "    target_lakehouse_id=configs.config_lakehouse_id,\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize profiler using lakehouse_utils\n",
    "print(\"\\nðŸ”Œ Initializing Tiered Profiler with lakehouse_utils...\")\n",
    "\n",
    "# Create profiler with lakehouse_utils (persistence now in lakehouse)\n",
    "profiler = TieredProfiler(\n",
    "    lakehouse=lakehouse,\n",
    "    table_prefix=\"test_l3_profile\"\n",
    ")\n",
    "\n",
    "x = profiler.get_explorer()\n",
    "x.list_available_profiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88df886a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+-----------------+-------------------+-----------------+-------------------+-----------------+-------------------+----------+---------------+--------------------+\n",
      "|          table_name|   level_1_completed|level_1_duration_ms|level_2_completed|level_2_duration_ms|level_3_completed|level_3_duration_ms|level_4_completed|level_4_duration_ms|last_error|last_error_time|        last_updated|\n",
      "+--------------------+--------------------+-------------------+-----------------+-------------------+-----------------+-------------------+-----------------+-------------------+----------+---------------+--------------------+\n",
      "| synthetic_customers|2025-08-24 10:19:...|               1189|             NULL|               NULL|             NULL|               NULL|             NULL|               NULL|      NULL|           NULL|2025-08-24 10:19:...|\n",
      "|     profile_results|2025-08-24 10:19:...|               1125|             NULL|               NULL|             NULL|               NULL|             NULL|               NULL|      NULL|           NULL|2025-08-24 10:19:...|\n",
      "|log_synthetic_dat...|2025-08-24 10:19:...|               1091|             NULL|               NULL|             NULL|               NULL|             NULL|               NULL|      NULL|           NULL|2025-08-24 10:19:...|\n",
      "|config_flat_file_...|2025-08-24 10:19:...|               1315|             NULL|               NULL|             NULL|               NULL|             NULL|               NULL|      NULL|           NULL|2025-08-24 10:19:...|\n",
      "|    sample_customers|2025-08-24 10:19:...|               1066|             NULL|               NULL|             NULL|               NULL|             NULL|               NULL|      NULL|           NULL|2025-08-24 10:19:...|\n",
      "|config_synthetic_...|2025-08-24 10:19:...|               1265|             NULL|               NULL|             NULL|               NULL|             NULL|               NULL|      NULL|           NULL|2025-08-24 10:19:...|\n",
      "|config_synthetic_...|2025-08-24 10:19:...|               1336|             NULL|               NULL|             NULL|               NULL|             NULL|               NULL|      NULL|           NULL|2025-08-24 10:19:...|\n",
      "|     profile_history|2025-08-24 10:19:...|               1120|             NULL|               NULL|             NULL|               NULL|             NULL|               NULL|      NULL|           NULL|2025-08-24 10:19:...|\n",
      "|           customers|2025-08-24 10:19:...|               1333|             NULL|               NULL|             NULL|               NULL|             NULL|               NULL|      NULL|           NULL|2025-08-24 10:19:...|\n",
      "|ddl_script_execut...|2025-08-24 10:19:...|               1485|             NULL|               NULL|             NULL|               NULL|             NULL|               NULL|      NULL|           NULL|2025-08-24 10:19:...|\n",
      "|            products|2025-08-24 10:19:...|               1342|             NULL|               NULL|             NULL|               NULL|             NULL|               NULL|      NULL|           NULL|2025-08-24 10:19:...|\n",
      "|config_data_profi...|2025-08-24 10:19:...|               1729|             NULL|               NULL|             NULL|               NULL|             NULL|               NULL|      NULL|           NULL|2025-08-24 10:19:...|\n",
      "|  data_quality_rules|2025-08-24 10:19:...|               1111|             NULL|               NULL|             NULL|               NULL|             NULL|               NULL|      NULL|           NULL|2025-08-24 10:19:...|\n",
      "|test_complete_fix...|2025-08-24 10:20:...|               1582|             NULL|               NULL|             NULL|               NULL|             NULL|               NULL|      NULL|           NULL|2025-08-24 10:20:...|\n",
      "|   test_fix_metadata|2025-08-24 10:20:...|               1088|             NULL|               NULL|             NULL|               NULL|             NULL|               NULL|      NULL|           NULL|2025-08-24 10:20:...|\n",
      "|test_error_report...|2025-08-24 10:20:...|               1063|             NULL|               NULL|             NULL|               NULL|             NULL|               NULL|      NULL|           NULL|2025-08-24 10:20:...|\n",
      "|test_complete_fix...|2025-08-24 10:20:...|               1057|             NULL|               NULL|             NULL|               NULL|             NULL|               NULL|      NULL|           NULL|2025-08-24 10:20:...|\n",
      "|test_error_report...|2025-08-24 10:20:...|               1146|             NULL|               NULL|             NULL|               NULL|             NULL|               NULL|      NULL|           NULL|2025-08-24 10:20:...|\n",
      "|test_error_report...|2025-08-24 10:20:...|               1094|             NULL|               NULL|             NULL|               NULL|             NULL|               NULL|      NULL|           NULL|2025-08-24 10:20:...|\n",
      "|test_l3_edge_schemas|2025-08-24 10:20:...|               1116|             NULL|               NULL|             NULL|               NULL|             NULL|               NULL|      NULL|           NULL|2025-08-24 10:20:...|\n",
      "+--------------------+--------------------+-------------------+-----------------+-------------------+-----------------+-------------------+-----------------+-------------------+----------+---------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "target_lakehouse.spark.sql(\"SELECT * FROM test_optimized_l3_progress\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e4b5c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing Spark session, reusing it.\n",
      "Delta table already exists at path: file:////workspaces/i4f/tmp/spark/Tables/ddl_script_executions, skipping creation.\n",
      "Skipping ddl_script_executions as it already exists\n",
      "Found existing Spark session, reusing it.\n"
     ]
    }
   ],
   "source": [
    "du = ddl_utils(\n",
    "    target_workspace_id=get_configs_as_object().config_workspace_id,\n",
    "    target_lakehouse_id=get_configs_as_object().config_lakehouse_id,\n",
    ")\n",
    "\n",
    "config_lakehouse = lakehouse_utils(\n",
    "    target_workspace_id=get_configs_as_object().config_workspace_id,\n",
    "    target_lakehouse_id=get_configs_as_object().config_lakehouse_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98163c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  Alert: Registering table 'config_flat_file_ingestion' in the Hive catalog for local Spark.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/20 22:47:05 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`default`.`config_flat_file_ingestion` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "25/08/20 22:47:05 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/08/20 22:47:05 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n"
     ]
    }
   ],
   "source": [
    "# Sample configuration data for flat file ingestion testing - Universal schema (Lakehouse version)\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Import the universal schema definition\n",
    "from pyspark.sql.types import (\n",
    "    BooleanType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"config_id\", StringType(), nullable=False),\n",
    "        StructField(\"config_name\", StringType(), nullable=False),\n",
    "        StructField(\"source_file_path\", StringType(), nullable=False),\n",
    "        StructField(\n",
    "            \"source_file_format\", StringType(), nullable=False\n",
    "        ),  # csv, json, parquet, avro, xml\n",
    "        # Source location fields (optional - defaults to target or raw workspace)\n",
    "        StructField(\n",
    "            \"source_workspace_id\", StringType(), nullable=True\n",
    "        ),  # Source workspace (defaults to target if null)\n",
    "        StructField(\n",
    "            \"source_datastore_id\", StringType(), nullable=True\n",
    "        ),  # Source lakehouse/warehouse (defaults to raw if null)\n",
    "        StructField(\n",
    "            \"source_datastore_type\", StringType(), nullable=True\n",
    "        ),  # 'lakehouse' or 'warehouse' (defaults to lakehouse)\n",
    "        StructField(\n",
    "            \"source_file_root_path\", StringType(), nullable=True\n",
    "        ),  # Root path override (e.g., \"Files\", \"Tables\")\n",
    "        # Target location fields\n",
    "        StructField(\n",
    "            \"target_workspace_id\", StringType(), nullable=False\n",
    "        ),  # Universal field for workspace\n",
    "        StructField(\n",
    "            \"target_datastore_id\", StringType(), nullable=False\n",
    "        ),  # Universal field for lakehouse/warehouse\n",
    "        StructField(\n",
    "            \"target_datastore_type\", StringType(), nullable=False\n",
    "        ),  # 'lakehouse' or 'warehouse'\n",
    "        StructField(\"target_schema_name\", StringType(), nullable=False),\n",
    "        StructField(\"target_table_name\", StringType(), nullable=False),\n",
    "        StructField(\n",
    "            \"staging_table_name\", StringType(), nullable=True\n",
    "        ),  # For warehouse COPY INTO staging\n",
    "        StructField(\"file_delimiter\", StringType(), nullable=True),  # for CSV files\n",
    "        StructField(\"has_header\", BooleanType(), nullable=True),  # for CSV files\n",
    "        StructField(\"encoding\", StringType(), nullable=True),  # utf-8, latin-1, etc.\n",
    "        StructField(\"date_format\", StringType(), nullable=True),  # for date columns\n",
    "        StructField(\n",
    "            \"timestamp_format\", StringType(), nullable=True\n",
    "        ),  # for timestamp columns\n",
    "        StructField(\n",
    "            \"schema_inference\", BooleanType(), nullable=False\n",
    "        ),  # whether to infer schema\n",
    "        StructField(\n",
    "            \"custom_schema_json\", StringType(), nullable=True\n",
    "        ),  # custom schema definition\n",
    "        StructField(\n",
    "            \"partition_columns\", StringType(), nullable=True\n",
    "        ),  # comma-separated list\n",
    "        StructField(\n",
    "            \"sort_columns\", StringType(), nullable=True\n",
    "        ),  # comma-separated list\n",
    "        StructField(\n",
    "            \"write_mode\", StringType(), nullable=False\n",
    "        ),  # overwrite, append, merge\n",
    "        StructField(\"merge_keys\", StringType(), nullable=True),  # for merge operations\n",
    "        StructField(\n",
    "            \"data_validation_rules\", StringType(), nullable=True\n",
    "        ),  # JSON validation rules\n",
    "        StructField(\n",
    "            \"error_handling_strategy\", StringType(), nullable=False\n",
    "        ),  # fail, skip, log\n",
    "        StructField(\"execution_group\", IntegerType(), nullable=False),\n",
    "        StructField(\"active_yn\", StringType(), nullable=False),\n",
    "        StructField(\"created_date\", StringType(), nullable=False),\n",
    "        StructField(\"modified_date\", StringType(), nullable=True),\n",
    "        StructField(\"created_by\", StringType(), nullable=False),\n",
    "        StructField(\"modified_by\", StringType(), nullable=True),\n",
    "        # Advanced CSV configuration fields\n",
    "        StructField(\"quote_character\", StringType(), nullable=True),  # Default: '\"'\n",
    "        StructField(\n",
    "            \"escape_character\", StringType(), nullable=True\n",
    "        ),  # Default: '\"' (Excel style)\n",
    "        StructField(\"multiline_values\", BooleanType(), nullable=True),  # Default: True\n",
    "        StructField(\n",
    "            \"ignore_leading_whitespace\", BooleanType(), nullable=True\n",
    "        ),  # Default: False\n",
    "        StructField(\n",
    "            \"ignore_trailing_whitespace\", BooleanType(), nullable=True\n",
    "        ),  # Default: False\n",
    "        StructField(\"null_value\", StringType(), nullable=True),  # Default: \"\"\n",
    "        StructField(\"empty_value\", StringType(), nullable=True),  # Default: \"\"\n",
    "        StructField(\"comment_character\", StringType(), nullable=True),  # Default: None\n",
    "        StructField(\"max_columns\", IntegerType(), nullable=True),  # Default: 100\n",
    "        StructField(\n",
    "            \"max_chars_per_column\", IntegerType(), nullable=True\n",
    "        ),  # Default: 50000\n",
    "        # New fields for incremental synthetic data import support\n",
    "        StructField(\n",
    "            \"import_pattern\", StringType(), nullable=True\n",
    "        ),  # 'single_file', 'date_partitioned', 'wildcard_pattern'\n",
    "        StructField(\n",
    "            \"date_partition_format\", StringType(), nullable=True\n",
    "        ),  # Date partition format (e.g., 'YYYY/MM/DD')\n",
    "        StructField(\n",
    "            \"table_relationship_group\", StringType(), nullable=True\n",
    "        ),  # Group for related table imports\n",
    "        StructField(\n",
    "            \"batch_import_enabled\", BooleanType(), nullable=True\n",
    "        ),  # Enable batch processing\n",
    "        StructField(\n",
    "            \"file_discovery_pattern\", StringType(), nullable=True\n",
    "        ),  # Pattern for automatic file discovery\n",
    "        StructField(\n",
    "            \"import_sequence_order\", IntegerType(), nullable=True\n",
    "        ),  # Order for related table imports\n",
    "        StructField(\n",
    "            \"date_range_start\", StringType(), nullable=True\n",
    "        ),  # Start date for batch import\n",
    "        StructField(\n",
    "            \"date_range_end\", StringType(), nullable=True\n",
    "        ),  # End date for batch import\n",
    "        StructField(\n",
    "            \"skip_existing_dates\", BooleanType(), nullable=True\n",
    "        ),  # Skip already imported dates\n",
    "        StructField(\n",
    "            \"source_is_folder\", BooleanType(), nullable=True\n",
    "        ),  # True for folder with part files, False for single file\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Sample configuration records for testing - Using synthetic data generator files\n",
    "sample_configs = [\n",
    "    Row(\n",
    "        config_id=\"synthetic_customers_folder_001\",\n",
    "        config_name=\"Synthetic Data - Customers Folder (Retail OLTP Small)\",\n",
    "        source_file_path=\"synthetic_data/csv/series/retail_oltp_small/test_data_feb_small/flat/snapshot_customers/snapshot_customers_20240201.csv\",\n",
    "        source_file_format=\"csv\",\n",
    "        source_workspace_id=\"{{varlib:sample_lh_workspace_id}}\",\n",
    "        source_datastore_id=\"{{varlib:sample_lh_lakehouse_id}}\",\n",
    "        source_datastore_type=\"lakehouse\",\n",
    "        source_file_root_path=None,  # No root path override needed\n",
    "        target_workspace_id=\"{{varlib:sample_lh_workspace_id}}\",\n",
    "        target_datastore_id=\"{{varlib:sample_lh_lakehouse_id}}\",\n",
    "        target_datastore_type=\"lakehouse\",\n",
    "        target_schema_name=\"raw\",\n",
    "        target_table_name=\"synthetic_customers\",\n",
    "        staging_table_name=None,\n",
    "        file_delimiter=\",\",\n",
    "        has_header=True,\n",
    "        encoding=\"utf-8\",\n",
    "        date_format=\"yyyy-MM-dd\",\n",
    "        timestamp_format=\"yyyy-MM-dd HH:mm:ss\",\n",
    "        schema_inference=True,\n",
    "        custom_schema_json=None,\n",
    "        partition_columns=\"\",\n",
    "        sort_columns=\"customer_id\",\n",
    "        write_mode=\"overwrite\",\n",
    "        merge_keys=\"\",\n",
    "        data_validation_rules=None,\n",
    "        error_handling_strategy=\"log\",\n",
    "        execution_group=1,  # Folder-based snapshot processing (Group 1)\n",
    "        active_yn=\"Y\",\n",
    "        created_date=\"2024-01-15\",\n",
    "        modified_date=None,\n",
    "        created_by=\"system\",\n",
    "        modified_by=None,\n",
    "        quote_character='\"',\n",
    "        escape_character='\"',\n",
    "        multiline_values=True,\n",
    "        ignore_leading_whitespace=False,\n",
    "        ignore_trailing_whitespace=False,\n",
    "        null_value=\"\",\n",
    "        empty_value=\"\",\n",
    "        comment_character=None,\n",
    "        max_columns=100,\n",
    "        max_chars_per_column=50000,\n",
    "        # New fields for incremental synthetic data import support\n",
    "        import_pattern=\"single_file\",\n",
    "        date_partition_format=None,\n",
    "        table_relationship_group=\"retail_oltp_single\",\n",
    "        batch_import_enabled=False,\n",
    "        file_discovery_pattern=None,\n",
    "        import_sequence_order=1,\n",
    "        date_range_start=None,\n",
    "        date_range_end=None,\n",
    "        skip_existing_dates=None,\n",
    "        source_is_folder=True,  # Read all part files from the folder\n",
    "    ),\n",
    "    Row(\n",
    "        config_id=\"synthetic_orders_folder_002\",\n",
    "        config_name=\"Synthetic Data - Orders Folder (Retail OLTP Small)\",\n",
    "        source_file_path=\"synthetic_data/parquet/series/retail_oltp_small/high_volume_summer_data/flat/orders/\",\n",
    "        source_file_format=\"parquet\",\n",
    "        source_workspace_id=\"{{varlib:sample_lh_workspace_id}}\",\n",
    "        source_datastore_id=\"{{varlib:sample_lh_lakehouse_id}}\",\n",
    "        source_datastore_type=\"lakehouse\",\n",
    "        source_file_root_path=None,  # No root path override needed\n",
    "        target_workspace_id=\"{{varlib:sample_lh_workspace_id}}\",\n",
    "        target_datastore_id=\"{{varlib:sample_lh_lakehouse_id}}\",\n",
    "        target_datastore_type=\"lakehouse\",\n",
    "        target_schema_name=\"raw\",\n",
    "        target_table_name=\"synthetic_orders\",\n",
    "        staging_table_name=None,\n",
    "        file_delimiter=\",\",\n",
    "        has_header=True,\n",
    "        encoding=\"utf-8\",\n",
    "        date_format=\"yyyyMMdd\",\n",
    "        timestamp_format=\"yyyy-MM-dd HH:mm:ss\",\n",
    "        schema_inference=True,\n",
    "        custom_schema_json=None,\n",
    "        partition_columns=\"\",\n",
    "        sort_columns=\"order_id\",\n",
    "        write_mode=\"overwrite\",\n",
    "        merge_keys=\"\",\n",
    "        data_validation_rules=None,\n",
    "        error_handling_strategy=\"log\",\n",
    "        execution_group=1,\n",
    "        active_yn=\"Y\",\n",
    "        created_date=\"2024-01-15\",\n",
    "        modified_date=None,\n",
    "        created_by=\"system\",\n",
    "        modified_by=None,\n",
    "        quote_character='\"',\n",
    "        escape_character='\"',\n",
    "        multiline_values=True,\n",
    "        ignore_leading_whitespace=False,\n",
    "        ignore_trailing_whitespace=False,\n",
    "        null_value=\"\",\n",
    "        empty_value=\"\",\n",
    "        comment_character=None,\n",
    "        max_columns=100,\n",
    "        max_chars_per_column=50000,\n",
    "        # New fields for incremental synthetic data import support\n",
    "        import_pattern=\"date_partitioned\",\n",
    "        date_partition_format=\"yyyyMMdd\",\n",
    "        table_relationship_group=\"retail_oltp_single\",\n",
    "        batch_import_enabled=False,\n",
    "        file_discovery_pattern=\"orders_*.parquet\",\n",
    "        import_sequence_order=1,\n",
    "        date_range_start=None,\n",
    "        date_range_end=None,\n",
    "        skip_existing_dates=None,\n",
    "        source_is_folder=True,  # Read all part files from the folder\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create DataFrame and insert records\n",
    "print(len(schema.fields))\n",
    "print(len(sample_configs[0].asDict()))\n",
    "df = target_lakehouse.get_connection.createDataFrame(sample_configs, schema)\n",
    "target_lakehouse.drop_table(\"config_flat_file_ingestion\")\n",
    "target_lakehouse.write_to_table(\n",
    "    df=df, table_name=\"config_flat_file_ingestion\", mode=\"overwrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0335314",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config_lakehouse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mconfig_lakehouse\u001b[49m.execute_query(\u001b[33m\"\u001b[39m\u001b[33mSelect * from profile_results\u001b[39m\u001b[33m\"\u001b[39m).show()\n",
      "\u001b[31mNameError\u001b[39m: name 'config_lakehouse' is not defined"
     ]
    }
   ],
   "source": [
    "config_lakehouse.execute_query(\"Select * from profile_results\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37e6fa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0592eb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+----------+--------------+-----------+-------------+---------------+------------+--------------+\n",
      "|order_id|customer_id|order_date|    status|payment_method|order_total|shipping_cost|discount_amount|shipped_date|delivered_date|\n",
      "+--------+-----------+----------+----------+--------------+-----------+-------------+---------------+------------+--------------+\n",
      "| 1150001|          2|2022-09-09|   Pending|    Debit Card|      170.1|          0.1|            0.1|        NULL|          NULL|\n",
      "| 1150002|          3|2022-09-10|   Pending|        PayPal|      170.2|          0.2|            0.2|        NULL|          NULL|\n",
      "| 1150003|          4|2022-09-11|   Pending| Bank Transfer|      170.3|          0.3|            0.0|        NULL|          NULL|\n",
      "| 1150004|          5|2022-09-12|   Pending|          Cash|      170.4|          0.4|            0.0|        NULL|          NULL|\n",
      "| 1150005|          6|2022-09-13|   Pending|   Credit Card|      170.5|          0.5|            0.0|        NULL|          NULL|\n",
      "| 1150006|          7|2022-09-14|   Pending|    Debit Card|      170.6|          0.6|            0.0|        NULL|          NULL|\n",
      "| 1150007|          8|2022-09-15|   Pending|        PayPal|      170.7|          0.7|            0.0|        NULL|          NULL|\n",
      "| 1150008|          9|2022-09-16|   Pending| Bank Transfer|      170.8|          0.8|            0.0|        NULL|          NULL|\n",
      "| 1150009|         10|2022-09-17|   Pending|          Cash|      170.9|          0.9|            0.0|        NULL|          NULL|\n",
      "| 1150010|         11|2022-09-18|   Pending|   Credit Card|      171.0|          1.0|            1.0|        NULL|          NULL|\n",
      "| 1150011|         12|2022-09-19|   Pending|    Debit Card|      171.1|          1.1|            1.1|        NULL|          NULL|\n",
      "| 1150012|         13|2022-09-20|   Pending|        PayPal|      171.2|          1.2|            1.2|        NULL|          NULL|\n",
      "| 1150013|         14|2022-09-21|   Pending| Bank Transfer|      171.3|          1.3|            0.0|        NULL|          NULL|\n",
      "| 1150014|         15|2022-09-22|   Pending|          Cash|      171.4|          1.4|            0.0|        NULL|          NULL|\n",
      "| 1150015|         16|2022-09-23|Processing|   Credit Card|      171.5|          1.5|            0.0|        NULL|          NULL|\n",
      "| 1150016|         17|2022-09-24|Processing|    Debit Card|      171.6|          1.6|            0.0|        NULL|          NULL|\n",
      "| 1150017|         18|2022-09-25|Processing|        PayPal|      171.7|          1.7|            0.0|        NULL|          NULL|\n",
      "| 1150018|         19|2022-09-26|Processing| Bank Transfer|      171.8|          1.8|            0.0|        NULL|          NULL|\n",
      "| 1150019|         20|2022-09-27|Processing|          Cash|      171.9|          1.9|            0.0|        NULL|          NULL|\n",
      "| 1150020|         21|2022-09-28|Processing|   Credit Card|      172.0|          2.0|            2.0|        NULL|          NULL|\n",
      "+--------+-----------+----------+----------+--------------+-----------+-------------+---------------+------------+--------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "target_lakehouse.spark.read.format(\"parquet\").load(\n",
    "    \"tmp/spark/Files/synthetic_data/parquet/series/retail_oltp_small/high_volume_summer_data/flat/orders/orders_20240601.parquet/part-00023-a53a5973-8248-405a-9982-e270544646be-c000.snappy.parquet\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e9631e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9afcadc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62359b3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mlu\u001b[49m.execute_query(\u001b[33m\"\u001b[39m\u001b[33mSELECT * from log_synapse_extract_run_log\u001b[39m\u001b[33m\"\u001b[39m).show()\n",
      "\u001b[31mNameError\u001b[39m: name 'lu' is not defined"
     ]
    }
   ],
   "source": [
    "lu.execute_query(\"SELECT * from log_synapse_extract_run_log\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "971b15fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/31 08:23:47 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------------+---------------------+------------------+--------------------+-----------------+-------------------+------------+------------------+------------+----------+-------------+-----------+\n",
      "|        extract_name|is_active|trigger_name|extract_pipeline_name|extract_table_name|extract_table_schema|extract_view_name|extract_view_schema|is_full_load|   execution_group|created_date|created_by|modified_date|modified_by|\n",
      "+--------------------+---------+------------+---------------------+------------------+--------------------+-----------------+-------------------+------------+------------------+------------+----------+-------------+-----------+\n",
      "|SAMPLE_CUSTOMERS_...|     true|        NULL|                 NULL|         customers|             default|             NULL|               NULL|        true|LAKEHOUSE_EXTRACTS|  2024-01-15|    system|         NULL|       NULL|\n",
      "|SAMPLE_PRODUCTS_L...|     true|        NULL|                 NULL|          products|             default|             NULL|               NULL|        true|LAKEHOUSE_EXTRACTS|  2024-01-15|    system|         NULL|       NULL|\n",
      "|SAMPLE_ORDERS_LAK...|     true|        NULL|                 NULL|            orders|             default|             NULL|               NULL|        true|LAKEHOUSE_EXTRACTS|  2024-01-15|    system|         NULL|       NULL|\n",
      "+--------------------+---------+------------+---------------------+------------------+--------------------+-----------------+-------------------+------------+------------------+------------+----------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lu.execute_query(\"SELECT * from config_extract_generation\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb6f03c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/31 08:51:08 WARN DeltaLog: Change in the table id detected while updating snapshot. \n",
      "Previous snapshot = Snapshot(path=file:/workspaces/ingen_fab/tmp/spark/Tables/config_extract_generation_details/_delta_log, version=1, metadata=Metadata(82c2237b-0ce7-44e7-9f94-c5248df68ee6,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"extract_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_generation_group\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_container\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_directory\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_timestamp_format\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_period_end_day\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_ordering\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_column_delimiter\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_row_delimiter\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_encoding\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_quote_character\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_escape_character\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_header\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_null_value\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_max_rows_per_file\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"output_format\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"is_trigger_file\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"trigger_file_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"is_compressed\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_type\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_level\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_file_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"fabric_lakehouse_path\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"created_date\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"created_by\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"modified_date\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"modified_by\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1753950105913)), logSegment=LogSegment(file:/workspaces/ingen_fab/tmp/spark/Tables/config_extract_generation_details/_delta_log,1,ArraySeq(DeprecatedRawLocalFileStatus{path=file:/workspaces/ingen_fab/tmp/spark/Tables/config_extract_generation_details/_delta_log/00000000000000000000.json; isDirectory=false; length=3422; replication=1; blocksize=33554432; modification_time=1753950115809; access_time=1753950116336; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}, DeprecatedRawLocalFileStatus{path=file:/workspaces/ingen_fab/tmp/spark/Tables/config_extract_generation_details/_delta_log/00000000000000000001.json; isDirectory=false; length=7967; replication=1; blocksize=33554432; modification_time=1753950116919; access_time=1753950117120; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}),org.apache.spark.sql.delta.EmptyCheckpointProvider$@4f46dba,1753950116919), checksumOpt=Some(VersionChecksum(Some(fe581cc3-2dc9-43a6-beb9-b591a365db5f),27871,3,None,None,1,1,None,Some(List()),Some(List()),Metadata(82c2237b-0ce7-44e7-9f94-c5248df68ee6,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"extract_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_generation_group\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_container\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_directory\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_timestamp_format\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_period_end_day\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_ordering\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_column_delimiter\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_row_delimiter\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_encoding\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_quote_character\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_escape_character\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_header\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_null_value\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_max_rows_per_file\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"output_format\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"is_trigger_file\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"trigger_file_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"is_compressed\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_type\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_level\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_file_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"fabric_lakehouse_path\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"created_date\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"created_by\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"modified_date\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"modified_by\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1753950105913)),Protocol(1,2),None,None,Some(List(AddFile(part-00004-d5a5e933-6242-4ffc-a338-2e5823b5b26d-c000.snappy.parquet,Map(),9294,1753950116900,false,{\"numRecords\":1,\"minValues\":{\"extract_name\":\"SAMPLE_PRODUCTS_LAKEHOUSE\",\"file_generation_group\":\"PRODUCT_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"products\",\"extract_file_name\":\"products_catalog\",\"extract_file_name_timestamp_format\":\"yyyyMMdd\",\"extract_file_name_extension\":\"parquet\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"parquet\",\"trigger_file_extension\":\".done\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"maxValues\":{\"extract_name\":\"SAMPLE_PRODUCTS_LAKEHOUSE\",\"file_generation_group\":\"PRODUCT_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"products\",\"extract_file_name\":\"products_catalog\",\"extract_file_name_timestamp_format\":\"yyyyMMdd\",\"extract_file_name_extension\":\"parquet\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"parquet\",\"trigger_file_extension\":\".done\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"nullCount\":{\"extract_name\":0,\"file_generation_group\":0,\"extract_container\":0,\"extract_directory\":0,\"extract_file_name\":0,\"extract_file_name_timestamp_format\":0,\"extract_file_name_period_end_day\":1,\"extract_file_name_extension\":0,\"extract_file_name_ordering\":0,\"file_properties_column_delimiter\":0,\"file_properties_row_delimiter\":0,\"file_properties_encoding\":0,\"file_properties_quote_character\":0,\"file_properties_escape_character\":0,\"file_properties_header\":0,\"file_properties_null_value\":0,\"file_properties_max_rows_per_file\":1,\"output_format\":0,\"is_trigger_file\":0,\"trigger_file_extension\":0,\"is_compressed\":0,\"compressed_type\":1,\"compressed_level\":1,\"compressed_file_name\":1,\"compressed_extension\":1,\"fabric_lakehouse_path\":1,\"created_date\":0,\"created_by\":0,\"modified_date\":1,\"modified_by\":1}},null,null,None,None,None), AddFile(part-00002-d4ae1eda-a5bc-489b-a25a-17da7c72aa6e-c000.snappy.parquet,Map(),9282,1753950116892,false,{\"numRecords\":1,\"minValues\":{\"extract_name\":\"SAMPLE_CUSTOMERS_LAKEHOUSE\",\"file_generation_group\":\"CUSTOMER_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"customers\",\"extract_file_name\":\"customers_lakehouse\",\"extract_file_name_timestamp_format\":\"yyyyMMdd_HHmmss\",\"extract_file_name_extension\":\"csv\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"csv\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"maxValues\":{\"extract_name\":\"SAMPLE_CUSTOMERS_LAKEHOUSE\",\"file_generation_group\":\"CUSTOMER_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"customers\",\"extract_file_name\":\"customers_lakehouse\",\"extract_file_name_timestamp_format\":\"yyyyMMdd_HHmmss\",\"extract_file_name_extension\":\"csv\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"csv\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"nullCount\":{\"extract_name\":0,\"file_generation_group\":0,\"extract_container\":0,\"extract_directory\":0,\"extract_file_name\":0,\"extract_file_name_timestamp_format\":0,\"extract_file_name_period_end_day\":1,\"extract_file_name_extension\":0,\"extract_file_name_ordering\":0,\"file_properties_column_delimiter\":0,\"file_properties_row_delimiter\":0,\"file_properties_encoding\":0,\"file_properties_quote_character\":0,\"file_properties_escape_character\":0,\"file_properties_header\":0,\"file_properties_null_value\":0,\"file_properties_max_rows_per_file\":1,\"output_format\":0,\"is_trigger_file\":0,\"trigger_file_extension\":1,\"is_compressed\":0,\"compressed_type\":1,\"compressed_level\":1,\"compressed_file_name\":1,\"compressed_extension\":1,\"fabric_lakehouse_path\":1,\"created_date\":0,\"created_by\":0,\"modified_date\":1,\"modified_by\":1}},null,null,None,None,None), AddFile(part-00006-35ee8dda-5590-414a-a49d-1c2e027e9caa-c000.snappy.parquet,Map(),9295,1753950116908,false,{\"numRecords\":1,\"minValues\":{\"extract_name\":\"SAMPLE_ORDERS_LAKEHOUSE\",\"file_generation_group\":\"ORDER_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"orders\",\"extract_file_name\":\"orders_daily\",\"extract_file_name_timestamp_format\":\"yyyyMMdd_HHmmss\",\"extract_file_name_extension\":\"csv\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"csv\",\"compressed_type\":\"GZIP\",\"compressed_level\":\"NORMAL\",\"compressed_extension\":\".gz\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"maxValues\":{\"extract_name\":\"SAMPLE_ORDERS_LAKEHOUSE\",\"file_generation_group\":\"ORDER_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"orders\",\"extract_file_name\":\"orders_daily\",\"extract_file_name_timestamp_format\":\"yyyyMMdd_HHmmss\",\"extract_file_name_extension\":\"csv\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"csv\",\"compressed_type\":\"GZIP\",\"compressed_level\":\"NORMAL\",\"compressed_extension\":\".gz\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"nullCount\":{\"extract_name\":0,\"file_generation_group\":0,\"extract_container\":0,\"extract_directory\":0,\"extract_file_name\":0,\"extract_file_name_timestamp_format\":0,\"extract_file_name_period_end_day\":1,\"extract_file_name_extension\":0,\"extract_file_name_ordering\":0,\"file_properties_column_delimiter\":0,\"file_properties_row_delimiter\":0,\"file_properties_encoding\":0,\"file_properties_quote_character\":0,\"file_properties_escape_character\":0,\"file_properties_header\":0,\"file_properties_null_value\":0,\"file_properties_max_rows_per_file\":1,\"output_format\":0,\"is_trigger_file\":0,\"trigger_file_extension\":1,\"is_compressed\":0,\"compressed_type\":0,\"compressed_level\":0,\"compressed_file_name\":1,\"compressed_extension\":0,\"fabric_lakehouse_path\":1,\"created_date\":0,\"created_by\":0,\"modified_date\":1,\"modified_by\":1}},null,null,None,None,None))))))\n",
      "New snapshot = Snapshot(path=file:/workspaces/ingen_fab/tmp/spark/Tables/config_extract_generation_details/_delta_log, version=1, metadata=Metadata(4e1d4d7e-21bb-4a7c-97ac-09fc5cd9b2aa,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"extract_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_generation_group\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_container\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_directory\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_timestamp_format\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_period_end_day\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_ordering\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_column_delimiter\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_row_delimiter\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_encoding\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_quote_character\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_escape_character\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_header\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_null_value\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_max_rows_per_file\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"output_format\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"is_trigger_file\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"trigger_file_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"is_compressed\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_type\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_level\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_file_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"fabric_lakehouse_path\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"force_single_file\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"created_date\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"created_by\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"modified_date\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"modified_by\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1753951721528)), logSegment=LogSegment(file:/workspaces/ingen_fab/tmp/spark/Tables/config_extract_generation_details/_delta_log,1,ArraySeq(DeprecatedRawLocalFileStatus{path=file:/workspaces/ingen_fab/tmp/spark/Tables/config_extract_generation_details/_delta_log/00000000000000000000.json; isDirectory=false; length=3510; replication=1; blocksize=33554432; modification_time=1753951731435; access_time=1753951731643; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}, DeprecatedRawLocalFileStatus{path=file:/workspaces/ingen_fab/tmp/spark/Tables/config_extract_generation_details/_delta_log/00000000000000000001.json; isDirectory=false; length=8039; replication=1; blocksize=33554432; modification_time=1753951732549; access_time=1753951732595; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}),org.apache.spark.sql.delta.EmptyCheckpointProvider$@4f46dba,1753951732549), checksumOpt=Some(VersionChecksum(Some(6708e170-b0ff-49e6-8b79-7b0603150ff2),28642,3,None,None,1,1,None,Some(List()),Some(List()),Metadata(4e1d4d7e-21bb-4a7c-97ac-09fc5cd9b2aa,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"extract_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_generation_group\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_container\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_directory\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_timestamp_format\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_period_end_day\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"extract_file_name_ordering\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_column_delimiter\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_row_delimiter\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_encoding\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_quote_character\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_escape_character\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_header\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_null_value\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"file_properties_max_rows_per_file\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"output_format\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"is_trigger_file\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"trigger_file_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"is_compressed\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_type\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_level\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_file_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"compressed_extension\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"fabric_lakehouse_path\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"force_single_file\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"created_date\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"created_by\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"modified_date\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"modified_by\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1753951721528)),Protocol(1,2),None,None,Some(List(AddFile(part-00006-4572af7d-520a-4ed9-bdac-47f070fa2301-c000.snappy.parquet,Map(),9552,1753951732522,false,{\"numRecords\":1,\"minValues\":{\"extract_name\":\"SAMPLE_ORDERS_LAKEHOUSE\",\"file_generation_group\":\"ORDER_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"orders\",\"extract_file_name\":\"orders_daily\",\"extract_file_name_timestamp_format\":\"yyyyMMdd_HHmmss\",\"extract_file_name_extension\":\"csv\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"csv\",\"compressed_type\":\"GZIP\",\"compressed_level\":\"NORMAL\",\"compressed_extension\":\".gz\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"maxValues\":{\"extract_name\":\"SAMPLE_ORDERS_LAKEHOUSE\",\"file_generation_group\":\"ORDER_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"orders\",\"extract_file_name\":\"orders_daily\",\"extract_file_name_timestamp_format\":\"yyyyMMdd_HHmmss\",\"extract_file_name_extension\":\"csv\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"csv\",\"compressed_type\":\"GZIP\",\"compressed_level\":\"NORMAL\",\"compressed_extension\":\".gz\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"nullCount\":{\"extract_name\":0,\"file_generation_group\":0,\"extract_container\":0,\"extract_directory\":0,\"extract_file_name\":0,\"extract_file_name_timestamp_format\":0,\"extract_file_name_period_end_day\":1,\"extract_file_name_extension\":0,\"extract_file_name_ordering\":0,\"file_properties_column_delimiter\":0,\"file_properties_row_delimiter\":0,\"file_properties_encoding\":0,\"file_properties_quote_character\":0,\"file_properties_escape_character\":0,\"file_properties_header\":0,\"file_properties_null_value\":0,\"file_properties_max_rows_per_file\":1,\"output_format\":0,\"is_trigger_file\":0,\"trigger_file_extension\":1,\"is_compressed\":0,\"compressed_type\":0,\"compressed_level\":0,\"compressed_file_name\":1,\"compressed_extension\":0,\"fabric_lakehouse_path\":1,\"force_single_file\":0,\"created_date\":0,\"created_by\":0,\"modified_date\":1,\"modified_by\":1}},null,null,None,None,None), AddFile(part-00004-3f3237d5-0e63-4103-834d-eff10648adee-c000.snappy.parquet,Map(),9551,1753951732512,false,{\"numRecords\":1,\"minValues\":{\"extract_name\":\"SAMPLE_PRODUCTS_LAKEHOUSE\",\"file_generation_group\":\"PRODUCT_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"products\",\"extract_file_name\":\"products_catalog\",\"extract_file_name_timestamp_format\":\"yyyyMMdd\",\"extract_file_name_extension\":\"parquet\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"parquet\",\"trigger_file_extension\":\".done\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"maxValues\":{\"extract_name\":\"SAMPLE_PRODUCTS_LAKEHOUSE\",\"file_generation_group\":\"PRODUCT_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"products\",\"extract_file_name\":\"products_catalog\",\"extract_file_name_timestamp_format\":\"yyyyMMdd\",\"extract_file_name_extension\":\"parquet\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"parquet\",\"trigger_file_extension\":\".done\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"nullCount\":{\"extract_name\":0,\"file_generation_group\":0,\"extract_container\":0,\"extract_directory\":0,\"extract_file_name\":0,\"extract_file_name_timestamp_format\":0,\"extract_file_name_period_end_day\":1,\"extract_file_name_extension\":0,\"extract_file_name_ordering\":0,\"file_properties_column_delimiter\":0,\"file_properties_row_delimiter\":0,\"file_properties_encoding\":0,\"file_properties_quote_character\":0,\"file_properties_escape_character\":0,\"file_properties_header\":0,\"file_properties_null_value\":0,\"file_properties_max_rows_per_file\":1,\"output_format\":0,\"is_trigger_file\":0,\"trigger_file_extension\":0,\"is_compressed\":0,\"compressed_type\":1,\"compressed_level\":1,\"compressed_file_name\":1,\"compressed_extension\":1,\"fabric_lakehouse_path\":1,\"force_single_file\":0,\"created_date\":0,\"created_by\":0,\"modified_date\":1,\"modified_by\":1}},null,null,None,None,None), AddFile(part-00002-150e6eab-24cd-4c2c-8ff4-ae1f25ca2c1b-c000.snappy.parquet,Map(),9539,1753951732537,false,{\"numRecords\":1,\"minValues\":{\"extract_name\":\"SAMPLE_CUSTOMERS_LAKEHOUSE\",\"file_generation_group\":\"CUSTOMER_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"customers\",\"extract_file_name\":\"customers_lakehouse\",\"extract_file_name_timestamp_format\":\"yyyyMMdd_HHmmss\",\"extract_file_name_extension\":\"csv\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"csv\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"maxValues\":{\"extract_name\":\"SAMPLE_CUSTOMERS_LAKEHOUSE\",\"file_generation_group\":\"CUSTOMER_DATA\",\"extract_container\":\"Files/extracts\",\"extract_directory\":\"customers\",\"extract_file_name\":\"customers_lakehouse\",\"extract_file_name_timestamp_format\":\"yyyyMMdd_HHmmss\",\"extract_file_name_extension\":\"csv\",\"extract_file_name_ordering\":1,\"file_properties_column_delimiter\":\",\",\"file_properties_row_delimiter\":\"\\\\n\",\"file_properties_encoding\":\"UTF-8\",\"file_properties_quote_character\":\"\\\"\",\"file_properties_escape_character\":\"\\\\\",\"file_properties_null_value\":\"\",\"output_format\":\"csv\",\"created_date\":\"2024-01-15\",\"created_by\":\"system\"},\"nullCount\":{\"extract_name\":0,\"file_generation_group\":0,\"extract_container\":0,\"extract_directory\":0,\"extract_file_name\":0,\"extract_file_name_timestamp_format\":0,\"extract_file_name_period_end_day\":1,\"extract_file_name_extension\":0,\"extract_file_name_ordering\":0,\"file_properties_column_delimiter\":0,\"file_properties_row_delimiter\":0,\"file_properties_encoding\":0,\"file_properties_quote_character\":0,\"file_properties_escape_character\":0,\"file_properties_header\":0,\"file_properties_null_value\":0,\"file_properties_max_rows_per_file\":1,\"output_format\":0,\"is_trigger_file\":0,\"trigger_file_extension\":1,\"is_compressed\":0,\"compressed_type\":1,\"compressed_level\":1,\"compressed_file_name\":1,\"compressed_extension\":1,\"fabric_lakehouse_path\":1,\"force_single_file\":0,\"created_date\":0,\"created_by\":0,\"modified_date\":1,\"modified_by\":1}},null,null,None,None,None)))))).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+-----------------+-----------------+-------------------+----------------------------------+--------------------------------+---------------------------+--------------------------+--------------------------------+-----------------------------+------------------------+-------------------------------+--------------------------------+----------------------+--------------------------+---------------------------------+-------------+---------------+----------------------+-------------+---------------+----------------+--------------------+--------------------+---------------------+-----------------+------------+----------+-------------+-----------+\n",
      "|        extract_name|file_generation_group|extract_container|extract_directory|  extract_file_name|extract_file_name_timestamp_format|extract_file_name_period_end_day|extract_file_name_extension|extract_file_name_ordering|file_properties_column_delimiter|file_properties_row_delimiter|file_properties_encoding|file_properties_quote_character|file_properties_escape_character|file_properties_header|file_properties_null_value|file_properties_max_rows_per_file|output_format|is_trigger_file|trigger_file_extension|is_compressed|compressed_type|compressed_level|compressed_file_name|compressed_extension|fabric_lakehouse_path|force_single_file|created_date|created_by|modified_date|modified_by|\n",
      "+--------------------+---------------------+-----------------+-----------------+-------------------+----------------------------------+--------------------------------+---------------------------+--------------------------+--------------------------------+-----------------------------+------------------------+-------------------------------+--------------------------------+----------------------+--------------------------+---------------------------------+-------------+---------------+----------------------+-------------+---------------+----------------+--------------------+--------------------+---------------------+-----------------+------------+----------+-------------+-----------+\n",
      "|SAMPLE_ORDERS_LAK...|           ORDER_DATA|   Files/extracts|           orders|       orders_daily|                   yyyyMMdd_HHmmss|                            NULL|                        csv|                         1|                               ,|                           \\n|                   UTF-8|                              \"|                               \\|                  true|                          |                             NULL|          csv|          false|                  NULL|         true|           GZIP|          NORMAL|                NULL|                 .gz|                 NULL|             true|  2024-01-15|    system|         NULL|       NULL|\n",
      "|SAMPLE_PRODUCTS_L...|         PRODUCT_DATA|   Files/extracts|         products|   products_catalog|                          yyyyMMdd|                            NULL|                    parquet|                         1|                               ,|                           \\n|                   UTF-8|                              \"|                               \\|                  true|                          |                             NULL|      parquet|           true|                 .done|        false|           NULL|            NULL|                NULL|                NULL|                 NULL|             true|  2024-01-15|    system|         NULL|       NULL|\n",
      "|SAMPLE_CUSTOMERS_...|        CUSTOMER_DATA|   Files/extracts|        customers|customers_lakehouse|                   yyyyMMdd_HHmmss|                            NULL|                        csv|                         1|                               ,|                           \\n|                   UTF-8|                              \"|                               \\|                  true|                          |                             NULL|          csv|          false|                  NULL|        false|           NULL|            NULL|                NULL|                NULL|                 NULL|             true|  2024-01-15|    system|         NULL|       NULL|\n",
      "+--------------------+---------------------+-----------------+-----------------+-------------------+----------------------------------+--------------------------------+---------------------------+--------------------------+--------------------------------+-----------------------------+------------------------+-------------------------------+--------------------------------+----------------------+--------------------------+---------------------------------+-------------+---------------+----------------------+-------------+---------------+----------------+--------------------+--------------------+---------------------+-----------------+------------+----------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lu.execute_query(\"SELECT * from config_extract_generation_details\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7e15957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------------+--------------+------------+------+----------------+----------------------+-------------------------+-----------------+-----------------+----------------+---------------+---------------+--------------+----------------+-----------------+-----------------------+----------------------+-------------------------------+--------------------+---------------------+-------------------------+-----------------+-----------------+-------------------+------------+------------------------+-------------+-------------+--------------------------+--------------------+------------+----------+\n",
      "|log_id|config_id|execution_id|job_start_time|job_end_time|status|source_file_path|source_file_size_bytes|source_file_modified_time|target_table_name|records_processed|records_inserted|records_updated|records_deleted|records_failed|source_row_count|staging_row_count|target_row_count_before|target_row_count_after|row_count_reconciliation_status|row_count_difference|data_read_duration_ms|staging_write_duration_ms|merge_duration_ms|total_duration_ms|avg_rows_per_second|data_size_mb|throughput_mb_per_second|error_message|error_details|execution_duration_seconds|spark_application_id|created_date|created_by|\n",
      "+------+---------+------------+--------------+------------+------+----------------+----------------------+-------------------------+-----------------+-----------------+----------------+---------------+---------------+--------------+----------------+-----------------+-----------------------+----------------------+-------------------------------+--------------------+---------------------+-------------------------+-----------------+-----------------+-------------------+------------+------------------------+-------------+-------------+--------------------------+--------------------+------------+----------+\n",
      "+------+---------+------------+--------------+------------+------+----------------+----------------------+-------------------------+-----------------+-----------------+----------------+---------------+---------------+--------------+----------------+-----------------+-----------------------+----------------------+-------------------------------+--------------------+---------------------+-------------------------+-----------------+-----------------+-------------------+------------+------------------------+-------------+-------------+--------------------------+--------------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lu.execute_query(\"SELECT * FROM log_flat_file_ingestion\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92053fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/14 06:03:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_HOME: /opt/bitnami/spark\n",
      "Expected config file: /opt/bitnami/spark/conf/spark-defaults.conf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-bf33e5a4-c086-4102-a9b8-7e3faf685181\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-bf33e5a4-c086-4102-a9b8-7e3faf685181\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-9723fbe2-13b2-4fd9-bda6-7c511cb1b88f\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-9723fbe2-13b2-4fd9-bda6-7c511cb1b88f\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-0aadf707-5118-4dbf-8dc8-6a4acdb9ce1a\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-0aadf707-5118-4dbf-8dc8-6a4acdb9ce1a\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-9f347010-be87-476f-a558-c28da585f837\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-9f347010-be87-476f-a558-c28da585f837\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-d02cf7ca-50f4-4047-b004-8a03a92911d2\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-d02cf7ca-50f4-4047-b004-8a03a92911d2\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-6d73eb59-91f0-4532-99aa-6b490d08e60e\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-6d73eb59-91f0-4532-99aa-6b490d08e60e\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-a86f5bd9-19ef-4a7e-8467-64b29d50badd\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-a86f5bd9-19ef-4a7e-8467-64b29d50badd\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-c5fd85d7-a06c-4010-a6e4-4a265de68245\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-c5fd85d7-a06c-4010-a6e4-4a265de68245\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-e9cd36ce-e32e-4973-be7e-040f48e3220c\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-e9cd36ce-e32e-4973-be7e-040f48e3220c\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-d66d3cc2-0a20-4d4d-b1d3-09656ab6f511\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-d66d3cc2-0a20-4d4d-b1d3-09656ab6f511\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o35.showString.\n: java.io.IOException: Failed to create a temp directory (under artifacts) after 10 attempts!\n\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:411)\n\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected config file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.environ.get(\u001b[33m'\u001b[39m\u001b[33mSPARK_HOME\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/conf/spark-defaults.conf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m df = spark.sql(\u001b[33m\"\u001b[39m\u001b[33mSELECT 1 as test\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/sql/classic/dataframe.py:285\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/sql/classic/dataframe.py:303\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    298\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    299\u001b[39m         messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvertical\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical).\u001b[34m__name__\u001b[39m},\n\u001b[32m    300\u001b[39m     )\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/lib/py4j.zip/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/errors/exceptions/captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/lib/py4j.zip/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o35.showString.\n: java.io.IOException: Failed to create a temp directory (under artifacts) after 10 attempts!\n\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:411)\n\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ConfigCheck\").getOrCreate()\n",
    "\n",
    "# Method 1: Check SPARK_HOME and config file locations\n",
    "print(f\"SPARK_HOME: {os.environ.get('SPARK_HOME', 'Not set')}\")\n",
    "print(\n",
    "    f\"Expected config file: {os.environ.get('SPARK_HOME', '')}/conf/spark-defaults.conf\"\n",
    ")\n",
    "df = spark.sql(\"SELECT 1 as test\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20d901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = (\n",
    "    SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5788119e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file from: synthetic_data/single/retail_oltp_small/customers.csv/\n",
      "First part of path: synthetic_data\n",
      "Reading file from: file:////workspaces/ingen_fab/tmp/spark/Files/synthetic_data/single/retail_oltp_small/customers.csv/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lu.read_file(\n",
    "    file_path=\"synthetic_data/single/retail_oltp_small/customers.csv/\",\n",
    "    file_format=\"csv\",\n",
    ").count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i4f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
