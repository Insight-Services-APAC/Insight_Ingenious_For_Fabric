{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f54f3fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NotebookUtils not available, assumed running in local mode.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "# Check if running in Fabric environment\n",
    "if \"notebookutils\" in sys.modules:\n",
    "    import sys\n",
    "    \n",
    "    notebookutils.fs.mount(\"abfss://{{varlib:config_workspace_name}}@onelake.dfs.fabric.microsoft.com/{{varlib:config_lakehouse_name}}.Lakehouse/Files/\", \"/config_files\")  # type: ignore # noqa: F821\n",
    "    mount_path = notebookutils.fs.getMountPath(\"/config_files\")  # type: ignore # noqa: F821\n",
    "    \n",
    "    run_mode = \"fabric\"\n",
    "    sys.path.insert(0, mount_path)\n",
    "\n",
    "    \n",
    "    # PySpark environment - spark session should be available\n",
    "    \n",
    "else:\n",
    "    print(\"NotebookUtils not available, assumed running in local mode.\")\n",
    "    from ingen_fab.python_libs.pyspark.notebook_utils_abstraction import (\n",
    "        NotebookUtilsFactory,\n",
    "    )\n",
    "    notebookutils = NotebookUtilsFactory.create_instance()\n",
    "        \n",
    "    spark = None\n",
    "    \n",
    "    mount_path = None\n",
    "    run_mode = \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d662654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting...ingen_fab.python_libs\n",
      "deleting...ingen_fab.python_libs.interfaces\n",
      "deleting...ingen_fab.python_libs.interfaces.ddl_utils_interface\n",
      "deleting...ingen_fab.python_libs.common\n",
      "deleting...ingen_fab.python_libs.common.config_utils\n",
      "deleting...ingen_fab.python_libs.interfaces.data_store_interface\n",
      "deleting...ingen_fab.python_libs.pyspark.lakehouse_utils\n",
      "deleting...ingen_fab.python_libs.pyspark.ddl_utils\n",
      "deleting...ingen_fab.python_libs.pyspark.notebook_utils_abstraction\n",
      "deleting...ingen_fab.python_libs.pyspark.parquet_load_utils\n",
      "deleting...ingen_fab.python_libs.pyspark\n",
      "deleting...ingen_fab\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "\n",
    "def load_python_modules_from_path(base_path: str, relative_files: list[str], max_chars: int = 1_000_000_000):\n",
    "    \"\"\"\n",
    "    Executes Python files from a Fabric-mounted file path using notebookutils.fs.head.\n",
    "    \n",
    "    Args:\n",
    "        base_path (str): The root directory where modules are located.\n",
    "        relative_files (list[str]): List of relative paths to Python files (from base_path).\n",
    "        max_chars (int): Max characters to read from each file (default: 1,000,000).\n",
    "    \"\"\"\n",
    "    success_files = []\n",
    "    failed_files = []\n",
    "\n",
    "    for relative_path in relative_files:\n",
    "        full_path = f\"file:{base_path}/{relative_path}\"\n",
    "        try:\n",
    "            print(f\"üîÑ Loading: {full_path}\")\n",
    "            code = notebookutils.fs.head(full_path, max_chars)\n",
    "            exec(code, globals())  # Use globals() to share context across modules\n",
    "            success_files.append(relative_path)\n",
    "        except Exception as e:\n",
    "            failed_files.append(relative_path)\n",
    "            print(f\"‚ùå Error loading {relative_path}\")\n",
    "\n",
    "    print(\"\\n‚úÖ Successfully loaded:\")\n",
    "    for f in success_files:\n",
    "        print(f\" - {f}\")\n",
    "\n",
    "    if failed_files:\n",
    "        print(\"\\n‚ö†Ô∏è Failed to load:\")\n",
    "        for f in failed_files:\n",
    "            print(f\" - {f}\")\n",
    "\n",
    "def clear_module_cache(prefix: str):\n",
    "    \"\"\"Clear module cache for specified prefix\"\"\"\n",
    "    for mod in list(sys.modules):\n",
    "        if mod.startswith(prefix):\n",
    "            print(\"deleting...\" + mod)\n",
    "            del sys.modules[mod]\n",
    "\n",
    "# Always clear the module cache - We may remove this once the libs are stable\n",
    "clear_module_cache(\"ingen_fab.python_libs\")\n",
    "clear_module_cache(\"ingen_fab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9f94d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if run_mode == \"local\":\n",
    "    from ingen_fab.python_libs.common.config_utils import get_configs_as_object\n",
    "    from ingen_fab.python_libs.pyspark.ddl_utils import ddl_utils\n",
    "    from ingen_fab.python_libs.pyspark.lakehouse_utils import lakehouse_utils\n",
    "    from ingen_fab.python_libs.pyspark.notebook_utils_abstraction import (\n",
    "        NotebookUtilsFactory,\n",
    "    )\n",
    "    notebookutils = NotebookUtilsFactory.create_instance() \n",
    "else:\n",
    "    files_to_load = [\n",
    "        \"ingen_fab/python_libs/common/config_utils.py\",\n",
    "        \"ingen_fab/python_libs/pyspark/lakehouse_utils.py\",\n",
    "        \"ingen_fab/python_libs/pyspark/ddl_utils.py\",\n",
    "        \"ingen_fab/python_libs/pyspark/notebook_utils_abstraction.py\"\n",
    "    ]\n",
    "\n",
    "    load_python_modules_from_path(mount_path, files_to_load)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "223d1dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No active Spark session found, creating a new one with Delta support.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /root/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /root/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-80cf91b2-b22b-45c8-981c-de89abcc9c55;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      ":: resolution report :: resolve 183ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-80cf91b2-b22b-45c8-981c-de89abcc9c55\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/6ms)\n",
      "25/07/14 13:46:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/14 13:46:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/07/14 13:46:59 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "lu = lakehouse_utils(\n",
    "    target_workspace_id=get_configs_as_object().config_workspace_id,\n",
    "    target_lakehouse_id=get_configs_as_object().config_lakehouse_id    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e4b5c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing Spark session, reusing it.\n",
      "Path file:////workspaces/ingen_fab/tmp/spark/Tables/ddl_script_executions is not a Delta table.\n",
      "Creating execution log table at file:////workspaces/ingen_fab/tmp/spark/Tables/ddl_script_executions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö† Alert: Registering table 'ddl_script_executions' in the Hive catalog for local Spark.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/14 13:47:13 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`default`.`ddl_script_executions` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "25/07/14 13:47:13 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/07/14 13:47:13 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "du = ddl_utils(\n",
    "    target_workspace_id=get_configs_as_object().config_workspace_id,\n",
    "    target_lakehouse_id=get_configs_as_object().config_lakehouse_id    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98163c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lu.spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0335314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/14 13:32:08 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`default`.`test_table` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "25/07/14 13:32:08 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/07/14 13:32:08 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lu.execute_query(\"CREATE TABLE IF NOT EXISTS test_table (id INT, name STRING) USING DELTA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62359b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/14 13:48:36 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------+--------------------+\n",
      "|           script_id|         script_name|execution_status|         update_date|\n",
      "+--------------------+--------------------+----------------+--------------------+\n",
      "|b8c83c87-36d2-46a...|ddl_script_execut...|         Success|2025-07-14 13:47:...|\n",
      "+--------------------+--------------------+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lu.execute_query(\"SELECT * from ddl_script_executions\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "971b15fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='ddl_script_executions', catalog='spark_catalog', namespace=['default'], description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='test_table', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lu.list_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92053fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/14 06:03:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_HOME: /opt/bitnami/spark\n",
      "Expected config file: /opt/bitnami/spark/conf/spark-defaults.conf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-bf33e5a4-c086-4102-a9b8-7e3faf685181\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-bf33e5a4-c086-4102-a9b8-7e3faf685181\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-9723fbe2-13b2-4fd9-bda6-7c511cb1b88f\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-9723fbe2-13b2-4fd9-bda6-7c511cb1b88f\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-0aadf707-5118-4dbf-8dc8-6a4acdb9ce1a\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-0aadf707-5118-4dbf-8dc8-6a4acdb9ce1a\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-9f347010-be87-476f-a558-c28da585f837\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-9f347010-be87-476f-a558-c28da585f837\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-d02cf7ca-50f4-4047-b004-8a03a92911d2\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-d02cf7ca-50f4-4047-b004-8a03a92911d2\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-6d73eb59-91f0-4532-99aa-6b490d08e60e\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-6d73eb59-91f0-4532-99aa-6b490d08e60e\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-a86f5bd9-19ef-4a7e-8467-64b29d50badd\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-a86f5bd9-19ef-4a7e-8467-64b29d50badd\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-c5fd85d7-a06c-4010-a6e4-4a265de68245\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-c5fd85d7-a06c-4010-a6e4-4a265de68245\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-e9cd36ce-e32e-4973-be7e-040f48e3220c\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-e9cd36ce-e32e-4973-be7e-040f48e3220c\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/07/14 06:03:23 ERROR JavaUtils: Failed to create directory artifacts/spark-d66d3cc2-0a20-4d4d-b1d3-09656ab6f511\n",
      "java.nio.file.AccessDeniedException: /workspaces/ingen_fab/artifacts/spark-d66d3cc2-0a20-4d4d-b1d3-09656ab6f511\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)\n",
      "\tat java.base/java.nio.file.Files.createDirectories(Unknown Source)\n",
      "\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:416)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n",
      "\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n",
      "\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n",
      "\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o35.showString.\n: java.io.IOException: Failed to create a temp directory (under artifacts) after 10 attempts!\n\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:411)\n\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected config file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.environ.get(\u001b[33m'\u001b[39m\u001b[33mSPARK_HOME\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/conf/spark-defaults.conf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m df = spark.sql(\u001b[33m\"\u001b[39m\u001b[33mSELECT 1 as test\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/sql/classic/dataframe.py:285\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/sql/classic/dataframe.py:303\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    298\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    299\u001b[39m         messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvertical\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical).\u001b[34m__name__\u001b[39m},\n\u001b[32m    300\u001b[39m     )\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/lib/py4j.zip/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/errors/exceptions/captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/lib/py4j.zip/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o35.showString.\n: java.io.IOException: Failed to create a temp directory (under artifacts) after 10 attempts!\n\tat org.apache.spark.network.util.JavaUtils.createDirectory(JavaUtils.java:411)\n\tat org.apache.spark.util.SparkFileUtils.createDirectory(SparkFileUtils.scala:95)\n\tat org.apache.spark.util.SparkFileUtils.createDirectory$(SparkFileUtils.scala:94)\n\tat org.apache.spark.util.Utils$.createDirectory(Utils.scala:99)\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:249)\n\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory$lzycompute(ArtifactManager.scala:468)\n\tat org.apache.spark.sql.artifact.ArtifactManager$.artifactRootDirectory(ArtifactManager.scala:467)\n\tat org.apache.spark.sql.artifact.ArtifactManager.artifactRootPath(ArtifactManager.scala:60)\n\tat org.apache.spark.sql.artifact.ArtifactManager.<init>(ArtifactManager.scala:70)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$artifactManager$2(BaseSessionStateBuilder.scala:395)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.artifactManager(BaseSessionStateBuilder.scala:395)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$6(BaseSessionStateBuilder.scala:433)\n\tat org.apache.spark.sql.internal.SessionState.artifactManager$lzycompute(SessionState.scala:109)\n\tat org.apache.spark.sql.internal.SessionState.artifactManager(SessionState.scala:109)\n\tat org.apache.spark.sql.classic.SparkSession.artifactManager(SparkSession.scala:233)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:123)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ConfigCheck\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Method 1: Check SPARK_HOME and config file locations\n",
    "print(f\"SPARK_HOME: {os.environ.get('SPARK_HOME', 'Not set')}\")\n",
    "print(f\"Expected config file: {os.environ.get('SPARK_HOME', '')}/conf/spark-defaults.conf\")\n",
    "df = spark.sql(\"SELECT 1 as test\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20d901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = (\n",
    "    SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ingen_fab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
