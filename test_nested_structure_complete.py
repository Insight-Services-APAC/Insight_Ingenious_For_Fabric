#!/usr/bin/env python3
"""
Complete end-to-end test for hierarchical nested structure support
Tests the full processing pipeline including parallel execution
"""
import sys
import os
from datetime import datetime

# Add the project root to the path
sys.path.insert(0, '/workspaces/ingen_fab')

from ingen_fab.python_libs.interfaces.flat_file_ingestion_interface import FlatFileIngestionConfig
from ingen_fab.python_libs.pyspark.flat_file_ingestion_pyspark import (
    PySparkFlatFileDiscovery,
    PySparkFlatFileProcessor,
    PySparkFlatFileLogging,
    PySparkFlatFileIngestionOrchestrator
)
from ingen_fab.python_libs.pyspark.lakehouse_utils import lakehouse_utils

def create_complete_nested_structure_configs():
    """Create comprehensive test configurations for hierarchical nested structure"""
    configs = []
    
    # Configuration 1: orders table (execution group 1 - parallel with customers)
    configs.append(FlatFileIngestionConfig(
        config_id="nested_orders_full_test",
        config_name="Full Test - Nested Orders",
        source_file_path="synthetic_data/retail_oltp_incremental_nested_parquet",
        source_file_format="parquet",
        target_workspace_id="test_workspace",
        target_datastore_id="test_lakehouse",
        target_datastore_type="lakehouse",
        target_schema_name="raw",
        target_table_name="nested_orders_test",
        execution_group=1,  # Parallel group
        active_yn="Y",
        
        # Enable hierarchical nested structure
        import_pattern="date_partitioned",
        hierarchical_date_structure=True,
        table_subfolder="orders",
        date_partition_format="YYYY/MM/DD",
        date_range_start="2024-01-01",
        date_range_end="2025-01-02",  # Just 2 days for faster testing
        skip_existing_dates=False,
        source_is_folder=True,  # This is key - tells it to read folders
        write_mode="overwrite"
    ))
    
    # Configuration 2: customers table (execution group 1 - parallel with orders)
    configs.append(FlatFileIngestionConfig(
        config_id="nested_customers_full_test",
        config_name="Full Test - Nested Customers",
        source_file_path="synthetic_data/retail_oltp_incremental_nested_parquet",
        source_file_format="parquet",
        target_workspace_id="test_workspace",
        target_datastore_id="test_lakehouse",
        target_datastore_type="lakehouse",
        target_schema_name="raw",
        target_table_name="nested_customers_test",
        execution_group=1,  # Parallel group
        active_yn="Y",
        
        # Enable hierarchical nested structure
        import_pattern="date_partitioned",
        hierarchical_date_structure=True,
        table_subfolder="customers",
        date_partition_format="YYYY/MM/DD",
        date_range_start="2024-01-01",
        date_range_end="2024-01-02",
        skip_existing_dates=False,
        source_is_folder=True,
        write_mode="overwrite"
    ))
    
    # Configuration 3: products table (execution group 2 - sequential after group 1)
    configs.append(FlatFileIngestionConfig(
        config_id="nested_products_full_test",
        config_name="Full Test - Nested Products",
        source_file_path="synthetic_data/retail_oltp_incremental_nested_parquet",
        source_file_format="parquet",
        target_workspace_id="test_workspace",
        target_datastore_id="test_lakehouse",
        target_datastore_type="lakehouse",
        target_schema_name="raw",
        target_table_name="nested_products_test",
        execution_group=2,  # Sequential group - runs after group 1
        active_yn="Y",
        
        # Enable hierarchical nested structure
        import_pattern="date_partitioned",
        hierarchical_date_structure=True,
        table_subfolder="products",
        date_partition_format="YYYY/MM/DD",
        date_range_start="2024-01-01",
        date_range_end="2025-01-01",  # Just 1 day for products
        skip_existing_dates=False,
        source_is_folder=True,
        write_mode="overwrite"
    ))
    
    return configs

def test_nested_discovery_and_processing():
    """Test the complete hierarchical nested structure processing pipeline"""
    print("üöÄ Complete Hierarchical Nested Structure Test")
    print("=" * 70)
    
    # Create test configurations
    configs = create_complete_nested_structure_configs()
    
    # Set up lakehouse utils
    lakehouse = lakehouse_utils("test_workspace", "test_lakehouse")
    
    # Create service instances
    discovery = PySparkFlatFileDiscovery(lakehouse)
    processor = PySparkFlatFileProcessor(lakehouse.spark, lakehouse)
    logging = PySparkFlatFileLogging(lakehouse)
    
    # Create orchestrator
    orchestrator = PySparkFlatFileIngestionOrchestrator(discovery, processor, logging)
    
    print(f"üìã Testing {len(configs)} nested structure configurations:")
    for i, config in enumerate(configs, 1):
        print(f"  {i}. {config.config_name} (Group {config.execution_group}, Table: {config.table_subfolder})")
    
    print("\n" + "=" * 70)
    print("üîç Phase 1: File Discovery Testing")
    print("=" * 70)
    
    # Test discovery for each configuration
    all_discovered_files = {}
    for config in configs:
        print(f"\n--- Testing Discovery: {config.config_name} ---")
        discovered_files = discovery.discover_files(config)
        all_discovered_files[config.config_id] = discovered_files
        
        print(f"üìÅ Discovery Results for {config.table_subfolder}:")
        print(f"  - Found {len(discovered_files)} date/table combinations")
        
        for j, file_result in enumerate(discovered_files, 1):
            print(f"  {j}. Path: {file_result.file_path}")
            print(f"     Date: {file_result.date_partition}")
            print(f"     Folder: {file_result.folder_name}")
    
    print("\n" + "=" * 70)
    print("üîÑ Phase 2: Full Processing Pipeline Test")
    print("=" * 70)
    
    # Test the complete processing pipeline
    execution_id = f"nested_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    try:
        # Process all configurations using the orchestrator
        print(f"üöÄ Starting orchestrated processing with execution_id: {execution_id}")
        results = orchestrator.process_configurations(configs, execution_id)
        
        print(f"\n‚úÖ Processing completed!")
        print(f"üìä Results Summary:")
        print(f"  - Total configurations processed: {len(results)}")
        
        # Analyze results by status
        successful = [r for r in results if r.get('status') == 'completed']
        failed = [r for r in results if r.get('status') == 'failed']
        
        print(f"  - Successful: {len(successful)}")
        print(f"  - Failed: {len(failed)}")
        
        # Show detailed results
        for result in results:
            status_icon = "‚úÖ" if result.get('status') == 'completed' else "‚ùå"
            print(f"\n  {status_icon} {result.get('config_name', 'Unknown')}")
            print(f"      Status: {result.get('status', 'unknown')}")
            
            if 'metrics' in result and result['metrics']:
                metrics = result['metrics']
                if hasattr(metrics, 'records_processed'):
                    print(f"      Records Processed: {metrics.records_processed:,}")
                if hasattr(metrics, 'total_duration_ms'):
                    print(f"      Duration: {metrics.total_duration_ms:,}ms")
            
            if result.get('errors'):
                print(f"      Errors: {result['errors']}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Processing failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_individual_table_processing():
    """Test processing a single table configuration"""
    print("\n" + "=" * 70)
    print("üîç Phase 3: Individual Table Processing Test")
    print("=" * 70)
    
    # Test single table processing
    config = FlatFileIngestionConfig(
        config_id="single_table_test",
        config_name="Single Table Test - Order Items",
        source_file_path="synthetic_data/retail_oltp_incremental_nested_parquet",
        source_file_format="parquet",
        target_workspace_id="test_workspace",
        target_datastore_id="test_lakehouse",
        target_datastore_type="lakehouse",
        target_schema_name="raw",
        target_table_name="nested_order_items_single_test",
        execution_group=1,
        active_yn="Y",
        
        # Enable hierarchical nested structure
        import_pattern="date_partitioned",
        hierarchical_date_structure=True,
        table_subfolder="order_items",
        date_partition_format="YYYY/MM/DD",
        date_range_start="2024-01-01",
        date_range_end="2024-01-01",  # Just one day
        skip_existing_dates=False,
        source_is_folder=True,
        write_mode="overwrite"
    )
    
    # Set up services
    lakehouse = lakehouse_utils("test_workspace", "test_lakehouse")
    discovery = PySparkFlatFileDiscovery(lakehouse)
    processor = PySparkFlatFileProcessor(lakehouse.spark, lakehouse)
    logging = PySparkFlatFileLogging(lakehouse)
    orchestrator = PySparkFlatFileIngestionOrchestrator(discovery, processor, logging)
    
    # Process single configuration
    execution_id = f"single_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    try:
        print(f"üîÑ Processing single table: {config.table_subfolder}")
        result = orchestrator.process_configuration(config, execution_id)
        
        status_icon = "‚úÖ" if result.get('status') == 'completed' else "‚ùå"
        print(f"\n{status_icon} Single table processing result:")
        print(f"  Status: {result.get('status', 'unknown')}")
        
        if 'metrics' in result and result['metrics']:
            metrics = result['metrics']
            if hasattr(metrics, 'records_processed'):
                print(f"  Records Processed: {metrics.records_processed:,}")
        
        return result.get('status') == 'completed'
        
    except Exception as e:
        print(f"‚ùå Single table processing failed: {e}")
        return False

def main():
    """Main test function"""
    print("üß™ Comprehensive Hierarchical Nested Structure Testing")
    print("=" * 70)
    print("üìã Test Overview:")
    print("  ‚Ä¢ Discovery functionality for hierarchical YYYY/MM/DD/table_name structure")  
    print("  ‚Ä¢ Parallel processing within execution groups")
    print("  ‚Ä¢ Sequential processing between execution groups")
    print("  ‚Ä¢ Full end-to-end processing pipeline")
    print("  ‚Ä¢ Individual table processing")
    print("=" * 70)
    
    # Run all tests
    test_results = []
    
    # Test 1: Discovery and complete processing
    test_results.append(test_nested_discovery_and_processing())
    
    # Test 2: Individual table processing  
    test_results.append(test_individual_table_processing())
    
    # Final summary
    print("\n" + "=" * 70)
    print("üìä Final Test Summary")
    print("=" * 70)
    
    passed_tests = sum(test_results)
    total_tests = len(test_results)
    
    print(f"‚úÖ Tests Passed: {passed_tests}/{total_tests}")
    
    if passed_tests == total_tests:
        print("üéâ All tests passed! Hierarchical nested structure support is working correctly.")
        print("\nüìñ Key Features Validated:")
        print("  ‚úì YYYY/MM/DD/table_name hierarchical discovery")
        print("  ‚úì Table-specific filtering within date partitions")
        print("  ‚úì Date range filtering")
        print("  ‚úì Parallel processing within execution groups")
        print("  ‚úì Sequential processing between execution groups")
        print("  ‚úì Folder reading of Spark part-* files")
        print("  ‚úì Full data processing pipeline")
        print("  ‚úì Backward compatibility with existing flat structures")
        
        print("\nüîß Usage Example:")
        print("  ‚Ä¢ Set hierarchical_date_structure=True")
        print("  ‚Ä¢ Set table_subfolder='your_table_name'")
        print("  ‚Ä¢ Set date_partition_format='YYYY/MM/DD'")
        print("  ‚Ä¢ Set source_is_folder=True")
        print("  ‚Ä¢ Configure execution groups for parallelization")
    else:
        print("‚ùå Some tests failed. Please review the output above.")
    
    return passed_tests == total_tests

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)