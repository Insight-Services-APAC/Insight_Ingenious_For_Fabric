{%- import 'nb_macros.py.jinja' as macros -%}
{%- include "nb_header.py.jinja" %}

{{macros.python_cell_with_heading("## ðŸ“¦ Inject Reusable Classes and Functions")}}

import sys

if "notebookutils" in sys.modules:
    import sys
    {% raw %}
    notebookutils.fs.mount("abfss://{{varlib:config_workspace_id}}@onelake.dfs.fabric.microsoft.com/{{varlib:config_workspace_name}}.Lakehouse/Files/", "/config_files")  # type: ignore # noqa: F821
    new_Path = notebookutils.fs.getMountPath("/config_files")  # type: ignore # noqa: F821
    {% endraw %}
    sys.path.insert(0, new_Path)
else:
    print("NotebookUtils not available, skipping config files mount.")
    from ingen_fab.python_libs.pyspark.notebook_utils_abstraction import (
        NotebookUtilsFactory,
    )
    notebookutils = NotebookUtilsFactory.create_instance()


{{macros.python_cell_with_heading("## Instantiate the Helper Classes")}}



from ingen_fab.python_libs.common.config_utils import ConfigUtils
from ingen_fab.python_libs.pyspark.ddl_utils import ddl_utils
from ingen_fab.python_libs.pyspark.lakehouse_utils import lakehouse_utils

target_lakehouse_config_prefix = "{{target_lakehouse_config_prefix | required }}"

config_utils = ConfigUtils()
configs: ConfigUtils.ConfigsObject = config_utils.get_configs_as_object
config_lakehouse = lakehouse_utils(
    target_workspace_id=configs.edw_workspace_id,
    target_lakehouse_id=configs.edw_lakehouse_id
)

{{macros.python_cell_with_heading("## Run the lakehouse DDL Notebooks")}}

# Import required libraries
import sys
from datetime import datetime

# Initialize variables
workspace_id = mssparkutils.runtime.context.get("currentWorkspaceId")
success_count = 0
failed_notebook = None
start_time = datetime.now()

# Define execution function
def execute_notebook(notebook_name, index, total, timeout_seconds=3600):
    """Execute a single notebook and handle success/failure."""
    global success_count
    
    try:
        
        print(f"{'='*60}")
        print(f"Executing notebook {index}/{total}:{notebook_name}")
        print(f"{'='*60}")
        
        # Run the notebook
        result = mssparkutils.notebook.run(
            notebook_name,
            timeout_seconds,
            params
        )
        
        if (result == 'success'):
            success_count += 1
        else: 
            raise Exception({"result": result}) 

        print(f"âœ“ Successfully executed: {notebook_name}")
        print(f"Exit value: {result}")
        return True
        
    except Exception as e:
        print(f"âœ— Failed to execute: {notebook_name}")
        print(f"Error: {str(e)}")
        
        # Stop execution on failure
        error_msg = f"Orchestration stopped due to failure in notebook: {notebook_name}. Error: {str(e)}"
        mssparkutils.notebook.exit(error_msg)
        return False

print(f"Starting orchestration for {{ lakehouse_name }} lakehouse")
print(f"Start time: {start_time}")
print(f"Workspace ID: {workspace_id}")
print(f"Total notebooks to execute: {{ total_notebooks }}")
print("="*60)

{%- for notebook in notebooks %}
execute_notebook("{{ notebook.name }}", {{ notebook.index }}, {{ notebook.total }})
{%- endfor %}

# Final Summary
end_time = datetime.now()
duration = end_time - start_time

print(f"{'='*60}")
print(f"Orchestration Complete!")
print(f"{'='*60}")
print(f"End time: {end_time}")
print(f"Duration: {duration}")
print(f"Total notebooks: {{ total_notebooks }}")
print(f"Successfully executed: {success_count}")
print(f"Failed: {{ total_notebooks }} - {success_count}")

if success_count == {{ total_notebooks }}:
    print("âœ“ All notebooks executed successfully!")
    mssparkutils.notebook.exit("success")
else:
    print(f"âœ— Orchestration completed with failures")
    mssparkutils.notebook.exit(f"Orchestration completed with {success_count}/{{ total_notebooks }} successful executions")

{%include "nb_footer.py.jinja" %}