{%- import 'shared/notebook/macros/notebook_macros.py.jinja' as macros -%}
# Fabric notebook source

# METADATA ********************

# META {
# META   "kernel_info": {
# META     "name": "synapse_pyspark",
# META     "display_name": "Synapse PySpark"
# META   },
# META   "language_info": {
# META     "name": "python",
# META     "language_group": "synapse_pyspark"
# META   }
# META }

{{ macros.parameters_cell() }}

# Default parameters
execution_group = ""  # Optional: filter by execution group, empty = process all
environment = "development"
run_type = "FULL"  # FULL or INCREMENTAL

{{ macros.python_cell_with_heading("## ğŸ“¤ Extract Generation Notebook (Lakehouse)") }}

# This notebook generates extract files from lakehouse tables and views
# based on configuration metadata, supporting multiple formats and compression options.

{% set runtime_type = "pyspark" %}
{% set language_group = "synapse_pyspark" %}
{% set include_ddl_utils = true %}
{% include 'shared/notebook/environment/library_loader.py.jinja' %}

{{ macros.python_cell_with_heading("## ğŸ”§ Load Configuration and Initialize") }}
{% include 'shared/notebook/environment/config_loader.py.jinja' %}

# Additional imports for extract generation
import uuid
import json
import io
import gzip
import zipfile
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import pandas as pd
from pyspark.sql import functions as F
from pyspark.sql.types import *

# Import lakehouse_utils and sql_templates for lakehouse operations
if run_mode == "local":
    from ingen_fab.python_libs.pyspark.lakehouse_utils import lakehouse_utils
    from ingen_fab.python_libs.python.sql_templates import SQLTemplates
else:
    files_to_load = ["ingen_fab/python_libs/pyspark/lakehouse_utils.py", "ingen_fab/python_libs/python/sql_templates.py"]
    load_python_modules_from_path(mount_path, files_to_load)

import time

run_id = str(uuid.uuid4())
start_time = datetime.utcnow()

print(f"Run ID: {run_id}")
print(f"Execution Group Filter: {execution_group or 'ALL'}")
print(f"Environment: {environment}")
print(f"Run Type: {run_type}")

{{ macros.python_cell_with_heading("## ğŸ“‹ Load Extract Configuration") }}

class ExtractConfiguration:
    """Configuration class for extract generation"""
    
    def __init__(self, config_row, details_row):
        # Main configuration
        self.extract_name = config_row["extract_name"]
        self.is_active = config_row["is_active"]
        self.trigger_name = config_row.get("trigger_name")
        self.extract_pipeline_name = config_row.get("extract_pipeline_name")
        
        # Source configuration
        self.extract_table_name = config_row.get("extract_table_name")
        self.extract_table_schema = config_row.get("extract_table_schema")
        self.extract_view_name = config_row.get("extract_view_name")
        self.extract_view_schema = config_row.get("extract_view_schema")
        
        # Note: Stored procedures not supported in lakehouse
        self.extract_sp_name = None
        self.extract_sp_schema = None
        
        # Validation configuration (simplified for lakehouse)
        self.validation_table_sp_name = None
        self.validation_table_sp_schema = None
        
        # Load configuration
        self.is_full_load = config_row["is_full_load"]
        self.execution_group = config_row.get("execution_group")
        
        # File details configuration
        self.file_generation_group = details_row.get("file_generation_group")
        self.extract_container = details_row.get("extract_container", "extracts")
        self.extract_directory = details_row.get("extract_directory", "")
        
        # File naming
        self.extract_file_name = details_row.get("extract_file_name", self.extract_name)
        self.extract_file_name_timestamp_format = details_row.get("extract_file_name_timestamp_format")
        self.extract_file_name_period_end_day = details_row.get("extract_file_name_period_end_day")
        self.extract_file_name_extension = details_row.get("extract_file_name_extension", "csv")
        self.extract_file_name_ordering = details_row.get("extract_file_name_ordering", 1)
        
        # File properties
        self.file_properties_column_delimiter = details_row.get("file_properties_column_delimiter", ",")
        self.file_properties_row_delimiter = details_row.get("file_properties_row_delimiter", "\\n")
        self.file_properties_encoding = details_row.get("file_properties_encoding", "UTF-8")
        self.file_properties_quote_character = details_row.get("file_properties_quote_character", '"')
        self.file_properties_escape_character = details_row.get("file_properties_escape_character", "\\")
        self.file_properties_header = details_row.get("file_properties_header", True)
        self.file_properties_null_value = details_row.get("file_properties_null_value", "")
        self.file_properties_max_rows_per_file = details_row.get("file_properties_max_rows_per_file")
        
        # Output format
        self.output_format = details_row.get("output_format", "csv")
        
        # Trigger file
        self.is_trigger_file = details_row.get("is_trigger_file", False)
        self.trigger_file_extension = details_row.get("trigger_file_extension", ".done")
        
        # Compression
        self.is_compressed = details_row.get("is_compressed", False)
        self.compressed_type = details_row.get("compressed_type")
        self.compressed_level = details_row.get("compressed_level", "NORMAL")
        self.compressed_file_name = details_row.get("compressed_file_name")
        self.compressed_extension = details_row.get("compressed_extension", ".zip")
        
        # Fabric paths
        self.fabric_lakehouse_path = details_row.get("fabric_lakehouse_path")
        
    def get_source_query(self) -> Tuple[str, str]:
        """Get the Spark SQL query to extract data and the source type"""
        if self.extract_table_name:
            # For lakehouse, we don't use schema prefixes typically
            table_ref = self.extract_table_name
            if self.extract_table_schema and self.extract_table_schema != "default":
                table_ref = f"{self.extract_table_schema}.{self.extract_table_name}"
            return f"SELECT * FROM {table_ref}", "TABLE"
        elif self.extract_view_name:
            view_ref = self.extract_view_name
            if self.extract_view_schema and self.extract_view_schema != "default":
                view_ref = f"{self.extract_view_schema}.{self.extract_view_name}"
            return f"SELECT * FROM {view_ref}", "VIEW"
        else:
            raise ValueError("No source object defined for extract")
    
    def generate_file_name(self, sequence: int = 1) -> str:
        """Generate file name based on configuration"""
        parts = []
        
        # Base name
        parts.append(self.extract_file_name)
        
        # Timestamp
        if self.extract_file_name_timestamp_format:
            if self.extract_file_name_period_end_day:
                # Calculate period end date
                today = datetime.utcnow()
                if today.day <= self.extract_file_name_period_end_day:
                    # Previous month
                    period_end = today.replace(day=1) - timedelta(days=1)
                    period_end = period_end.replace(day=self.extract_file_name_period_end_day)
                else:
                    # Current month
                    period_end = today.replace(day=self.extract_file_name_period_end_day)
                timestamp_str = period_end.strftime(self.extract_file_name_timestamp_format)
            else:
                timestamp_str = datetime.utcnow().strftime(self.extract_file_name_timestamp_format)
            parts.append(timestamp_str)
        
        # Sequence number for file splitting
        if sequence > 1:
            parts.append(f"part{sequence:04d}")
        
        # Join parts and add extension
        file_name = "_".join(parts)
        if not file_name.endswith(f".{self.extract_file_name_extension}"):
            file_name = f"{file_name}.{self.extract_file_name_extension}"
        
        return file_name

# Initialize lakehouse utilities
lakehouse = lakehouse_utils(
    target_workspace_id=configs.config_workspace_id,
    target_lakehouse_id=configs.config_lakehouse_id
)

# For lakehouse, we'll use a simplified configuration loading approach
# Since we don't have a full warehouse setup, we'll simulate loading configurations
print("ğŸ“‹ Loading extract configurations from lakehouse...")

# In a real scenario, configurations would be stored in lakehouse tables
# For now, we'll create sample configurations that work with synthetic data
sample_configs = [
    {
        # Main config
        "extract_name": "SAMPLE_CUSTOMERS_LAKEHOUSE",
        "is_active": True,
        "extract_table_name": "customers",
        "extract_table_schema": "default",
        "is_full_load": True,
        "execution_group": "LAKEHOUSE_EXTRACTS",
        
        # Details config  
        "file_generation_group": "CUSTOMER_DATA",
        "extract_container": "Files/extracts",
        "extract_directory": "customers",
        "extract_file_name": "customers_lakehouse",
        "extract_file_name_timestamp_format": "yyyyMMdd_HHmmss",
        "extract_file_name_extension": "csv",
        "file_properties_column_delimiter": ",",
        "file_properties_header": True,
        "output_format": "csv",
        "is_trigger_file": False,
        "is_compressed": False
    },
    {
        # Products export
        "extract_name": "SAMPLE_PRODUCTS_LAKEHOUSE",
        "is_active": True,
        "extract_table_name": "products",
        "extract_table_schema": "default",
        "is_full_load": True,
        "execution_group": "LAKEHOUSE_EXTRACTS",
        
        # Details
        "file_generation_group": "PRODUCT_DATA",
        "extract_container": "Files/extracts",
        "extract_directory": "products",
        "extract_file_name": "products_catalog",
        "extract_file_name_timestamp_format": "yyyyMMdd",
        "extract_file_name_extension": "parquet",
        "output_format": "parquet",
        "is_trigger_file": True,
        "trigger_file_extension": ".done",
        "is_compressed": False
    },
    {
        # Orders with compression
        "extract_name": "SAMPLE_ORDERS_LAKEHOUSE",
        "is_active": True,
        "extract_table_name": "orders",
        "extract_table_schema": "default", 
        "is_full_load": True,
        "execution_group": "LAKEHOUSE_EXTRACTS",
        
        # Details
        "file_generation_group": "ORDER_DATA",
        "extract_container": "Files/extracts",
        "extract_directory": "orders",
        "extract_file_name": "orders_daily",
        "extract_file_name_timestamp_format": "yyyyMMdd_HHmmss",
        "extract_file_name_extension": "csv",
        "file_properties_column_delimiter": ",",
        "file_properties_header": True,
        "output_format": "csv",
        "is_compressed": True,
        "compressed_type": "GZIP",
        "compressed_level": "NORMAL",
        "compressed_extension": ".gz",
        "is_trigger_file": False
    }
]

# Filter by execution group if specified
if execution_group:
    sample_configs = [c for c in sample_configs if c.get("execution_group") == execution_group]

# Create extract configuration objects
extract_configs = []
for config_data in sample_configs:
    # Split data into config and details parts
    config_part = {k: v for k, v in config_data.items() if k in [
        'extract_name', 'is_active', 'extract_table_name', 'extract_table_schema',
        'extract_view_name', 'extract_view_schema', 'is_full_load', 'execution_group'
    ]}
    details_part = {k: v for k, v in config_data.items() if k not in config_part}
    
    config = ExtractConfiguration(config_part, details_part)
    extract_configs.append(config)

print(f"Found {len(extract_configs)} active extracts to process")
for config in extract_configs:
    print(f"  - {config.extract_name} ({config.get_source_query()[1]}, {config.output_format})")

{{ macros.python_cell_with_heading("## ğŸ’¾ Helper Functions") }}

def write_dataframe_to_lakehouse_file(df: pd.DataFrame, file_path: str, output_format: str, config: ExtractConfiguration) -> dict:
    """Write DataFrame to lakehouse Files using lakehouse_utils abstraction"""
    
    # Convert Pandas DataFrame back to Spark DataFrame for lakehouse operations
    spark_df = lakehouse.spark.createDataFrame(df)
    
    # Prepare write options based on configuration
    options = {
        "mode": "overwrite"
    }
    
    # Add compression options if configured
    if config.is_compressed:
        if config.compressed_type == "GZIP":
            if output_format.lower() in ["csv", "tsv", "json", "text"]:
                options["compression"] = "gzip"
            elif output_format.lower() == "parquet":
                options["compression"] = "gzip"
        elif config.compressed_type == "SNAPPY":
            if output_format.lower() == "parquet":
                options["compression"] = "snappy"
        elif config.compressed_type == "LZ4":
            if output_format.lower() == "parquet":
                options["compression"] = "lz4"
        elif config.compressed_type == "BROTLI":
            if output_format.lower() == "parquet":
                options["compression"] = "brotli"
    
    if output_format.lower() == "csv":
        options.update({
            "header": str(config.file_properties_header).lower(),
            "delimiter": config.file_properties_column_delimiter,
            "encoding": config.file_properties_encoding,
            "quote": config.file_properties_quote_character,
            "escape": config.file_properties_escape_character,
            "nullValue": config.file_properties_null_value
        })
    elif output_format.lower() == "parquet":
        # Parquet has built-in compression support
        if not config.is_compressed:
            options["compression"] = "snappy"  # Default for Parquet
    elif output_format.lower() == "tsv":
        options.update({
            "header": str(config.file_properties_header).lower(),
            "delimiter": "\t",
            "encoding": config.file_properties_encoding,
            "nullValue": config.file_properties_null_value
        })
    
    # Use lakehouse_utils to write the file
    try:
        lakehouse.write_file(
            df=spark_df,
            file_path=file_path,
            file_format=output_format,
            options=options
        )
        
        # Return metadata about the written file
        return {
            "file_path": file_path,
            "rows": len(df),
            "format": output_format,
            "success": True,
            "compressed": config.is_compressed,
            "compression_type": config.compressed_type if config.is_compressed else None
        }
    except Exception as e:
        print(f"âŒ Error writing file {file_path}: {str(e)}")
        return {
            "file_path": file_path,
            "rows": len(df),
            "format": output_format,
            "success": False,
            "error": str(e)
        }

def create_trigger_file(file_path: str) -> bool:
    """Create a trigger file using lakehouse_utils"""
    try:
        # Create an empty DataFrame for the trigger file
        empty_df = lakehouse.spark.createDataFrame([], "dummy STRING")
        
        # Write empty file as trigger
        lakehouse.write_file(
            df=empty_df,
            file_path=file_path,
            file_format="text",
            options={"mode": "overwrite"}
        )
        return True
    except Exception as e:
        print(f"âŒ Error creating trigger file {file_path}: {str(e)}")
        return False

{{ macros.python_cell_with_heading("## ğŸ” Process All Active Extracts") }}

def log_extract_run(extract_config, status: str, extract_start_time, error_message: str = None, **kwargs):
    """Log extract run (simplified for lakehouse - just print)"""
    log_data = {
        "extract_name": extract_config.extract_name,
        "execution_group": extract_config.execution_group,
        "run_id": run_id,
        "run_timestamp": datetime.utcnow(),
        "run_status": status,
        "run_type": run_type,
        "start_time": extract_start_time,
        "end_time": datetime.utcnow(),
        "duration_seconds": int((datetime.utcnow() - extract_start_time).total_seconds()),
        "error_message": error_message,
        "workspace_id": configs.config_workspace_id,
        "lakehouse_id": configs.config_lakehouse_id,
        "created_by": "extract_generation_lakehouse_notebook"
    }
    
    # Add optional fields
    log_data.update(kwargs)
    
    # For lakehouse, just print the log data
    print(f"ğŸ“ LOG: {extract_config.extract_name} - {status}")
    if error_message:
        print(f"    Error: {error_message}")
    if kwargs:
        print(f"    Details: {kwargs}")

# Process each extract configuration
total_extracts = len(extract_configs)
successful_extracts = 0
failed_extracts = 0

print(f"\nğŸš€ Starting batch extract processing for {total_extracts} extracts...")

for extract_index, config in enumerate(extract_configs, 1):
    extract_start_time = datetime.utcnow()
    
    print(f"\n{'='*60}")
    print(f"Processing Extract {extract_index}/{total_extracts}: {config.extract_name}")
    print(f"{'='*60}")
    print(f"Source: {config.get_source_query()[1]} - {config.get_source_query()[0][:100]}...")
    print(f"Output: {config.output_format} | Compression: {'Yes' if config.is_compressed else 'No'}")
    
    try:
        # Get source query
        source_query, source_type = config.get_source_query()
        
        # Execute query using lakehouse_utils abstraction
        print(f"Executing query using lakehouse_utils: {source_query[:100]}...")
        spark_df = lakehouse.execute_query(query=source_query)
        
        # Get row count from PySpark DataFrame
        total_rows = spark_df.count()
        print(f"Retrieved {total_rows:,} rows from source")
        
        # Convert to Pandas DataFrame for file generation
        data_df = spark_df.toPandas()
        
        # Log initial status
        log_extract_run(
            config, "IN_PROGRESS", extract_start_time,
            source_type=source_type,
            source_object=config.extract_table_name or config.extract_view_name,
            source_schema=config.extract_table_schema or config.extract_view_schema,
            rows_extracted=total_rows
        )
        
        # Generate extract files for this extract using lakehouse_utils
        files_generated = []
        total_rows_written = 0
        
        # Show compression status
        if config.is_compressed:
            print(f"ğŸ—œï¸ Compression enabled: {config.compressed_type} ({config.compressed_level})")
        else:
            print(f"ğŸ“„ No compression specified")
        
        # Check if we need to split files
        max_rows = config.file_properties_max_rows_per_file
        
        if max_rows and total_rows > max_rows:
            # Split into multiple files
            num_files = (total_rows + max_rows - 1) // max_rows
            print(f"Splitting into {num_files} files ({max_rows:,} rows each)")
            
            for i in range(num_files):
                start_idx = i * max_rows
                end_idx = min((i + 1) * max_rows, total_rows)
                df_chunk = data_df.iloc[start_idx:end_idx]
                
                # Generate file name
                file_name = config.generate_file_name(sequence=i+1)
                
                # Build full file path for lakehouse Files
                file_path = f"{config.extract_container}/{config.extract_directory}/{file_name}".replace("//", "/")
                
                compression_info = f" ({config.compressed_type})" if config.is_compressed else ""
                print(f"ğŸ“„ Writing file chunk {i+1}/{num_files}: {file_path}{compression_info}")
                
                # Write using lakehouse_utils abstraction
                write_result = write_dataframe_to_lakehouse_file(
                    df=df_chunk,
                    file_path=file_path,
                    output_format=config.output_format,
                    config=config
                )
                
                if write_result["success"]:
                    files_generated.append({
                        "file_name": file_name,
                        "file_path": file_path,
                        "file_size": 0,  # Size not available with lakehouse_utils
                        "rows": len(df_chunk),
                        "compressed": write_result.get("compressed", False),
                        "compression_type": write_result.get("compression_type")
                    })
                else:
                    print(f"âŒ Failed to write file chunk {i+1}: {write_result.get('error', 'Unknown error')}")
                
                total_rows_written += len(df_chunk)
        else:
            # Single file
            file_name = config.generate_file_name()
            
            # Build full file path for lakehouse Files
            file_path = f"{config.extract_container}/{config.extract_directory}/{file_name}".replace("//", "/")
            
            compression_info = f" ({config.compressed_type})" if config.is_compressed else ""
            print(f"ğŸ“„ Writing single file: {file_path}{compression_info}")
            
            # Write using lakehouse_utils abstraction
            write_result = write_dataframe_to_lakehouse_file(
                df=data_df,
                file_path=file_path,
                output_format=config.output_format,
                config=config
            )
            
            if write_result["success"]:
                files_generated.append({
                    "file_name": file_name,
                    "file_path": file_path,
                    "file_size": 0,  # Size not available with lakehouse_utils
                    "rows": total_rows,
                    "compressed": write_result.get("compressed", False),
                    "compression_type": write_result.get("compression_type")
                })
                total_rows_written = total_rows
            else:
                print(f"âŒ Failed to write file: {write_result.get('error', 'Unknown error')}")
        
        # Generate trigger file if configured
        if config.is_trigger_file:
            trigger_file_name = f"{config.extract_file_name}{config.trigger_file_extension}"
            trigger_file_path = f"{config.extract_container}/{config.extract_directory}/{trigger_file_name}".replace("//", "/")
            
            print(f"ğŸ¯ Creating trigger file: {trigger_file_path}")
            trigger_success = create_trigger_file(trigger_file_path)
            if not trigger_success:
                print(f"âš ï¸  Warning: Failed to create trigger file {trigger_file_path}")
        
        # Log successful completion
        log_extract_run(
            config, "SUCCESS", extract_start_time,
            source_type=source_type,
            source_object=config.extract_table_name or config.extract_view_name,
            source_schema=config.extract_table_schema or config.extract_view_schema,
            rows_extracted=total_rows,
            rows_written=total_rows_written,
            files_generated=len(files_generated),
            output_format=config.output_format,
            output_file_path=files_generated[0]["file_path"] if files_generated else None,
            output_file_name=files_generated[0]["file_name"] if files_generated else None,
            output_file_size_bytes=sum(f["file_size"] for f in files_generated),
            is_compressed=config.is_compressed,
            compression_type=config.compressed_type if config.is_compressed else None,
            trigger_file_created=config.is_trigger_file,
            trigger_file_path=trigger_file_path if config.is_trigger_file else None
        )
        
        print(f"âœ… Extract {config.extract_name} completed successfully!")
        print(f"   Files: {len(files_generated)} | Rows: {total_rows_written:,} | Size: {sum(f['file_size'] for f in files_generated):,} bytes")
        successful_extracts += 1
        
    except Exception as e:
        # Enhanced error reporting with line numbers and stack trace
        import traceback
        import sys
        
        # Get the current frame info for line number
        exc_type, exc_value, exc_traceback = sys.exc_info()
        
        # Format the full traceback
        full_traceback = ''.join(traceback.format_exception(exc_type, exc_value, exc_traceback))
        
        # Get the line number where the error occurred
        if exc_traceback:
            line_number = exc_traceback.tb_lineno
            filename = exc_traceback.tb_frame.f_code.co_filename
        else:
            line_number = "Unknown"
            filename = "Unknown"
        
        error_msg = f"Error extracting data from {config.extract_name}: {str(e)}"
        detailed_error = f"""
âŒ EXTRACT GENERATION ERROR DETAILS (LAKEHOUSE):
   Extract Name: {config.extract_name}
   Error Type: {type(e).__name__}
   Error Message: {str(e)}
   File: {filename}
   Line Number: {line_number}
   
ğŸ“‹ Full Traceback:
{full_traceback}
        """
        
        print(detailed_error)
        log_extract_run(config, "FAILED", extract_start_time, error_message=f"{str(e)} (Line: {line_number})")
        failed_extracts += 1
        continue

# End of extract processing loop
print(f"\nğŸ‰ Batch extract processing completed!")
print(f"ğŸ“Š Summary:")
print(f"   Total Extracts: {total_extracts}")
print(f"   âœ… Successful: {successful_extracts}")
print(f"   âŒ Failed: {failed_extracts}")
print(f"   â±ï¸ Total Duration: {int((datetime.utcnow() - start_time).total_seconds())} seconds")

if failed_extracts > 0:
    print(f"\nâš ï¸ WARNING: {failed_extracts} extracts failed. Check logs for details.")
else:
    print(f"\nğŸ¯ All extracts completed successfully!")


{{ macros.python_cell_with_heading("## ğŸ“Š Final Summary") }}

# Display final batch summary
print(f"ğŸ”„ Batch Extract Run Complete (Lakehouse)")
print(f"Run ID: {run_id}")
print(f"â±ï¸ Total Duration: {int((datetime.utcnow() - start_time).total_seconds())} seconds")
print(f"ğŸ“ˆ Execution Summary:")
print(f"   â€¢ Total Extracts Processed: {total_extracts}")
print(f"   â€¢ âœ… Successfully Completed: {successful_extracts}")
print(f"   â€¢ âŒ Failed: {failed_extracts}")
print(f"   â€¢ ğŸ“Š Success Rate: {(successful_extracts/total_extracts*100):.1f}%" if total_extracts > 0 else "   â€¢ ğŸ“Š Success Rate: 0%")

if execution_group:
    print(f"   â€¢ ğŸ¯ Execution Group Filter: {execution_group}")

print(f"\nğŸ Lakehouse extract generation batch processing completed at {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC")