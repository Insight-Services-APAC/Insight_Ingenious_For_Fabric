{%- import 'shared/notebook/macros/notebook_macros.py.jinja' as macros -%}
# Fabric notebook source
# METADATA ********************
# META {
# META   "kernel_info": {
# META     "name": "synapse_python",
# META     "display_name": "Python (Synapse)"
# META   },
# META   "language_info": {
# META     "name": "python",
# META     "language_group": "synapse_python"
# META   }
# META }
{{ macros.parameters_cell() }}
# Default parameters
execution_group = ""  # Optional: filter by execution group, empty = process all
environment = "development"
run_type = "FULL"  # FULL or INCREMENTAL
{{ macros.python_cell_with_heading("## ğŸ“¤ Extract Generation Notebook (Warehouse)") }}
# This notebook generates extract files from warehouse tables, views, or stored procedures
# based on configuration metadata, supporting multiple formats and compression options.
{% set runtime_type = "python" %}
{% set language_group = "synapse_python" %}
{% set include_ddl_utils = true %}
{% include 'shared/notebook/environment/library_loader.py.jinja' %}
{{ macros.python_cell_with_heading("## ğŸ”§ Load Configuration and Initialize") }}
{% include 'shared/notebook/environment/config_loader.py.jinja' %}
# Additional imports for extract generation
import uuid
import json
import io
import gzip
import zipfile
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import pandas as pd
# Import warehouse_utils and sql_templates for warehouse operations
if run_mode == "local":
from ingen_fab.python_libs.python.warehouse_utils import warehouse_utils
from ingen_fab.python_libs.python.sql_templates import SQLTemplates
else:
files_to_load = ["ingen_fab/python_libs/python/warehouse_utils.py", "ingen_fab/python_libs/python/sql_templates.py"]
load_python_modules_from_path(mount_path, files_to_load)
import time
run_id = str(uuid.uuid4())
start_time = datetime.utcnow()
print(f"Run ID: {run_id}")
print(f"Execution Group Filter: {execution_group or 'ALL'}")
print(f"Environment: {environment}")
print(f"Run Type: {run_type}")
{{ macros.python_cell_with_heading("## ğŸ“‹ Load Extract Configuration") }}
class ExtractConfiguration:
"""Configuration class for extract generation"""
def __init__(self, config_row, details_row):
# Main configuration
self.extract_name = config_row["extract_name"]
self.is_active = config_row["is_active"]
self.trigger_name = config_row.get("trigger_name")
self.extract_pipeline_name = config_row.get("extract_pipeline_name")
# Source configuration
self.extract_sp_name = config_row.get("extract_sp_name")
self.extract_sp_schema = config_row.get("extract_sp_schema")
self.extract_table_name = config_row.get("extract_table_name")
self.extract_table_schema = config_row.get("extract_table_schema")
self.extract_view_name = config_row.get("extract_view_name")
self.extract_view_schema = config_row.get("extract_view_schema")
# Validation configuration
self.validation_table_sp_name = config_row.get("validation_table_sp_name")
self.validation_table_sp_schema = config_row.get("validation_table_sp_schema")
# Load configuration
self.is_full_load = config_row["is_full_load"]
self.execution_group = config_row.get("execution_group")
# File details configuration
self.file_generation_group = details_row.get("file_generation_group")
self.extract_container = details_row.get("extract_container", "extracts")
self.extract_directory = details_row.get("extract_directory", "")
# File naming
self.extract_file_name = details_row.get("extract_file_name", self.extract_name)
self.extract_file_name_timestamp_format = details_row.get("extract_file_name_timestamp_format")
self.extract_file_name_period_end_day = details_row.get("extract_file_name_period_end_day")
self.extract_file_name_extension = details_row.get("extract_file_name_extension", "csv")
self.extract_file_name_ordering = details_row.get("extract_file_name_ordering", 1)
# File properties
self.file_properties_column_delimiter = details_row.get("file_properties_column_delimiter", ",")
self.file_properties_row_delimiter = details_row.get("file_properties_row_delimiter", "\\n")
self.file_properties_encoding = details_row.get("file_properties_encoding", "UTF-8")
self.file_properties_quote_character = details_row.get("file_properties_quote_character", '"')
self.file_properties_escape_character = details_row.get("file_properties_escape_character", "\\")
self.file_properties_header = details_row.get("file_properties_header", True)
self.file_properties_null_value = details_row.get("file_properties_null_value", "")
self.file_properties_max_rows_per_file = details_row.get("file_properties_max_rows_per_file")
# Output format
self.output_format = details_row.get("output_format", "csv")
# Trigger file
self.is_trigger_file = details_row.get("is_trigger_file", False)
self.trigger_file_extension = details_row.get("trigger_file_extension", ".done")
# Compression
self.is_compressed = details_row.get("is_compressed", False)
self.compressed_type = details_row.get("compressed_type")
self.compressed_level = details_row.get("compressed_level", "NORMAL")
self.compressed_file_name = details_row.get("compressed_file_name")
self.compressed_extension = details_row.get("compressed_extension", ".zip")
# Fabric paths
self.fabric_lakehouse_path = details_row.get("fabric_lakehouse_path")
def get_source_query(self) -> Tuple[str, str]:
"""Get the SQL query to extract data and the source type"""
if self.extract_table_name:
return f"SELECT * FROM [{self.extract_table_schema}].[{self.extract_table_name}]", "TABLE"
elif self.extract_view_name:
return f"SELECT * FROM [{self.extract_view_schema}].[{self.extract_view_name}]", "VIEW"
elif self.extract_sp_name:
return f"EXEC [{self.extract_sp_schema}].[{self.extract_sp_name}]", "STORED_PROCEDURE"
else:
raise ValueError("No source object defined for extract")
def generate_file_name(self, sequence: int = 1) -> str:
"""Generate file name based on configuration"""
parts = []
# Base name
parts.append(self.extract_file_name)
# Timestamp
if self.extract_file_name_timestamp_format:
if self.extract_file_name_period_end_day:
# Calculate period end date
today = datetime.utcnow()
if today.day <= self.extract_file_name_period_end_day:
# Previous month
period_end = today.replace(day=1) - timedelta(days=1)
period_end = period_end.replace(day=self.extract_file_name_period_end_day)
else:
# Current month
period_end = today.replace(day=self.extract_file_name_period_end_day)
timestamp_str = period_end.strftime(self.extract_file_name_timestamp_format)
else:
timestamp_str = datetime.utcnow().strftime(self.extract_file_name_timestamp_format)
parts.append(timestamp_str)
# Sequence number for file splitting
if sequence > 1:
parts.append(f"part{sequence:04d}")
# Join parts and add extension
file_name = "_".join(parts)
if not file_name.endswith(f".{self.extract_file_name_extension}"):
file_name = f"{file_name}.{self.extract_file_name_extension}"
return file_name
# Initialize warehouse utilities and SQL templates
warehouse = warehouse_utils(
target_workspace_id=configs.config_workspace_id,
target_warehouse_id=configs.config_wh_warehouse_id
)
sql_templates = SQLTemplates()
# Load all active extract configurations using SQL templates
extracts_query = sql_templates.render(
"get_all_active_extracts",
config_schema="config",
execution_group=execution_group if execution_group else None
)
extracts_df = warehouse.execute_query(query=extracts_query)
if extracts_df.empty:
filter_msg = f" for execution group '{execution_group}'" if execution_group else ""
raise ValueError(f"No active extract configurations found{filter_msg}")
# Process column name conflicts (both tables have same column names)
# We'll create separate config and details dictionaries
extract_configs = []
for _, row in extracts_df.iterrows():
row_dict = row.to_dict()
# Split into config and details based on expected columns
config_columns = ['extract_name', 'is_active', 'trigger_name', 'extract_pipeline_name',
'extract_sp_name', 'extract_sp_schema', 'extract_table_name',
'extract_table_schema', 'extract_view_name', 'extract_view_schema',
'validation_table_sp_name', 'validation_table_sp_schema',
'is_full_load', 'execution_group']
config_data = {col: row_dict.get(col) for col in config_columns if col in row_dict}
details_data = {col: val for col, val in row_dict.items() if col not in config_columns}
# Ensure extract_name is in both
config_data['extract_name'] = row_dict['extract_name']
details_data['extract_name'] = row_dict['extract_name']
config = ExtractConfiguration(config_data, details_data)
extract_configs.append(config)
print(f"Found {len(extract_configs)} active extracts to process")
for config in extract_configs:
print(f"  - {config.extract_name} ({config.get_source_query()[1]}, {config.output_format})")
{{ macros.python_cell_with_heading("## ğŸ” Process All Active Extracts") }}
def log_extract_run(extract_config, status: str, extract_start_time, error_message: str = None, **kwargs):
"""Log extract run to log table"""
log_data = {
"extract_name": extract_config.extract_name,
"execution_group": extract_config.execution_group,
"run_id": run_id,
"run_timestamp": datetime.utcnow(),
"run_status": status,
"run_type": run_type,
"start_time": extract_start_time,
"end_time": datetime.utcnow(),
"duration_seconds": int((datetime.utcnow() - extract_start_time).total_seconds()),
"error_message": error_message,
"workspace_id": configs.config_workspace_id,
"warehouse_id": configs.config_wh_warehouse_id,
"created_by": "extract_generation_notebook"
}
# Add optional fields
log_data.update(kwargs)
# For now, just print the log data since write_to_table method needs to be checked
print(f"ğŸ“ LOG: {extract_config.extract_name} - {status}")
if error_message:
print(f"    Error: {error_message}")
if kwargs:
print(f"    Details: {kwargs}")
# Process each extract configuration
total_extracts = len(extract_configs)
successful_extracts = 0
failed_extracts = 0
print(f"\nğŸš€ Starting batch extract processing for {total_extracts} extracts...")
for extract_index, config in enumerate(extract_configs, 1):
extract_start_time = datetime.utcnow()
memory_before = get_memory_usage_mb()
print(f"\n{'='*60}")
print(f"Processing Extract {extract_index}/{total_extracts}: {config.extract_name}")
print(f"{'='*60}")
print(f"Source: {config.get_source_query()[1]} - {config.get_source_query()[0][:100]}...")
print(f"Output: {config.output_format} | Compression: {'Yes' if config.is_compressed else 'No'}")
print(f"ğŸ’¾ Initial Memory Usage: {memory_before:.1f}MB")
try:
# Run validation if configured
if config.validation_table_sp_name:
print(f"Running validation procedure: [{config.validation_table_sp_schema}].[{config.validation_table_sp_name}]")
validation_query = f"EXEC [{config.validation_table_sp_schema}].[{config.validation_table_sp_name}]"
warehouse.execute_query(validation_query)
# Get source query
source_query, source_type = config.get_source_query()
print(f"Executing query: {source_query[:100]}...")
# Execute query and get results
if source_type == "STORED_PROCEDURE":
# For stored procedures, we need to capture the result set
# This assumes the SP returns a result set
data_df = warehouse.execute_query(query=source_query)
else:
data_df = warehouse.execute_query(query=source_query)
total_rows = len(data_df)
print(f"Retrieved {total_rows:,} rows from source")
# Log initial status
log_extract_run(
config, "IN_PROGRESS", extract_start_time,
source_type=source_type,
source_object=config.extract_table_name or config.extract_view_name or config.extract_sp_name,
source_schema=config.extract_table_schema or config.extract_view_schema or config.extract_sp_schema,
rows_extracted=total_rows
)
# Generate extract files for this extract
files_generated = []
total_rows_written = 0
# Check if we need to split files
max_rows = config.file_properties_max_rows_per_file
if max_rows and total_rows > max_rows:
# Split into multiple files
num_files = (total_rows + max_rows - 1) // max_rows
print(f"Splitting into {num_files} files ({max_rows:,} rows each)")
for i in range(num_files):
start_idx = i * max_rows
end_idx = min((i + 1) * max_rows, total_rows)
df_chunk = data_df.iloc[start_idx:end_idx]
# Generate file name
file_name = config.generate_file_name(sequence=i+1)
# Convert to specified format
file_data = write_dataframe_to_format(df_chunk, config.output_format, config)
# Compress if needed
if config.is_compressed:
compressed_name = f"{file_name}.{config.compressed_extension}"
file_data = compress_file(file_data, file_name, config.compressed_type, config.compressed_level)
file_name = compressed_name
# Write to lakehouse
file_path = f"{config.extract_container}/{config.extract_directory}/{file_name}".replace("//", "/")
# For now, we'll simulate file writing
print(f"ğŸ“„ Writing file: {file_path} ({len(file_data):,} bytes)")
files_generated.append({
"file_name": file_name,
"file_path": file_path,
"file_size": len(file_data),
"rows": len(df_chunk)
})
total_rows_written += len(df_chunk)
else:
# Single file
file_name = config.generate_file_name()
# Convert to specified format
file_data = write_dataframe_to_format(data_df, config.output_format, config)
# Compress if needed
if config.is_compressed:
compressed_name = f"{file_name}.{config.compressed_extension}"
file_data = compress_file(file_data, file_name, config.compressed_type, config.compressed_level)
file_name = compressed_name
# Write to lakehouse
file_path = f"{config.extract_container}/{config.extract_directory}/{file_name}".replace("//", "/")
print(f"ğŸ“„ Writing file: {file_path} ({len(file_data):,} bytes)")
files_generated.append({
"file_name": file_name,
"file_path": file_path,
"file_size": len(file_data),
"rows": total_rows
})
total_rows_written = total_rows
# Generate trigger file if configured
if config.is_trigger_file:
trigger_file_name = f"{config.extract_file_name}{config.trigger_file_extension}"
trigger_file_path = f"{config.extract_container}/{config.extract_directory}/{trigger_file_name}".replace("//", "/")
print(f"ğŸ¯ Creating trigger file: {trigger_file_path}")
# In production, create empty file in lakehouse
# Calculate and display performance metrics
extract_end_time = datetime.utcnow()
memory_after = get_memory_usage_mb()
performance_metrics = calculate_performance_metrics(
extract_start_time, extract_end_time, total_rows_written, files_generated
)
print_performance_summary(config.extract_name, performance_metrics, memory_before, memory_after)
# Log successful completion with performance metrics
log_extract_run(
config, "SUCCESS", extract_start_time,
source_type=source_type,
source_object=config.extract_table_name or config.extract_view_name or config.extract_sp_name,
source_schema=config.extract_table_schema or config.extract_view_schema or config.extract_sp_schema,
rows_extracted=total_rows,
rows_written=total_rows_written,
files_generated=len(files_generated),
output_format=config.output_format,
output_file_path=files_generated[0]["file_path"] if files_generated else None,
output_file_name=files_generated[0]["file_name"] if files_generated else None,
output_file_size_bytes=sum(f["file_size"] for f in files_generated),
is_compressed=config.is_compressed,
compression_type=config.compressed_type if config.is_compressed else None,
trigger_file_created=config.is_trigger_file,
trigger_file_path=trigger_file_path if config.is_trigger_file else None,
# Performance metrics
duration_seconds=performance_metrics["duration_seconds"],
throughput_rows_per_second=performance_metrics["throughput_rows_per_second"],
memory_usage_mb_before=memory_before,
memory_usage_mb_after=memory_after
)
print(f"âœ… Extract {config.extract_name} completed successfully!")
print(f"   Files: {len(files_generated)} | Rows: {total_rows_written:,} | Duration: {performance_metrics['duration_formatted']} | Throughput: {performance_metrics['throughput_formatted']}")
successful_extracts += 1
except Exception as e:
# Enhanced error reporting with line numbers and stack trace
import traceback
import sys
# Get the current frame info for line number
exc_type, exc_value, exc_traceback = sys.exc_info()
# Format the full traceback
full_traceback = ''.join(traceback.format_exception(exc_type, exc_value, exc_traceback))
# Get the line number where the error occurred
if exc_traceback:
line_number = exc_traceback.tb_lineno
filename = exc_traceback.tb_frame.f_code.co_filename
else:
line_number = "Unknown"
filename = "Unknown"
error_msg = f"Error extracting data from {config.extract_name}: {str(e)}"
detailed_error = f"""
âŒ EXTRACT GENERATION ERROR DETAILS:
Extract Name: {config.extract_name}
Error Type: {type(e).__name__}
Error Message: {str(e)}
File: {filename}
Line Number: {line_number}
ğŸ“‹ Full Traceback:
{full_traceback}
"""
print(detailed_error)
log_extract_run(config, "FAILED", extract_start_time, error_message=f"{str(e)} (Line: {line_number})")
failed_extracts += 1
continue
# End of extract processing loop
print(f"\nğŸ‰ Batch extract processing completed!")
print(f"ğŸ“Š Summary:")
print(f"   Total Extracts: {total_extracts}")
print(f"   âœ… Successful: {successful_extracts}")
print(f"   âŒ Failed: {failed_extracts}")
print(f"   â±ï¸ Total Duration: {int((datetime.utcnow() - start_time).total_seconds())} seconds")
if failed_extracts > 0:
print(f"\nâš ï¸ WARNING: {failed_extracts} extracts failed. Check logs for details.")
else:
print(f"\nğŸ¯ All extracts completed successfully!")
{{ macros.python_cell_with_heading("## ğŸ“Š Performance Monitoring Functions") }}
import psutil
import os
def get_memory_usage_mb():
"""Get current memory usage in MB"""
process = psutil.Process(os.getpid())
return round(process.memory_info().rss / 1024 / 1024, 2)
def calculate_performance_metrics(start_time, end_time, total_rows, files_generated):
"""Calculate performance metrics for an extract"""
duration_seconds = (end_time - start_time).total_seconds()
metrics = {
"duration_seconds": round(duration_seconds, 3),
"duration_formatted": f"{duration_seconds:.3f}s",
"total_rows": total_rows,
"files_generated": len(files_generated),
"total_file_rows": sum(f.get("rows", 0) for f in files_generated if f.get("rows", 0) > 0),
"throughput_rows_per_second": 0,
"throughput_formatted": "0 rows/s"
}
# Calculate throughput
if duration_seconds > 0 and total_rows > 0:
throughput = total_rows / duration_seconds
metrics["throughput_rows_per_second"] = round(throughput, 2)
# Format throughput with appropriate units
if throughput >= 1000000:
metrics["throughput_formatted"] = f"{throughput/1000000:.2f}M rows/s"
elif throughput >= 1000:
metrics["throughput_formatted"] = f"{throughput/1000:.2f}K rows/s"
else:
metrics["throughput_formatted"] = f"{throughput:.0f} rows/s"
return metrics
def print_performance_summary(extract_name, metrics, memory_before, memory_after):
"""Print a formatted performance summary"""
print(f"\nğŸ“Š PERFORMANCE METRICS - {extract_name}")
print(f"    â±ï¸  Duration: {metrics['duration_formatted']}")
print(f"    ğŸ“ˆ Throughput: {metrics['throughput_formatted']}")
print(f"    ğŸ“ Rows Processed: {metrics['total_rows']:,}")
print(f"    ğŸ“ Files Generated: {metrics['files_generated']}")
if memory_before and memory_after:
memory_delta = memory_after - memory_before
memory_indicator = "ğŸ“ˆ" if memory_delta > 0 else "ğŸ“‰" if memory_delta < 0 else "ğŸ“Š"
print(f"    {memory_indicator} Memory: {memory_before:.1f}MB â†’ {memory_after:.1f}MB (Î” {memory_delta:+.1f}MB)")
{{ macros.python_cell_with_heading("## ğŸ’¾ File Generation Helper Functions") }}
def write_dataframe_to_format(df: pd.DataFrame, output_format: str, config: ExtractConfiguration) -> bytes:
"""Convert DataFrame to specified format and return as bytes"""
buffer = io.BytesIO()
if output_format == "csv":
df.to_csv(
buffer,
index=False,
sep=config.file_properties_column_delimiter,
header=config.file_properties_header,
encoding=config.file_properties_encoding,
quotechar=config.file_properties_quote_character,
escapechar=config.file_properties_escape_character,
na_rep=config.file_properties_null_value
)
elif output_format == "tsv":
df.to_csv(
buffer,
index=False,
sep='\t',
header=config.file_properties_header,
encoding=config.file_properties_encoding,
na_rep=config.file_properties_null_value
)
elif output_format == "dat":
# DAT format - typically uses pipe delimiter by default but can be customized
delimiter = config.file_properties_column_delimiter if config.file_properties_column_delimiter else '|'
df.to_csv(
buffer,
index=False,
sep=delimiter,
header=config.file_properties_header,
encoding=config.file_properties_encoding,
quotechar=config.file_properties_quote_character,
escapechar=config.file_properties_escape_character,
na_rep=config.file_properties_null_value
)
elif output_format == "parquet":
df.to_parquet(buffer, index=False, engine='pyarrow')
else:
raise ValueError(f"Unsupported output format: {output_format}")
buffer.seek(0)
return buffer.getvalue()
def compress_file(file_data: bytes, file_name: str, compression_type: str, compression_level: str) -> bytes:
"""Compress file data based on configuration"""
if compression_type == "GZIP":
# Map compression levels
level_map = {"MINIMUM": 1, "NORMAL": 6, "MAXIMUM": 9}
compress_level = level_map.get(compression_level, 6)
return gzip.compress(file_data, compresslevel=compress_level)
elif compression_type == "ZIP":
buffer = io.BytesIO()
with zipfile.ZipFile(buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
zip_file.writestr(file_name, file_data)
buffer.seek(0)
return buffer.getvalue()
else:
raise ValueError(f"Unsupported compression type: {compression_type}")
{{ macros.python_cell_with_heading("## ğŸ“Š Final Summary") }}
# Display final batch summary
print(f"ğŸ”„ Batch Extract Run Complete")
print(f"Run ID: {run_id}")
print(f"â±ï¸ Total Duration: {int((datetime.utcnow() - start_time).total_seconds())} seconds")
print(f"ğŸ“ˆ Execution Summary:")
print(f"   â€¢ Total Extracts Processed: {total_extracts}")
print(f"   â€¢ âœ… Successfully Completed: {successful_extracts}")
print(f"   â€¢ âŒ Failed: {failed_extracts}")
print(f"   â€¢ ğŸ“Š Success Rate: {(successful_extracts/total_extracts*100):.1f}%" if total_extracts > 0 else "   â€¢ ğŸ“Š Success Rate: 0%")
if execution_group:
print(f"   â€¢ ğŸ¯ Execution Group Filter: {execution_group}")
# Display recent run history for all extracts processed
if successful_extracts > 0:
print(f"\nğŸ“‹ Recent Extract Activity:")
for config in extract_configs[:5]:  # Show first 5 extracts
try:
history_query = sql_templates.render(
"get_extract_history",
extract_name=config.extract_name,
log_schema="log",
limit=3
)
history_df = warehouse.execute_query(query=history_query)
if not history_df.empty:
latest_run = history_df.iloc[0]
print(f"   â€¢ {config.extract_name}: {latest_run['run_status']} ({latest_run.get('rows_extracted', 0):,} rows)")
except Exception:
print(f"   â€¢ {config.extract_name}: Status unavailable")
print(f"\nğŸ Extract generation batch processing completed at {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC")
