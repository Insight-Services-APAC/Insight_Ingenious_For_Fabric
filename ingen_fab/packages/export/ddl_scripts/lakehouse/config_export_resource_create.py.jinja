# Configuration table for export resources - Lakehouse version
# Stores export configuration for exporting tables to files

from pyspark.sql.types import (
    ArrayType,
    BooleanType,
    IntegerType,
    MapType,
    StringType,
    StructField,
    StructType,
    TimestampType,
)

# Define schema for config_export_resource table
schema = StructType([
    # Primary identifiers (composite key)
    StructField("export_group_name", StringType(), False),
    StructField("export_name", StringType(), False),
    StructField("execution_group", IntegerType(), False),

    # Source configuration
    StructField("source_type", StringType(), False),  # "lakehouse" or "warehouse"
    StructField("source_workspace", StringType(), False),
    StructField("source_datastore", StringType(), False),
    StructField("source_schema", StringType(), True),
    StructField("source_table", StringType(), True),
    StructField("source_query", StringType(), True),
    StructField("source_columns", ArrayType(StringType()), True),  # Column list

    # Target configuration
    StructField("target_workspace", StringType(), False),
    StructField("target_lakehouse", StringType(), False),
    StructField("target_path", StringType(), False),
    StructField("target_filename_pattern", StringType(), True),

    # File format configuration
    StructField("file_format", StringType(), False),  # csv, tsv, dat, parquet, json
    StructField("file_format_options", MapType(StringType(), StringType()), True),  # Spark write options (header, sep, quote, etc.)
    StructField("compression", StringType(), True),  # gzip, zipdeflate, snappy, none
    StructField("compression_level", IntegerType(), True),  # Compression level (gzip: 1-9, zip: 0-9, brotli: 0-11)
    StructField("compressed_filename_pattern", StringType(), True),

    # File splitting
    StructField("max_rows_per_file", IntegerType(), True),

    # Extract type configuration
    StructField("extract_type", StringType(), True),  # "full", "incremental", "period"
    StructField("incremental_column", StringType(), True),  # Column for incremental/watermark tracking
    StructField("incremental_initial_watermark", StringType(), True),  # Starting point for first incremental run
    StructField("period_filter_column", StringType(), True),  # Column for period-based filtering

    # Period date query - SQL returning start_date, end_date columns
    StructField("period_date_query", StringType(), True),

    # Trigger file configuration
    StructField("trigger_file_pattern", StringType(), True),

    # Metadata
    StructField("description", StringType(), True),
    StructField("is_active", BooleanType(), False),
    StructField("created_at", TimestampType(), True),
    StructField("updated_at", TimestampType(), True),
    StructField("created_by", StringType(), True),
    StructField("updated_by", StringType(), True),
])

target_lakehouse.create_table(
    table_name="config_export_resource",
    schema=schema,
    mode="overwrite",
    partition_by=["export_group_name", "export_name"],
    options={
        "parquet.vorder.default": "true",
        "overwriteSchema": "true"
    }
)
