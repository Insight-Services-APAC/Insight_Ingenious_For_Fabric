{%- import 'shared/notebook/macros/notebook_macros.py.jinja' as macros -%}
# Fabric notebook source

# METADATA ********************

# META {
# META   "kernel_info": {
# META     "name": "synapse_pyspark",
# META     "display_name": "Synapse PySpark"
# META   },
# META   "language_info": {
# META     "name": "python"
# META   }
# META }

{{ macros.parameters_cell() }}

# Default parameters for retry processing
MASTER_EXECUTION_ID = None     # Required: master_execution_id of the original run
MAX_RETRIES = 3                # Maximum retry attempts per failed extraction
MAX_CONCURRENCY = 5            # Concurrency for retry operations
STATUS_FILTER = ["Failed", "Cancelled"]  # Statuses to consider for retry

{{ macros.python_cell_with_heading("## üîÑ Synapse Extract Retry Helper") }}

# **Synapse Extract Retry Helper**
# Author: Synapse Sync Package  
# Date: Generated from enhanced notebook implementations
# 
# ## Overview
# 
# The Retry Helper provides sophisticated retry functionality for failed Synapse extractions. This notebook:
# 
# 1. **Intelligent Retry** - Identifies and retries failed extractions with exponential backoff
# 2. **Selective Processing** - Can target specific execution IDs or time windows
# 3. **Enhanced Error Analysis** - Provides detailed failure analysis and retry recommendations
# 4. **Configurable Attempts** - Supports multiple retry attempts with incremental delay
# 
# The process uses enhanced retry mechanisms with jitter and concurrent write detection.

{% set runtime_type = "pyspark" %}
{% set language_group = "pyspark" %}
{% set include_ddl_utils = false %}

{% include 'shared/notebook/environment/library_loader.py.jinja' %}

{{ macros.python_cell_with_heading("## üîß Load Configuration and Initialize") }}
{% include 'shared/notebook/environment/config_loader.py.jinja' %}

{{ macros.python_cell_with_heading("## Load Enhanced Synapse Libraries") }}

# Load enhanced PySpark libraries for retry processing
from ingen_fab.python_libs.pyspark.synapse_orchestrator import SynapseOrchestrator
from ingen_fab.python_libs.pyspark.synapse_extract_utils import SynapseExtractUtils

{{ macros.python_cell_with_heading("## Enhanced Imports for Retry Processing") }}

# Enhanced imports for retry synapse sync processing
import asyncio
import json
import logging
import uuid
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any

# Third-party imports for enhanced retry functionality
import nest_asyncio
import numpy as np
from delta.tables import DeltaTable

# Fabric imports for pipeline orchestration
import sempy.fabric as fabric
from sempy.fabric.exceptions import FabricHTTPException, WorkspaceNotFoundException

import pyspark.sql.functions as F
from pyspark.sql.types import *

# Configure logging for retry processing tracking
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')
logger = logging.getLogger(__name__)

# Apply nest_asyncio for Jupyter compatibility
nest_asyncio.apply()

{{ macros.python_cell_with_heading("## Retry Processing Constants") }}

# Inject Fabric variables from Variable Library value set (no template changes required)
FABRIC_PIPELINE_ID = "{% raw %}{{varlib:synapse_sync_fabric_pipeline_id}}{% endraw %}"
SYNAPSE_DATASOURCE_NAME = "{% raw %}{{varlib:synapse_datasource_name}}{% endraw %}"
SYNAPSE_DATASOURCE_LOCATION = "{% raw %}{{varlib:synapse_datasource_location}}{% endraw %}"

# Generate retry execution ID
retry_execution_id = str(uuid.uuid4())

# Retry execution parameters with enhanced configuration
RETRY_EXECUTION_PARAMETERS = {
    "extraction_type": "retry",
    "target_master_execution_id": MASTER_EXECUTION_ID,
    "max_retry_attempts": MAX_RETRIES,
    "retry_timestamp": datetime.utcnow().isoformat()
}

TRIGGER_TYPE = "Manual"

{{ macros.python_cell_with_heading("## Initialize Enhanced Utilities") }}

# Initialize lakehouse utilities with enhanced configuration
lakehouse = lakehouse_utils(
    target_workspace_id="{{ config_workspace_id }}", 
    target_lakehouse_id="{{ config_lakehouse_id }}"
)

# Initialize enhanced orchestrator and extract utils
orchestrator = SynapseOrchestrator(lakehouse=lakehouse)
extract_utils = SynapseExtractUtils(lakehouse=lakehouse)

# Get environment name from Variable Library
environment_name = "{{ fabric_environment }}"
logger.info(f"Loaded configuration for environment: {environment_name}")
logger.info(f"Using datasource: {SYNAPSE_DATASOURCE_NAME}")

{{ macros.python_cell_with_heading("## Identify Failed Extractions") }}

logger.info(f"RETRY HELPER MODE - Retry Execution ID: {retry_execution_id}")
logger.info(f"Target Master Execution ID: {MASTER_EXECUTION_ID or 'Not provided'}")
logger.info(f"Max Retry Attempts: {MAX_RETRIES}")
logger.info(f"Max Concurrency: {MAX_CONCURRENCY}")

# Validate required parameter
if not MASTER_EXECUTION_ID:
    raise ValueError("MASTER_EXECUTION_ID is required for retry processing")

# Get candidate extractions using enhanced utils
failed_extractions = extract_utils.get_failed_extracts(
    master_execution_id=MASTER_EXECUTION_ID,
    status_filter=STATUS_FILTER
)

if not failed_extractions:
    logger.info("No failed extractions found matching the criteria")
    print("‚ÑπÔ∏è  No failed extractions found")
    if MASTER_EXECUTION_ID:
        print(f"   - No failures found for Master Execution ID: {MASTER_EXECUTION_ID}")
    else:
        print(f"   - No failures found for execution ID: {MASTER_EXECUTION_ID}")
    print("\n‚úÖ Nothing to retry!")
else:
    logger.info(f"Found {len(failed_extractions)} failed extractions for retry")
    
    logger.info(f"Found {len(failed_extractions)} candidate extractions for retry")

{{ macros.python_cell_with_heading("## Convert Failed Extractions to Work Items") }}

if failed_extractions:
    # Convert failed extractions to work item dicts for retry
    retry_work_items = []
    for failure in failed_extractions:
        # Reuse original select if provided; otherwise reconstruct using partition_clause
        orig_select = failure.get("custom_select_sql")
        if not orig_select:
            part = (failure.get("partition_clause") or "").strip()
            # Ensure partition clause is prefixed with a space when present
            part = f" {part}" if part else ""
            orig_select = (
                f"SELECT * FROM [{failure.get('source_schema_name')}]."
                f"[{failure.get('source_table_name')}]" + part
            )
        retry_work_items.append({
            "source_schema_name": failure.get("source_schema_name"),
            "source_table_name": failure.get("source_table_name"),
            "extract_mode": failure.get("extract_mode", "snapshot"),
            "execution_group": failure.get("execution_group", 1),
            "extract_start_dt": failure.get("extract_start_dt"),
            "extract_end_dt": failure.get("extract_end_dt"),
            # Carry forward the exact query shape used originally
            "custom_select_sql": orig_select,
            # Preserve original partition clause for downstream visibility
            "partition_clause": failure.get("partition_clause"),
            # Provide pipeline id on the work item (varlib-derived)
            "pipeline_id": FABRIC_PIPELINE_ID,
            # Prefer var-lib injected values
            "synapse_datasource_name": SYNAPSE_DATASOURCE_NAME,
            "synapse_datasource_location": SYNAPSE_DATASOURCE_LOCATION,
            # Run-scoped metadata for logging
            "trigger_type": TRIGGER_TYPE,
            "master_execution_parameters": RETRY_EXECUTION_PARAMETERS,
        })
    logger.info(f"Prepared {len(retry_work_items)} work items for retry processing")
    
    # Display retry summary
    print(f"\nüîÑ RETRY PROCESSING SUMMARY")
    print(f"   Total Candidate Extractions: {len(failed_extractions)}")
    print(f"   Work Items Prepared for Retry: {len(retry_work_items)}")
    print(f"   Max Retry Attempts per Item: {MAX_RETRIES}")
    print(f"   Retry Concurrency: {MAX_CONCURRENCY}")

{{ macros.python_cell_with_heading("## Execute Retry Processing (With Attempts)") }}

if failed_extractions:
    # Pre-log retry payloads and map execution IDs (WorkItem carries all metadata)
    extraction_payloads = extract_utils.prepare_extract_payloads(
        work_items=retry_work_items,
        master_execution_id=retry_execution_id,
    )
    execution_id_map = extract_utils.bulk_insert_queued_extracts(extraction_payloads) if extraction_payloads else {}
    external_table_map = {}
    for r in (extraction_payloads or []):
        key = f"{r['source_schema_name']}.{r['source_table_name']}|{r.get('extract_start_dt') or ''}|{r.get('extract_end_dt') or ''}|{r.get('extract_mode') or ''}"
        external_table_map[key] = r.get('external_table')

    # Define retry wrapper using orchestrator.process_extract
    from types import SimpleNamespace
    import asyncio

    # Global concurrency limiter shared across all tasks
    CONCURRENCY_LIMITER = asyncio.Semaphore(MAX_CONCURRENCY)

    async def process_extract_with_retry(work_item: dict, max_retries: int = MAX_RETRIES):
        wi = SimpleNamespace(
            source_schema_name=work_item["source_schema_name"],
            source_table_name=work_item["source_table_name"],
            extract_mode=work_item.get("extract_mode", "snapshot"),
            execution_group=work_item.get("execution_group", 1),
            extract_start_dt=work_item.get("extract_start_dt"),
            extract_end_dt=work_item.get("extract_end_dt"),
            # Pass through SQL and datasource details so CETAS matches original
            custom_select_sql=work_item.get("custom_select_sql"),
            synapse_datasource_name=work_item.get("synapse_datasource_name"),
            synapse_datasource_location=work_item.get("synapse_datasource_location"),
        )
        attempt = 0
        last_error = None
        while attempt < max_retries:
            attempt += 1
            # Look up execution_id by external_table derived from payload key
            key = f"{work_item['source_schema_name']}.{work_item['source_table_name']}|{work_item.get('extract_start_dt') or ''}|{work_item.get('extract_end_dt') or ''}|{work_item.get('extract_mode', 'snapshot')}"
            ext_tbl = external_table_map.get(key)
            execution_id = execution_id_map.get(ext_tbl) if ext_tbl else None
            success, error = await orchestrator.process_extract(
                wi,
                None,
                retry_execution_id,
                CONCURRENCY_LIMITER,
                synapse_sync_fabric_pipeline_id=FABRIC_PIPELINE_ID,
                workspace_id="{% raw %}{{varlib:fabric_deployment_workspace_id}}{% endraw %}",
                extract_utils=extract_utils,
                execution_id=execution_id,
            )
            if success:
                return True, None
            last_error = error
            await asyncio.sleep(min(5 * attempt, 30))
        return False, last_error

    async def run_retries():
        # Group by execution group
        groups = {}
        for item in retry_work_items:
            groups.setdefault(item.get("execution_group", 1), []).append(item)
        all_results = []
        for group in sorted(groups.keys()):
            items = groups[group]
            tasks = []
            for item in items:
                key = f"{item['source_schema_name']}.{item['source_table_name']}"
                # Execution id is looked up inside the retry function based on the external table mapping
                tasks.append(process_extract_with_retry(item, MAX_RETRIES))
            if tasks:
                results = await asyncio.gather(*tasks, return_exceptions=False)
                all_results.extend(results)
        total = len(all_results)
        successes = sum(1 for r in all_results if r[0])
        failed = total - successes
        rate = f"{(successes/total*100):.1f}%" if total else "N/A"
        return {
            "retry_master_execution_id": retry_execution_id,
            "total_tables": total,
            "successful_retries": successes,
            "failed_retries": failed,
            "success_rate": rate,
        }

    if extraction_payloads:
        logger.info("Starting retry processing")
        retry_summary = await run_retries()
        logger.info(f"RETRY SUMMARY - Master Execution ID: {retry_summary['retry_master_execution_id']}")
        logger.info(f"Total: {retry_summary['total_tables']}, Succeeded: {retry_summary['successful_retries']}, Failed: {retry_summary['failed_retries']}")
        logger.info(f"Success Rate: {retry_summary['success_rate']}")
    else:
        print("No retry payloads prepared.")
else:
    logger.info("No failed extractions to retry - nothing to do")
    print("\n‚úÖ No retry processing needed!")

{{ macros.exit_notebook("success") }}

{% include 'shared/notebook/cells/footer.py.jinja' %}
