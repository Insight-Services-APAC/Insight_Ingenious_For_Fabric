# Log table for batch-level extraction tracking - Lakehouse version
# Tracks individual extraction batches (file or folder level)

from pyspark.sql.types import (
    ArrayType,
    IntegerType,
    LongType,
    StringType,
    StructField,
    StructType,
    TimestampType,
)

# Define schema for the log_resource_extract_batch table
schema = StructType([
    # Primary key and identifiers
    StructField("extract_batch_id", StringType(), False),  # PK - unique batch ID
    StructField("extract_run_id", StringType(), False),  # FK to log_resource_extract_config
    StructField("master_execution_id", StringType(), False),  # Orchestrator execution
    StructField("source_name", StringType(), False),  # Source system
    StructField("resource_name", StringType(), False),  # Resource/table name
    StructField("extract_state", StringType(), False),  # pending, running, success, warning, error
    StructField("load_state", StringType(), False),  # pending, running, success, warning, error
    # Path information
    StructField("source_path", StringType(), True),  # Original location (inbound)
    StructField("extract_file_paths", ArrayType(StringType()), False),  # Files promoted to raw
    # Batch metrics
    StructField("file_count", IntegerType(), True),  # Files in batch
    StructField("file_size_bytes", LongType(), True),  # Total size
    # Timing
    StructField("started_at", TimestampType(), False),  # When extraction started
    StructField("completed_at", TimestampType(), True),  # When extraction finished
    StructField("duration_ms", LongType(), True),  # Extraction duration
    # Error tracking
    StructField("error_message", StringType(), True),
])

target_lakehouse.create_table(
    table_name="log_resource_extract_batch",
    schema=schema,
    mode="overwrite",
    partition_by=["source_name", "resource_name"],
    options={
        "parquet.vorder.default": "true",
        "overwriteSchema": "true"
    }
)
