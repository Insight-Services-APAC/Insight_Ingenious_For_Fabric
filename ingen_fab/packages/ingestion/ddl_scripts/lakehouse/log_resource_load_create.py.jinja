# Log table for resource-level loading tracking - Lakehouse version
# Tracks loading runs (one row per resource per execution)

from pyspark.sql.types import (
    LongType,
    StringType,
    StructField,
    StructType,
    TimestampType,
)

# Define schema for the log_resource_load table
schema = StructType([
    # Primary key and identifiers
    StructField("load_run_id", StringType(), False),  # PK - Resource run
    StructField("master_execution_id", StringType(), False),  # Orchestrator execution
    StructField("source_name", StringType(), False),  # Source system
    StructField("resource_name", StringType(), False),  # Resource/table name
    StructField("load_state", StringType(), False),  # pending, running, success, warning, error
    # File counts
    StructField("files_discovered", LongType(), True),
    StructField("files_processed", LongType(), True),
    StructField("files_failed", LongType(), True),
    StructField("files_skipped", LongType(), True),
    # Aggregated metrics
    StructField("records_processed", LongType(), True),
    StructField("records_inserted", LongType(), True),
    StructField("records_updated", LongType(), True),
    StructField("records_deleted", LongType(), True),
    StructField("total_duration_ms", LongType(), True),
    # Error tracking
    StructField("error_message", StringType(), True),
    # Timestamp tracking
    StructField("started_at", TimestampType(), False),  # Immutable
    StructField("updated_at", TimestampType(), False),  # Updated on merge
    StructField("completed_at", TimestampType(), True),  # Set when status is final
])

target_lakehouse.create_table(
    table_name="log_resource_load",
    schema=schema,
    mode="overwrite",
    partition_by=["source_name", "resource_name"],
    options={
        "parquet.vorder.default": "true",
        "overwriteSchema": "true"
    }
)
