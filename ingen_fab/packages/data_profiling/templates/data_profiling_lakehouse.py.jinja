{%- set datastore_type = "lakehouse" -%}
{%- set kernel_name = "synapse_pyspark" -%}
{%- set kernel_display_name = "PySpark (Synapse)" -%}
{%- set language_group = "synapse_pyspark" -%}
{%- set runtime_type = "pyspark" -%}
{%- set include_ddl_utils = false -%}

{%- extends "data_profiling_base.py.jinja" -%}

{% block datastore_specific_imports %}
from ingen_fab.python_libs.pyspark.lakehouse_utils import lakehouse_utils
{% endblock %}

{% block main_execution %}
print("=" * 80)
print(f"Data Profiling Processor - Lakehouse")
print(f"Execution Time: {datetime.now()}")
print("=" * 80)

# Initialize lakehouse utils (it creates its own spark session)
config_lakehouse = lakehouse_utils(
    target_workspace_id=configs.config_workspace_id,
    target_lakehouse_id=configs.config_lakehouse_id
)


# Initialize TieredProfiler (the only profiler available)
print("⚡ Using TieredProfiler - progressive multi-level profiling for optimal performance")
from ingen_fab.python_libs.pyspark.tiered_profiler import TieredProfiler
profiler = TieredProfiler(
    lakehouse=config_lakehouse, 
    spark=config_lakehouse.spark,
    exclude_views=exclude_views
)

# Determine which scan levels to run based on parameters
if isinstance(scan_levels, str):
    scan_levels = eval(scan_levels)  # Convert string "[1,2,3,4]" to list
levels_to_run = [level for level in scan_levels if min_scan_level <= level <= max_scan_level]

print(f"\n📊 Scan Level Configuration:")
print(f"  • Levels to run: {levels_to_run}")
print(f"  • Incremental scanning: {'Enabled' if enable_incremental else 'Disabled'}")
print(f"  • Force rescan: {'Yes' if force_rescan else 'No'}")
print(f"  • Sample size for Level 3+: {sample_size if sample_size else 'Full dataset'}")

# Get list of tables to profile
if target_tables:
    tables_to_profile = target_tables
else:
    # Get active profiling configurations from catalog
    try:
        config_df = config_lakehouse.read_table("config_data_profiling")
        active_configs = config_df.filter("active_yn = 'Y'").collect()
        tables_to_profile = [row.table_name for row in active_configs]
        print(f"\n✅ Found {len(tables_to_profile)} active tables from config_data_profiling")
    except Exception as e:
        print(f"\n⚠️  No configuration table found, discovering all tables")
        # Get all tables in the lakehouse
        all_tables = config_lakehouse.list_tables()
        
        if only_scan_new_tables and not force_rescan:
            # Filter to only new tables not in metadata
            try:
                from ingen_fab.python_libs.pyspark.tiered_profiler import ScanLevel
                existing_tables = profiler.persistence.list_completed_tables(ScanLevel.LEVEL_1_DISCOVERY)
                tables_to_profile = [t for t in all_tables if t not in existing_tables]
                print(f"✅ Found {len(tables_to_profile)} new tables to profile (out of {len(all_tables)} total)")
            except:
                tables_to_profile = all_tables
                print(f"✅ Found {len(all_tables)} tables in lakehouse")
        else:
            tables_to_profile = all_tables
            print(f"✅ Found {len(all_tables)} tables in lakehouse")

# Run progressive scan levels
execution_id = str(uuid.uuid4())
run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

print(f"\n🚀 Starting TieredProfiler Progressive Scanning")
print(f"   Execution ID: {execution_id}")
print(f"   Tables to process: {len(tables_to_profile)}")
print(f"   Timestamp: {run_timestamp}")

# Import ScanLevel enum
from ingen_fab.python_libs.pyspark.tiered_profiler import ScanLevel

# Run each scan level progressively
scan_results = {}

# Level 1: Discovery
if 1 in levels_to_run:
    print(f"\n{'='*60}")
    print("📍 LEVEL 1: Table Discovery")
    print(f"{'='*60}")
    print(f"Scanning {len(tables_to_profile)} tables for basic metadata...")
    
    try:
        discovered = profiler.scan_level_1_discovery(
            table_paths=tables_to_profile,
            resume=enable_incremental and not force_rescan
        )
        scan_results["level_1"] = discovered
        print(f"✅ Level 1 complete: {len(discovered)} tables discovered")
        
        # Update tables list if incremental
        if enable_incremental and not force_rescan:
            tables_to_profile = [t.table_name for t in discovered]
    except Exception as e:
        print(f"❌ Level 1 failed: {e}")
        scan_results["level_1"] = []

# Level 2: Schema Discovery  
if 2 in levels_to_run:
    print(f"\n{'='*60}")
    print("📋 LEVEL 2: Schema Discovery")
    print(f"{'='*60}")
    
    # Use discovered tables or original list
    level_2_tables = [t.table_name for t in scan_results.get("level_1", [])] if scan_results.get("level_1") else tables_to_profile
    print(f"Extracting schema for {len(level_2_tables)} tables...")
    
    try:
        schemas = profiler.scan_level_2_schema(
            table_names=level_2_tables,
            resume=enable_incremental and not force_rescan
        )
        scan_results["level_2"] = schemas
        print(f"✅ Level 2 complete: {len(schemas)} schemas extracted")
    except Exception as e:
        print(f"❌ Level 2 failed: {e}")
        scan_results["level_2"] = []

# Level 3: Detailed Profiling
if 3 in levels_to_run:
    print(f"\n{'='*60}")
    print("📊 LEVEL 3: Detailed Column Profiling")
    print(f"{'='*60}")
    
    # Use schema tables or original list
    level_3_tables = [s.table_name for s in scan_results.get("level_2", [])] if scan_results.get("level_2") else tables_to_profile
    print(f"Profiling {len(level_3_tables)} tables with detailed statistics...")
    if sample_size:
        print(f"   Using sample size: {sample_size:.1%}")
    
    try:
        profiles = profiler.scan_level_3_profile(
            table_names=level_3_tables,
            sample_size=sample_size,
            resume=enable_incremental and not force_rescan
        )
        scan_results["level_3"] = profiles
        print(f"✅ Level 3 complete: {len(profiles)} tables profiled")
    except Exception as e:
        print(f"❌ Level 3 failed: {e}")
        scan_results["level_3"] = []

# Level 4: Advanced Analysis
if 4 in levels_to_run:
    print(f"\n{'='*60}")
    print("🔬 LEVEL 4: Advanced Analysis & Relationship Discovery")
    print(f"{'='*60}")
    
    # Use profiled tables or original list
    level_4_tables = [p.dataset_name for p in scan_results.get("level_3", [])] if scan_results.get("level_3") else tables_to_profile
    print(f"Running advanced analysis on {len(level_4_tables)} tables...")
    print(f"   • Cross-column correlations")
    print(f"   • Relationship discovery")
    print(f"   • Pattern analysis")
    
    try:
        advanced_profiles = profiler.scan_level_4_advanced(
            table_names=level_4_tables,
            resume=enable_incremental and not force_rescan
        )
        scan_results["level_4"] = advanced_profiles
        print(f"✅ Level 4 complete: {len(advanced_profiles)} tables analyzed")
    except Exception as e:
        print(f"❌ Level 4 failed: {e}")
        scan_results["level_4"] = []

print(f"\n{'='*60}")
print("📈 Scan Summary")
print(f"{'='*60}")

# Process results and generate reports
profiling_results = []

# Get final profiles from highest completed level
final_profiles = []
if scan_results.get("level_4"):
    final_profiles = scan_results["level_4"]
    print(f"Using Level 4 (Advanced) results for {len(final_profiles)} tables")
elif scan_results.get("level_3"):
    final_profiles = scan_results["level_3"]
    print(f"Using Level 3 (Profile) results for {len(final_profiles)} tables")
else:
    print("⚠️  No detailed profiles available (Level 3+ required for reports)")

# Generate reports for each profile
for profile in final_profiles:
    print(f"\n📄 Generating reports for: {profile.dataset_name}")
    
    try:
        # Add metadata
        profile_result = {
            "execution_id": execution_id,
            "table_name": profile.dataset_name,
            "profile_timestamp": datetime.now().isoformat(),
            "scan_levels_completed": levels_to_run,
            "profile": profile,
            "status": "success"
        }
        
        # Generate quality report if requested
        if generate_report:
            report = profiler.generate_quality_report(profile, output_format)
            profile_result["report"] = report
            
            # Save report to Files with consistent timestamp
            if output_format in ["html", "yaml", "json", "markdown"]:
                ext_map = {
                    "html": "html",
                    "yaml": "yaml", 
                    "json": "json",
                    "markdown": "md"
                }
                extension = ext_map.get(output_format, "txt")
                report_path = f"Files/profiling_reports/{run_timestamp}_{profile.dataset_name}.{extension}"
                config_lakehouse.write_string_to_file(
                    content=report,
                    file_path=report_path,
                    encoding="utf-8",
                    mode="overwrite"
                )
                print(f"   ✅ Report saved: {report_path}")
        
        # Generate YAML output separately if requested
        if generate_yaml_output and output_format != "yaml":
            yaml_report = profiler.generate_quality_report(profile, "yaml")
            yaml_path = f"Files/profiling_reports/{run_timestamp}_{profile.dataset_name}.yaml"
            config_lakehouse.write_string_to_file(
                content=yaml_report,
                file_path=yaml_path,
                encoding="utf-8",
                mode="overwrite"
            )
            print(f"   ✅ YAML saved: {yaml_path}")
        
        # Calculate data quality score
        if profile.data_quality_score is not None:
            profile_result["quality_score"] = profile.data_quality_score
        
        profiling_results.append(profile_result)
        
        # Display summary
        print(f"   ✅ Profile complete:")
        print(f"      • Rows: {profile.row_count:,}")
        print(f"      • Columns: {profile.column_count}")
        if profile.data_quality_score:
            print(f"      • Quality Score: {profile.data_quality_score:.2%}")
        
        profiling_results.append(profile_result)
        
    except Exception as e:
        import traceback
        print(f"   ❌ Error generating report: {e}")
        traceback.print_exc()
        profiling_results.append({
            "execution_id": execution_id,
            "table_name": profile.dataset_name,
            "profile_timestamp": datetime.now().isoformat(),
            "status": "failed",
            "error": str(e)
        })

# Final Summary
print(f"\n{'='*60}")
print("🎯 PROFILING COMPLETE - FINAL SUMMARY")
print(f"{'='*60}")

# Calculate statistics
successful_profiles = [r for r in profiling_results if r.get("status") == "success"]
failed_profiles = [r for r in profiling_results if r.get("status") == "failed"]

print(f"\n📊 Execution Statistics:")
print(f"   • Total tables processed: {len(tables_to_profile)}")
print(f"   • Scan levels executed: {levels_to_run}")
print(f"   • Successful profiles: {len(successful_profiles)}")
print(f"   • Failed profiles: {len(failed_profiles)}")
print(f"   • Incremental mode: {'Enabled' if enable_incremental else 'Disabled'}")

# Show scan level results
print(f"\n📈 Scan Level Results:")
for level in range(1, 5):
    if level in levels_to_run:
        level_key = f"level_{level}"
        if level_key in scan_results:
            count = len(scan_results[level_key])
            print(f"   • Level {level}: {count} tables processed")

# Show quality scores if available (Level 3+)
if final_profiles and hasattr(final_profiles[0], 'data_quality_score'):
    quality_scores = [p.data_quality_score for p in final_profiles if p.data_quality_score is not None]
    if quality_scores:
        avg_quality = sum(quality_scores) / len(quality_scores)
        print(f"\n🏆 Data Quality:")
        print(f"   • Average Quality Score: {avg_quality:.2%}")
        print(f"   • Tables with scores: {len(quality_scores)}")

# Show relationship discovery results if Level 4 was run
if 4 in levels_to_run and scan_results.get("level_4"):
    print(f"\n🔗 Advanced Analysis (Level 4):")
    level_4_results = scan_results["level_4"]
    total_relationships = 0
    total_correlations = 0
    
    for profile in level_4_results:
        if hasattr(profile, 'relationships') and profile.relationships:
            total_relationships += len(profile.relationships)
        if hasattr(profile, 'correlations') and profile.correlations:
            # Count correlation pairs
            for col_corr in profile.correlations.values():
                total_correlations += len(col_corr)
    
    if total_relationships > 0:
        print(f"   • Cross-table relationships discovered: {total_relationships}")
    if total_correlations > 0:
        print(f"   • Column correlations found: {total_correlations}")
            
        # Display high-confidence relationships
        high_confidence = [r for r in relationships if r.confidence >= 0.7]
        if high_confidence:
            print(f"\n🏆 High Confidence Relationships ({len(high_confidence)}):")
            for rel in high_confidence[:10]:  # Show top 10
                print(f"  • {rel.source_table}.{rel.source_column.column_name} → "
                        f"{rel.target_table}.{rel.target_column.column_name} "
                        f"(confidence: {rel.confidence:.2f})")
                if rel.evidence:
                    print(f"    Evidence: {', '.join(rel.evidence[:3])}")
        
        # Generate relationship report
        if relationships:
            relationship_report = analyzer.generate_relationship_report(relationships)
            
            # Save relationship report to Files with consistent timestamp
            relationship_report_path = f"Files/profiling_reports/{run_timestamp}_cross_table_relationships.md"
            config_lakehouse.write_string_to_file(
                content=relationship_report,
                file_path=relationship_report_path,
                encoding="utf-8",
                mode="overwrite"
            )
            print(f"📄 Relationship report saved to: {relationship_report_path}")
    else:
            print("⚠️ Need at least 2 tables for cross-table relationship analysis")
            


print(f"\n✅ Profiling session completed successfully!")
print(f"   Execution ID: {execution_id}")
print(f"   Timestamp: {run_timestamp}")


# Save results to catalog if requested
if save_to_catalog and profiling_results:
    print(f"\n{'='*60}")
    print("Saving profiling results to catalog")
    print(f"{'='*60}")
    
    try:
        # Prepare profile results for storage
        profile_records = []
        history_records = []
        
        # Helper function to convert column profiles to serializable dicts
        def serialize_column_profile(col):
            """Convert a ColumnProfile to a serializable dictionary with enhanced relationship data."""
            col_dict = {}
            for key, value in col.__dict__.items():
                if value is None:
                    col_dict[key] = None
                elif isinstance(value, (datetime, date)):
                    col_dict[key] = value.isoformat()
                elif isinstance(value, dict):
                    # Handle dictionaries with potential datetime keys (e.g., value_distribution)
                    serialized_dict = {}
                    for k, v in value.items():
                        # Convert key to string if it's a datetime
                        if isinstance(k, (datetime, date)):
                            k = k.isoformat()
                        elif k is not None and not isinstance(k, (str, int, float, bool)):
                            k = str(k)
                        # Convert value if needed
                        if isinstance(v, (datetime, date)):
                            v = v.isoformat()
                        elif hasattr(v, '__dict__'):
                            v = str(v)
                        serialized_dict[k] = v
                    col_dict[key] = serialized_dict
                elif hasattr(value, '__dict__') and hasattr(value, '__dataclass_fields__'):
                    # Handle dataclass objects (ValueStatistics, NamingPattern, etc.)
                    nested_dict = {}
                    for nested_key, nested_value in value.__dict__.items():
                        if nested_value is None:
                            nested_dict[nested_key] = None
                        elif hasattr(nested_value, 'value'):  # Enum values
                            nested_dict[nested_key] = nested_value.value
                        elif isinstance(nested_value, (datetime, date)):
                            nested_dict[nested_key] = nested_value.isoformat()
                        elif isinstance(nested_value, list):
                            nested_dict[nested_key] = [str(item) if not isinstance(item, (str, int, float, bool, type(None))) else item for item in nested_value]
                        elif isinstance(nested_value, dict):
                            nested_dict[nested_key] = {str(k): str(v) for k, v in nested_value.items()}
                        else:
                            nested_dict[nested_key] = nested_value
                    col_dict[key] = nested_dict
                elif hasattr(value, 'value'):  # Handle enum values
                    col_dict[key] = value.value
                elif isinstance(value, list):
                    # Handle lists (e.g., top_distinct_values, business_rules)
                    serialized_list = []
                    for item in value:
                        if isinstance(item, (datetime, date)):
                            serialized_list.append(item.isoformat())
                        elif item is None or isinstance(item, (str, int, float, bool)):
                            serialized_list.append(item)
                        elif hasattr(item, '__dict__') and hasattr(item, '__dataclass_fields__'):
                            # Handle dataclass objects in lists
                            item_dict = {k: (v.value if hasattr(v, 'value') else v) for k, v in item.__dict__.items()}
                            serialized_list.append(item_dict)
                        else:
                            serialized_list.append(str(item))
                    col_dict[key] = serialized_list
                else:
                    col_dict[key] = value
            return col_dict
        
        for result in profiling_results:
            if result["status"] == "success":
                profile = result["profile"]
                
                # Create profile result record (enhanced with relationship data)
                profile_record = {
                    "profile_id": str(uuid.uuid4()),
                    "execution_id": result["execution_id"],
                    "table_name": result["table_name"],
                    "profile_timestamp": result["profile_timestamp"],
                    "profile_type": result["profile_type"],
                    "row_count": profile.row_count,
                    "column_count": profile.column_count,
                    "null_count": profile.null_count,
                    "duplicate_count": profile.duplicate_count,
                    "data_quality_score": profile.data_quality_score,
                    "column_profiles": json.dumps([serialize_column_profile(col) for col in profile.column_profiles], default=str),
                    "statistics": json.dumps(profile.statistics, default=str) if profile.statistics else None,
                    "data_quality_issues": json.dumps(profile.data_quality_issues, default=str) if profile.data_quality_issues else None,
                    "correlations": json.dumps(profile.correlations, default=str) if profile.correlations else None,
                    "entity_relationships": json.dumps(profile.entity_relationships.__dict__, default=str) if profile.entity_relationships else None,
                    "semantic_summary": json.dumps(profile.semantic_summary, default=str) if profile.semantic_summary else None,
                    "created_date": datetime.now().isoformat(),
                    "created_by": "data_profiling_processor"
                }
                profile_records.append(profile_record)
                
                # Create history record
                history_record = {
                    "history_id": str(uuid.uuid4()),
                    "table_name": result["table_name"],
                    "profile_timestamp": result["profile_timestamp"],
                    "row_count": profile.row_count,
                    "column_count": profile.column_count,
                    "data_quality_score": profile.data_quality_score,
                    "profile_type": result["profile_type"],
                    "execution_id": result["execution_id"]
                }
                history_records.append(history_record)
        
        # Write to profile results table
        if profile_records:
            from pyspark.sql.types import *
            
            # Define schema for profile results (enhanced with relationship fields)
            profile_schema = StructType([
                StructField("profile_id", StringType(), False),
                StructField("execution_id", StringType(), False),
                StructField("table_name", StringType(), False),
                StructField("profile_timestamp", StringType(), False),
                StructField("profile_type", StringType(), False),
                StructField("row_count", LongType(), True),
                StructField("column_count", IntegerType(), True),
                StructField("null_count", LongType(), True),
                StructField("duplicate_count", LongType(), True),
                StructField("data_quality_score", DoubleType(), True),
                StructField("column_profiles", StringType(), True),  # JSON with enhanced relationship data
                StructField("statistics", StringType(), True),
                StructField("data_quality_issues", StringType(), True),
                StructField("correlations", StringType(), True),  # JSON correlation matrix
                StructField("entity_relationships", StringType(), True),  # JSON relationship graph
                StructField("semantic_summary", StringType(), True),  # JSON semantic analysis summary
                StructField("created_date", StringType(), False),
                StructField("created_by", StringType(), False)
            ])
            
            profile_df = config_lakehouse.spark.createDataFrame(profile_records, profile_schema)
            config_lakehouse.write_to_table(
                df=profile_df,
                table_name="profile_results",
                mode="append"
            )
            print(f"✅ Saved {len(profile_records)} profile results")
        
        # Write to profile history table
        if history_records:
            history_schema = StructType([
                StructField("history_id", StringType(), False),
                StructField("table_name", StringType(), False),
                StructField("profile_timestamp", StringType(), False),
                StructField("row_count", LongType(), True),
                StructField("column_count", IntegerType(), True),
                StructField("data_quality_score", DoubleType(), True),
                StructField("profile_type", StringType(), False),
                StructField("execution_id", StringType(), False)
            ])
            
            history_df = config_lakehouse.spark.createDataFrame(history_records, history_schema)
            config_lakehouse.write_to_table(
                df=history_df,
                table_name="profile_history",
                mode="append"
            )
            print(f"✅ Saved {len(history_records)} history records")
            
    except Exception as e:
        import traceback
        print(f"❌ Error saving results to catalog: {e}")
        print("Stack trace:")
        traceback.print_exc()

# Display final summary
print(f"\n{'='*80}")
print("Data Profiling Execution Summary")
print(f"{'='*80}")
print(f"Execution ID: {execution_id}")
print(f"Total tables processed: {len(profiling_results)}")
successful = sum(1 for r in profiling_results if r.get("status") == "success")
failed = sum(1 for r in profiling_results if r.get("status") == "failed")
print(f"Successful: {successful}")
print(f"Failed: {failed}")

if successful > 0:
    avg_quality = sum(r.get("quality_score", 0) for r in profiling_results if r.get("quality_score")) / successful
    print(f"Average Quality Score: {avg_quality:.2%}")

print(f"\n✅ Data profiling completed successfully!")
{% endblock %}