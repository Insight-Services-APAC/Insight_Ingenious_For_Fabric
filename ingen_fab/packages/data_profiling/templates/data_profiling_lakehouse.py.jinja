{%- set datastore_type = "lakehouse" -%}
{%- set kernel_name = "synapse_pyspark" -%}
{%- set kernel_display_name = "PySpark (Synapse)" -%}
{%- set language_group = "synapse_pyspark" -%}
{%- set runtime_type = "pyspark" -%}
{%- set include_ddl_utils = false -%}

{%- extends "data_profiling_base.py.jinja" -%}

{% block datastore_specific_imports %}
from ingen_fab.python_libs.pyspark.lakehouse_utils import lakehouse_utils
{% endblock %}

{% block main_execution %}
print("=" * 80)
print(f"Data Profiling Processor - Lakehouse")
print(f"Execution Time: {datetime.now()}")
print("=" * 80)

# Initialize lakehouse utils (it creates its own spark session)
config_lakehouse = lakehouse_utils(
    target_workspace_id=configs.config_workspace_id,
    target_lakehouse_id=configs.config_lakehouse_id
)

# Map profile type string to enum
profile_type_map = {
    "basic": ProfileType.BASIC,
    "statistical": ProfileType.STATISTICAL,
    "data_quality": ProfileType.DATA_QUALITY,
    "relationship": ProfileType.RELATIONSHIP,
    "full": ProfileType.FULL
}
selected_profile_type = profile_type_map.get(profile_type.lower(), ProfileType.FULL)

# Initialize profiler with spark from lakehouse_utils
if enable_ultra_fast_mode:
    print("âš¡ Ultra-fast mode enabled - using single-pass profiling")
    from ingen_fab.python_libs.pyspark.ultra_fast_profiler import UltraFastProfiler
    profiler = UltraFastProfiler(config_lakehouse.spark)
elif enable_performance_mode:
    print("ðŸš€ Performance mode enabled - using optimized profiling")
    from ingen_fab.python_libs.pyspark.data_profiling_pyspark_optimized import OptimizedDataProfilingPySpark, ProfileConfig
    
    # Create performance config based on settings
    perf_config = ProfileConfig(
        max_top_values=top_values_limit,
        max_correlation_columns=max_correlation_columns,
        auto_sample_threshold=1_000_000 if auto_sample_large_tables else float('inf')
    )
    profiler = OptimizedDataProfilingPySpark(config_lakehouse.spark, perf_config)
else:
    profiler = DataProfilingPySpark(config_lakehouse.spark)

# Get list of tables to profile
if target_tables:
    tables_to_profile = target_tables
else:
    # Get active profiling configurations from catalog
    try:
        config_df = config_lakehouse.read_table("config_data_profiling")
        active_configs = config_df.filter("active_yn = 'Y'").collect()
        tables_to_profile = [row.table_name for row in active_configs]
        print(f"Found {len(tables_to_profile)} active tables to profile from configuration")
    except Exception as e:
        print(f"No configuration table found, will profile all tables: {e}")
        # Profile all tables in the lakehouse
        tables_to_profile = config_lakehouse.list_tables()
        print(f"Found {len(tables_to_profile)} tables in lakehouse")

# Profile each table
profiling_results = []
execution_id = str(uuid.uuid4())
# Generate consistent timestamp for this profiling run (prepended format)
run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

for table_name in tables_to_profile:
    print(f"\n{'='*60}")
    print(f"Profiling table: {table_name}")
    print(f"{'='*60}")
    
    try:
        # Read table
        df = config_lakehouse.read_table(table_name)
        
        # Check table size for adaptive sampling
        table_row_count = df.count()
        effective_sample_size = sample_size
        
        # With UltraFast mode, we can handle large datasets without sampling
        if auto_sample_large_tables and not sample_size and table_row_count > 1_000_000 and not enable_ultra_fast_mode:
            if table_row_count > 50_000_000:
                effective_sample_size = 0.01  # 1% for very large tables (>50M rows)
            elif table_row_count > 10_000_000:
                effective_sample_size = 0.05  # 5% for large tables (10M-50M rows)  
            else:
                effective_sample_size = 0.1   # 10% for medium-large tables (1M-10M rows)
            
            print(f"ðŸŽ¯ Auto-sampling enabled: {table_row_count:,} rows -> {effective_sample_size:.1%} sample")
        elif enable_ultra_fast_mode and table_row_count > 1_000_000:
            print(f"âš¡ UltraFast mode: Processing full dataset ({table_row_count:,} rows) without sampling")
        
        # Profile dataset
        profile = profiler.profile_dataset(
            dataset=df,
            profile_type=selected_profile_type,
            sample_size=effective_sample_size
        )
        
        # Add metadata
        profile_result = {
            "execution_id": execution_id,
            "table_name": table_name,
            "profile_timestamp": datetime.now().isoformat(),
            "profile_type": profile_type,
            "profile": profile,
            "status": "success"
        }
        
        # Generate quality report if requested
        if generate_report:
            report = profiler.generate_quality_report(profile, output_format)
            profile_result["report"] = report
            
            # Save report to Files with consistent timestamp (date/time prepended)
            if output_format in ["html", "yaml", "json", "markdown"]:
                # Determine file extension
                ext_map = {
                    "html": "html",
                    "yaml": "yaml", 
                    "json": "json",
                    "markdown": "md"
                }
                extension = ext_map.get(output_format, "txt")
                # Prepend timestamp to filename for better sorting
                report_path = f"Files/profiling_reports/{run_timestamp}_{table_name}.{extension}"
                config_lakehouse.write_string_to_file(
                    content=report,
                    file_path=report_path,
                    encoding="utf-8",
                    mode="overwrite"
                )

                print(f"Report saved to: {report_path}")
        
        # Generate YAML output separately if requested (independent of report format)
        if generate_yaml_output and output_format != "yaml":  # Avoid duplicate if already generating YAML
            yaml_report = profiler._generate_yaml_report(profile)
            yaml_path = f"Files/profiling_reports/{run_timestamp}_{table_name}.yaml"
            config_lakehouse.write_string_to_file(
                content=yaml_report,
                file_path=yaml_path,
                encoding="utf-8",
                mode="overwrite"
            )
            print(f"YAML output saved to: {yaml_path}")
        
        # Calculate data quality score
        if profile.data_quality_score is not None:
            profile_result["quality_score"] = profile.data_quality_score
        
        profiling_results.append(profile_result)
        
        # Display summary
        print(f"âœ… Profiling complete for {table_name}")
        print(f"  Rows: {profile.row_count:,}")
        print(f"  Columns: {profile.column_count}")
        if profile.data_quality_score:
            print(f"  Quality Score: {profile.data_quality_score:.2%}")
        
    except Exception as e:
        import traceback
        print(f"âŒ Error profiling {table_name}: {e}")
        print("Stack trace:")
        traceback.print_exc()
        profiling_results.append({
            "execution_id": execution_id,
            "table_name": table_name,
            "profile_timestamp": datetime.now().isoformat(),
            "status": "failed",
            "error": str(e)
        })

# Cross-table relationship discovery (if relationship profile type was used)
if selected_profile_type in [ProfileType.RELATIONSHIP, ProfileType.FULL] and cross_profile_available:
    print(f"\n{'='*60}")
    print("Cross-Table Relationship Discovery")
    print(f"{'='*60}")
    
    try:
        # Collect successful profiles for relationship analysis
        relationship_profiles = {}
        for result in profiling_results:
            if result["status"] == "success":
                profile = result["profile"]
                relationship_profiles[result["table_name"]] = profile
        
        if len(relationship_profiles) >= 2:
            print(f"Analyzing relationships across {len(relationship_profiles)} tables...")
            
            # Initialize cross-profile analyzer
            analyzer = CrossProfileAnalyzer(min_confidence=0.5)
            
            # Discover relationships
            relationships = analyzer.analyze_profiles(relationship_profiles)
            
            print(f"âœ… Discovered {len(relationships)} potential relationships")
            
            # Display high-confidence relationships
            high_confidence = [r for r in relationships if r.confidence >= 0.7]
            if high_confidence:
                print(f"\nðŸ† High Confidence Relationships ({len(high_confidence)}):")
                for rel in high_confidence[:10]:  # Show top 10
                    print(f"  â€¢ {rel.source_table}.{rel.source_column.column_name} â†’ "
                          f"{rel.target_table}.{rel.target_column.column_name} "
                          f"(confidence: {rel.confidence:.2f})")
                    if rel.evidence:
                        print(f"    Evidence: {', '.join(rel.evidence[:3])}")
            
            # Generate relationship report
            if relationships:
                relationship_report = analyzer.generate_relationship_report(relationships)
                
                # Save relationship report to Files with consistent timestamp
                relationship_report_path = f"Files/profiling_reports/{run_timestamp}_cross_table_relationships.md"
                config_lakehouse.write_string_to_file(
                    content=relationship_report,
                    file_path=relationship_report_path,
                    encoding="utf-8",
                    mode="overwrite"
                )
                print(f"ðŸ“„ Relationship report saved to: {relationship_report_path}")
        else:
            print("âš ï¸ Need at least 2 tables for cross-table relationship analysis")
            
    except Exception as e:
        import traceback
        print(f"âŒ Error in cross-table relationship discovery: {e}")
        print("Stack trace:")
        traceback.print_exc()

# Save results to catalog if requested
if save_to_catalog and profiling_results:
    print(f"\n{'='*60}")
    print("Saving profiling results to catalog")
    print(f"{'='*60}")
    
    try:
        # Prepare profile results for storage
        profile_records = []
        history_records = []
        
        # Helper function to convert column profiles to serializable dicts
        def serialize_column_profile(col):
            """Convert a ColumnProfile to a serializable dictionary with enhanced relationship data."""
            col_dict = {}
            for key, value in col.__dict__.items():
                if value is None:
                    col_dict[key] = None
                elif isinstance(value, (datetime, date)):
                    col_dict[key] = value.isoformat()
                elif isinstance(value, dict):
                    # Handle dictionaries with potential datetime keys (e.g., value_distribution)
                    serialized_dict = {}
                    for k, v in value.items():
                        # Convert key to string if it's a datetime
                        if isinstance(k, (datetime, date)):
                            k = k.isoformat()
                        elif k is not None and not isinstance(k, (str, int, float, bool)):
                            k = str(k)
                        # Convert value if needed
                        if isinstance(v, (datetime, date)):
                            v = v.isoformat()
                        elif hasattr(v, '__dict__'):
                            v = str(v)
                        serialized_dict[k] = v
                    col_dict[key] = serialized_dict
                elif hasattr(value, '__dict__') and hasattr(value, '__dataclass_fields__'):
                    # Handle dataclass objects (ValueStatistics, NamingPattern, etc.)
                    nested_dict = {}
                    for nested_key, nested_value in value.__dict__.items():
                        if nested_value is None:
                            nested_dict[nested_key] = None
                        elif hasattr(nested_value, 'value'):  # Enum values
                            nested_dict[nested_key] = nested_value.value
                        elif isinstance(nested_value, (datetime, date)):
                            nested_dict[nested_key] = nested_value.isoformat()
                        elif isinstance(nested_value, list):
                            nested_dict[nested_key] = [str(item) if not isinstance(item, (str, int, float, bool, type(None))) else item for item in nested_value]
                        elif isinstance(nested_value, dict):
                            nested_dict[nested_key] = {str(k): str(v) for k, v in nested_value.items()}
                        else:
                            nested_dict[nested_key] = nested_value
                    col_dict[key] = nested_dict
                elif hasattr(value, 'value'):  # Handle enum values
                    col_dict[key] = value.value
                elif isinstance(value, list):
                    # Handle lists (e.g., top_distinct_values, business_rules)
                    serialized_list = []
                    for item in value:
                        if isinstance(item, (datetime, date)):
                            serialized_list.append(item.isoformat())
                        elif item is None or isinstance(item, (str, int, float, bool)):
                            serialized_list.append(item)
                        elif hasattr(item, '__dict__') and hasattr(item, '__dataclass_fields__'):
                            # Handle dataclass objects in lists
                            item_dict = {k: (v.value if hasattr(v, 'value') else v) for k, v in item.__dict__.items()}
                            serialized_list.append(item_dict)
                        else:
                            serialized_list.append(str(item))
                    col_dict[key] = serialized_list
                else:
                    col_dict[key] = value
            return col_dict
        
        for result in profiling_results:
            if result["status"] == "success":
                profile = result["profile"]
                
                # Create profile result record (enhanced with relationship data)
                profile_record = {
                    "profile_id": str(uuid.uuid4()),
                    "execution_id": result["execution_id"],
                    "table_name": result["table_name"],
                    "profile_timestamp": result["profile_timestamp"],
                    "profile_type": result["profile_type"],
                    "row_count": profile.row_count,
                    "column_count": profile.column_count,
                    "null_count": profile.null_count,
                    "duplicate_count": profile.duplicate_count,
                    "data_quality_score": profile.data_quality_score,
                    "column_profiles": json.dumps([serialize_column_profile(col) for col in profile.column_profiles], default=str),
                    "statistics": json.dumps(profile.statistics, default=str) if profile.statistics else None,
                    "data_quality_issues": json.dumps(profile.data_quality_issues, default=str) if profile.data_quality_issues else None,
                    "correlations": json.dumps(profile.correlations, default=str) if profile.correlations else None,
                    "entity_relationships": json.dumps(profile.entity_relationships.__dict__, default=str) if profile.entity_relationships else None,
                    "semantic_summary": json.dumps(profile.semantic_summary, default=str) if profile.semantic_summary else None,
                    "created_date": datetime.now().isoformat(),
                    "created_by": "data_profiling_processor"
                }
                profile_records.append(profile_record)
                
                # Create history record
                history_record = {
                    "history_id": str(uuid.uuid4()),
                    "table_name": result["table_name"],
                    "profile_timestamp": result["profile_timestamp"],
                    "row_count": profile.row_count,
                    "column_count": profile.column_count,
                    "data_quality_score": profile.data_quality_score,
                    "profile_type": result["profile_type"],
                    "execution_id": result["execution_id"]
                }
                history_records.append(history_record)
        
        # Write to profile results table
        if profile_records:
            from pyspark.sql.types import *
            
            # Define schema for profile results (enhanced with relationship fields)
            profile_schema = StructType([
                StructField("profile_id", StringType(), False),
                StructField("execution_id", StringType(), False),
                StructField("table_name", StringType(), False),
                StructField("profile_timestamp", StringType(), False),
                StructField("profile_type", StringType(), False),
                StructField("row_count", LongType(), True),
                StructField("column_count", IntegerType(), True),
                StructField("null_count", LongType(), True),
                StructField("duplicate_count", LongType(), True),
                StructField("data_quality_score", DoubleType(), True),
                StructField("column_profiles", StringType(), True),  # JSON with enhanced relationship data
                StructField("statistics", StringType(), True),
                StructField("data_quality_issues", StringType(), True),
                StructField("correlations", StringType(), True),  # JSON correlation matrix
                StructField("entity_relationships", StringType(), True),  # JSON relationship graph
                StructField("semantic_summary", StringType(), True),  # JSON semantic analysis summary
                StructField("created_date", StringType(), False),
                StructField("created_by", StringType(), False)
            ])
            
            profile_df = config_lakehouse.spark.createDataFrame(profile_records, profile_schema)
            config_lakehouse.write_to_table(
                df=profile_df,
                table_name="profile_results",
                mode="append"
            )
            print(f"âœ… Saved {len(profile_records)} profile results")
        
        # Write to profile history table
        if history_records:
            history_schema = StructType([
                StructField("history_id", StringType(), False),
                StructField("table_name", StringType(), False),
                StructField("profile_timestamp", StringType(), False),
                StructField("row_count", LongType(), True),
                StructField("column_count", IntegerType(), True),
                StructField("data_quality_score", DoubleType(), True),
                StructField("profile_type", StringType(), False),
                StructField("execution_id", StringType(), False)
            ])
            
            history_df = config_lakehouse.spark.createDataFrame(history_records, history_schema)
            config_lakehouse.write_to_table(
                df=history_df,
                table_name="profile_history",
                mode="append"
            )
            print(f"âœ… Saved {len(history_records)} history records")
            
    except Exception as e:
        import traceback
        print(f"âŒ Error saving results to catalog: {e}")
        print("Stack trace:")
        traceback.print_exc()

# Display final summary
print(f"\n{'='*80}")
print("Data Profiling Execution Summary")
print(f"{'='*80}")
print(f"Execution ID: {execution_id}")
print(f"Total tables processed: {len(profiling_results)}")
successful = sum(1 for r in profiling_results if r.get("status") == "success")
failed = sum(1 for r in profiling_results if r.get("status") == "failed")
print(f"Successful: {successful}")
print(f"Failed: {failed}")

if successful > 0:
    avg_quality = sum(r.get("quality_score", 0) for r in profiling_results if r.get("quality_score")) / successful
    print(f"Average Quality Score: {avg_quality:.2%}")

print(f"\nâœ… Data profiling completed successfully!")
{% endblock %}