{%- set datastore_type = "lakehouse" -%}
{%- set kernel_name = "synapse_pyspark" -%}
{%- set kernel_display_name = "PySpark (Synapse)" -%}
{%- set language_group = "synapse_pyspark" -%}
{%- set runtime_type = "pyspark" -%}
{%- set include_ddl_utils = false -%}

{%- extends "data_profiling_base.py.jinja" -%}

{% block datastore_specific_imports %}
from ingen_fab.python_libs.pyspark.lakehouse_utils import lakehouse_utils
{% endblock %}

{% block main_execution %}
print("=" * 80)
print(f"Data Profiling Processor - Lakehouse")
print(f"Execution Time: {datetime.now()}")
print("=" * 80)

# Initialize lakehouse utils (it creates its own spark session)
config_lakehouse = lakehouse_utils(
    target_workspace_id=configs.config_workspace_id,
    target_lakehouse_id=configs.config_lakehouse_id
)

# Map profile type string to enum
profile_type_map = {
    "basic": ProfileType.BASIC,
    "statistical": ProfileType.STATISTICAL,
    "data_quality": ProfileType.DATA_QUALITY,
    "relationship": ProfileType.RELATIONSHIP,
    "full": ProfileType.FULL
}
selected_profile_type = profile_type_map.get(profile_type.lower(), ProfileType.FULL)

# Initialize profiler with spark from lakehouse_utils
if enable_ultra_fast_mode:
    print("âš¡ Ultra-fast mode enabled - using single-pass profiling")
    # Inline ultra-fast profiler for immediate use
    from typing import Any, Dict, List, Optional
    from pyspark.sql import functions as F
    from pyspark.sql.types import NumericType, StringType, DateType, TimestampType, IntegerType, LongType, FloatType, DoubleType
    
    class UltraFastProfiler:
        def __init__(self, spark):
            self.spark = spark
        
        def profile_dataset(self, dataset, profile_type=ProfileType.BASIC, sample_size=None):
            import time
            start_time = time.time()
            
            if isinstance(dataset, str):
                df = self.spark.table(dataset) 
            else:
                df = dataset
            
            # Cache for multiple operations
            df = df.cache()
            
            # Get row count
            row_count = df.count()
            
            # Apply sampling
            if sample_size and 0 < sample_size < 1:
                df = df.sample(fraction=sample_size, seed=42)
                scale_factor = row_count / df.count()
            else:
                scale_factor = 1.0
            
            # Single aggregation for ALL columns
            agg_exprs = []
            for col_name in df.columns:
                col_type = dict(df.dtypes)[col_name]
                
                # Core stats for all columns
                agg_exprs.extend([
                    F.count(F.when(F.col(col_name).isNotNull(), 1)).alias(f"{col_name}__non_null"),
                    # For distinct count, we need to count unique values including nulls (like original profiler)
                    F.countDistinct(F.when(F.col(col_name).isNotNull(), F.col(col_name))).alias(f"{col_name}__distinct_non_null"),
                    F.count(F.when(F.col(col_name).isNull(), 1)).alias(f"{col_name}__null_count"),
                ])
                
                # Type-specific stats
                if col_type in ['int', 'bigint', 'float', 'double', 'decimal']:
                    agg_exprs.extend([
                        F.min(col_name).alias(f"{col_name}__min"),
                        F.max(col_name).alias(f"{col_name}__max"),
                        F.mean(col_name).alias(f"{col_name}__mean"),
                    ])
                elif col_type == 'string':
                    agg_exprs.extend([
                        F.min(F.length(col_name)).alias(f"{col_name}__min_len"),
                        F.max(F.length(col_name)).alias(f"{col_name}__max_len"),
                        F.mean(F.length(col_name)).alias(f"{col_name}__mean_len"),
                    ])
                elif col_type in ['date', 'timestamp']:
                    agg_exprs.extend([
                        F.min(col_name).alias(f"{col_name}__min"),
                        F.max(col_name).alias(f"{col_name}__max"),
                    ])
            
            # Execute single query
            batch_result = df.agg(*agg_exprs).collect()[0].asDict()
            
            # Build column profiles
            column_profiles = []
            for col_name in df.columns:
                non_null = batch_result[f"{col_name}__non_null"]
                distinct_non_null = batch_result[f"{col_name}__distinct_non_null"]
                has_nulls = batch_result[f"{col_name}__null_count"] > 0
                null_count = int(row_count - (non_null * scale_factor))
                # Calculate distinct count including nulls (if any nulls exist, add 1 to distinct non-null count)
                distinct_count = int(distinct_non_null * scale_factor) + (1 if has_nulls else 0)
                
                profile = ColumnProfile(
                    column_name=col_name,
                    data_type=dict(df.dtypes)[col_name],
                    null_count=null_count,
                    null_percentage=(null_count / row_count * 100) if row_count > 0 else 0,
                    distinct_count=distinct_count,
                    distinct_percentage=(distinct_count / row_count * 100) if row_count > 0 else 0,
                    completeness=1 - (null_count / row_count) if row_count > 0 else 0,
                    uniqueness=distinct_count / row_count if row_count > 0 else 0,
                )
                
                # Add type-specific values
                col_type = dict(df.dtypes)[col_name]
                if col_type in ['int', 'bigint', 'float', 'double', 'decimal']:
                    profile.min_value = batch_result.get(f"{col_name}__min")
                    profile.max_value = batch_result.get(f"{col_name}__max") 
                    profile.mean_value = batch_result.get(f"{col_name}__mean")
                elif col_type == 'string':
                    profile.min_value = batch_result.get(f"{col_name}__min_len")
                    profile.max_value = batch_result.get(f"{col_name}__max_len")
                    profile.mean_value = batch_result.get(f"{col_name}__mean_len")
                elif col_type in ['date', 'timestamp']:
                    profile.min_value = batch_result.get(f"{col_name}__min")
                    profile.max_value = batch_result.get(f"{col_name}__max")
                
                # Quick top values for first few columns only
                if len(column_profiles) < 3:  # Limit to first 3 columns
                    try:
                        top_vals = df.groupBy(col_name).count().orderBy(F.desc("count")).limit(5).rdd.map(lambda x: x[0]).collect()
                        profile.top_distinct_values = top_vals
                    except:
                        profile.top_distinct_values = []
                
                column_profiles.append(profile)
            
            # Calculate overall null count (rows with at least one null value)
            null_conditions = [F.col(col_name).isNull() for col_name in df.columns]
            any_null_condition = null_conditions[0]
            for condition in null_conditions[1:]:
                any_null_condition = any_null_condition | condition
            total_null_count = df.filter(any_null_condition).count()
            
            df.unpersist()
            
            # Calculate quality score only for DATA_QUALITY and FULL profile types (matching original profiler)
            if profile_type in [ProfileType.DATA_QUALITY, ProfileType.FULL]:
                completeness_scores = [cp.completeness for cp in column_profiles if cp.completeness is not None]
                quality_score = sum(completeness_scores) / len(completeness_scores) if completeness_scores else 0.0
            else:
                quality_score = None
            
            duration = time.time() - start_time
            return DatasetProfile(
                dataset_name=dataset if isinstance(dataset, str) else "dataframe",
                row_count=row_count,
                column_count=len(df.columns),
                profile_timestamp=datetime.now().isoformat(),
                column_profiles=column_profiles,
                null_count=total_null_count,
                duplicate_count=0,  # Skip expensive operation
                data_quality_score=quality_score,
                statistics={"profiling_duration_seconds": round(duration, 2)}
            )
        
        def generate_quality_report(self, profile, output_format="yaml"):
            """Generate quality report - delegate to full profiler for YAML generation."""
            if output_format == "yaml":
                # Import YAML generation from main profiler
                from ingen_fab.python_libs.pyspark.data_profiling_pyspark import DataProfilingPySpark
                full_profiler = DataProfilingPySpark(self.spark)
                return full_profiler._generate_yaml_report(profile)
            else:
                # Simple fallback for other formats
                quality_display = f"{profile.data_quality_score:.2%}" if profile.data_quality_score else "N/A"
                columns_summary = "\n".join([f"- {cp.column_name}: {cp.data_type}, {cp.null_percentage:.1f}% nulls, {cp.distinct_count:,} distinct" for cp in profile.column_profiles])
                
                return f"""# Ultra-Fast Profile Report
Dataset: {profile.dataset_name}
Timestamp: {profile.profile_timestamp}
Rows: {profile.row_count:,}
Columns: {profile.column_count}
Quality Score: {quality_display}
Duration: {profile.statistics.get('profiling_duration_seconds', 'N/A')}s

## Column Summary
{columns_summary}
"""
    
    profiler = UltraFastProfiler(config_lakehouse.spark)

elif enable_performance_mode:
    print("ðŸš€ Performance mode enabled - using optimized profiling")
    from ingen_fab.python_libs.pyspark.data_profiling_pyspark_optimized import OptimizedDataProfilingPySpark, ProfileConfig
    
    # Create performance config based on settings
    perf_config = ProfileConfig(
        max_top_values=top_values_limit,
        max_correlation_columns=max_correlation_columns,
        auto_sample_threshold=1_000_000 if auto_sample_large_tables else float('inf')
    )
    profiler = OptimizedDataProfilingPySpark(config_lakehouse.spark, perf_config)
else:
    profiler = DataProfilingPySpark(config_lakehouse.spark)

# Get list of tables to profile
if target_tables:
    tables_to_profile = target_tables
else:
    # Get active profiling configurations from catalog
    try:
        config_df = config_lakehouse.read_table("config_data_profiling")
        active_configs = config_df.filter("active_yn = 'Y'").collect()
        tables_to_profile = [row.table_name for row in active_configs]
        print(f"Found {len(tables_to_profile)} active tables to profile from configuration")
    except Exception as e:
        print(f"No configuration table found, will profile all tables: {e}")
        # Profile all tables in the lakehouse
        tables_to_profile = config_lakehouse.list_tables()
        print(f"Found {len(tables_to_profile)} tables in lakehouse")

# Profile each table
profiling_results = []
execution_id = str(uuid.uuid4())

for table_name in tables_to_profile:
    print(f"\n{'='*60}")
    print(f"Profiling table: {table_name}")
    print(f"{'='*60}")
    
    try:
        # Read table
        df = config_lakehouse.read_table(table_name)
        
        # Check table size for adaptive sampling
        table_row_count = df.count()
        effective_sample_size = sample_size
        
        # With UltraFast mode, we can handle large datasets without sampling
        if auto_sample_large_tables and not sample_size and table_row_count > 1_000_000 and not enable_ultra_fast_mode:
            if table_row_count > 50_000_000:
                effective_sample_size = 0.01  # 1% for very large tables (>50M rows)
            elif table_row_count > 10_000_000:
                effective_sample_size = 0.05  # 5% for large tables (10M-50M rows)  
            else:
                effective_sample_size = 0.1   # 10% for medium-large tables (1M-10M rows)
            
            print(f"ðŸŽ¯ Auto-sampling enabled: {table_row_count:,} rows -> {effective_sample_size:.1%} sample")
        elif enable_ultra_fast_mode and table_row_count > 1_000_000:
            print(f"âš¡ UltraFast mode: Processing full dataset ({table_row_count:,} rows) without sampling")
        
        # Profile dataset
        profile = profiler.profile_dataset(
            dataset=df,
            profile_type=selected_profile_type,
            sample_size=effective_sample_size
        )
        
        # Add metadata
        profile_result = {
            "execution_id": execution_id,
            "table_name": table_name,
            "profile_timestamp": datetime.now().isoformat(),
            "profile_type": profile_type,
            "profile": profile,
            "status": "success"
        }
        
        # Generate quality report if requested
        if generate_report:
            report = profiler.generate_quality_report(profile, output_format)
            profile_result["report"] = report
            
            # Save report to Files
            if output_format in ["html", "yaml", "json", "markdown"]:
                # Determine file extension
                ext_map = {
                    "html": "html",
                    "yaml": "yaml", 
                    "json": "json",
                    "markdown": "md"
                }
                extension = ext_map.get(output_format, "txt")
                report_path = f"Files/profiling_reports/{table_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{extension}"
                config_lakehouse.write_string_to_file(
                    content=report,
                    file_path=report_path,
                    encoding="utf-8",
                    mode="overwrite"
                )

                print(f"Report saved to: {report_path}")
        
        # Calculate data quality score
        if profile.data_quality_score is not None:
            profile_result["quality_score"] = profile.data_quality_score
        
        profiling_results.append(profile_result)
        
        # Display summary
        print(f"âœ… Profiling complete for {table_name}")
        print(f"  Rows: {profile.row_count:,}")
        print(f"  Columns: {profile.column_count}")
        if profile.data_quality_score:
            print(f"  Quality Score: {profile.data_quality_score:.2%}")
        
    except Exception as e:
        import traceback
        print(f"âŒ Error profiling {table_name}: {e}")
        print("Stack trace:")
        traceback.print_exc()
        profiling_results.append({
            "execution_id": execution_id,
            "table_name": table_name,
            "profile_timestamp": datetime.now().isoformat(),
            "status": "failed",
            "error": str(e)
        })

# Cross-table relationship discovery (if relationship profile type was used)
if selected_profile_type in [ProfileType.RELATIONSHIP, ProfileType.FULL] and cross_profile_available:
    print(f"\n{'='*60}")
    print("Cross-Table Relationship Discovery")
    print(f"{'='*60}")
    
    try:
        # Collect successful profiles for relationship analysis
        relationship_profiles = {}
        for result in profiling_results:
            if result["status"] == "success":
                profile = result["profile"]
                relationship_profiles[result["table_name"]] = profile
        
        if len(relationship_profiles) >= 2:
            print(f"Analyzing relationships across {len(relationship_profiles)} tables...")
            
            # Initialize cross-profile analyzer
            analyzer = CrossProfileAnalyzer(min_confidence=0.5)
            
            # Discover relationships
            relationships = analyzer.analyze_profiles(relationship_profiles)
            
            print(f"âœ… Discovered {len(relationships)} potential relationships")
            
            # Display high-confidence relationships
            high_confidence = [r for r in relationships if r.confidence >= 0.7]
            if high_confidence:
                print(f"\nðŸ† High Confidence Relationships ({len(high_confidence)}):")
                for rel in high_confidence[:10]:  # Show top 10
                    print(f"  â€¢ {rel.source_table}.{rel.source_column.column_name} â†’ "
                          f"{rel.target_table}.{rel.target_column.column_name} "
                          f"(confidence: {rel.confidence:.2f})")
                    if rel.evidence:
                        print(f"    Evidence: {', '.join(rel.evidence[:3])}")
            
            # Generate relationship report
            if relationships:
                relationship_report = analyzer.generate_relationship_report(relationships)
                
                # Save relationship report to Files
                relationship_report_path = f"Files/profiling_reports/cross_table_relationships_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
                config_lakehouse.write_string_to_file(
                    content=relationship_report,
                    file_path=relationship_report_path,
                    encoding="utf-8",
                    mode="overwrite"
                )
                print(f"ðŸ“„ Relationship report saved to: {relationship_report_path}")
        else:
            print("âš ï¸ Need at least 2 tables for cross-table relationship analysis")
            
    except Exception as e:
        import traceback
        print(f"âŒ Error in cross-table relationship discovery: {e}")
        print("Stack trace:")
        traceback.print_exc()

# Save results to catalog if requested
if save_to_catalog and profiling_results:
    print(f"\n{'='*60}")
    print("Saving profiling results to catalog")
    print(f"{'='*60}")
    
    try:
        # Prepare profile results for storage
        profile_records = []
        history_records = []
        
        # Helper function to convert column profiles to serializable dicts
        def serialize_column_profile(col):
            """Convert a ColumnProfile to a serializable dictionary with enhanced relationship data."""
            col_dict = {}
            for key, value in col.__dict__.items():
                if value is None:
                    col_dict[key] = None
                elif isinstance(value, (datetime, date)):
                    col_dict[key] = value.isoformat()
                elif isinstance(value, dict):
                    # Handle dictionaries with potential datetime keys (e.g., value_distribution)
                    serialized_dict = {}
                    for k, v in value.items():
                        # Convert key to string if it's a datetime
                        if isinstance(k, (datetime, date)):
                            k = k.isoformat()
                        elif k is not None and not isinstance(k, (str, int, float, bool)):
                            k = str(k)
                        # Convert value if needed
                        if isinstance(v, (datetime, date)):
                            v = v.isoformat()
                        elif hasattr(v, '__dict__'):
                            v = str(v)
                        serialized_dict[k] = v
                    col_dict[key] = serialized_dict
                elif hasattr(value, '__dict__') and hasattr(value, '__dataclass_fields__'):
                    # Handle dataclass objects (ValueStatistics, NamingPattern, etc.)
                    nested_dict = {}
                    for nested_key, nested_value in value.__dict__.items():
                        if nested_value is None:
                            nested_dict[nested_key] = None
                        elif hasattr(nested_value, 'value'):  # Enum values
                            nested_dict[nested_key] = nested_value.value
                        elif isinstance(nested_value, (datetime, date)):
                            nested_dict[nested_key] = nested_value.isoformat()
                        elif isinstance(nested_value, list):
                            nested_dict[nested_key] = [str(item) if not isinstance(item, (str, int, float, bool, type(None))) else item for item in nested_value]
                        elif isinstance(nested_value, dict):
                            nested_dict[nested_key] = {str(k): str(v) for k, v in nested_value.items()}
                        else:
                            nested_dict[nested_key] = nested_value
                    col_dict[key] = nested_dict
                elif hasattr(value, 'value'):  # Handle enum values
                    col_dict[key] = value.value
                elif isinstance(value, list):
                    # Handle lists (e.g., top_distinct_values, business_rules)
                    serialized_list = []
                    for item in value:
                        if isinstance(item, (datetime, date)):
                            serialized_list.append(item.isoformat())
                        elif item is None or isinstance(item, (str, int, float, bool)):
                            serialized_list.append(item)
                        elif hasattr(item, '__dict__') and hasattr(item, '__dataclass_fields__'):
                            # Handle dataclass objects in lists
                            item_dict = {k: (v.value if hasattr(v, 'value') else v) for k, v in item.__dict__.items()}
                            serialized_list.append(item_dict)
                        else:
                            serialized_list.append(str(item))
                    col_dict[key] = serialized_list
                else:
                    col_dict[key] = value
            return col_dict
        
        for result in profiling_results:
            if result["status"] == "success":
                profile = result["profile"]
                
                # Create profile result record (enhanced with relationship data)
                profile_record = {
                    "profile_id": str(uuid.uuid4()),
                    "execution_id": result["execution_id"],
                    "table_name": result["table_name"],
                    "profile_timestamp": result["profile_timestamp"],
                    "profile_type": result["profile_type"],
                    "row_count": profile.row_count,
                    "column_count": profile.column_count,
                    "null_count": profile.null_count,
                    "duplicate_count": profile.duplicate_count,
                    "data_quality_score": profile.data_quality_score,
                    "column_profiles": json.dumps([serialize_column_profile(col) for col in profile.column_profiles], default=str),
                    "statistics": json.dumps(profile.statistics, default=str) if profile.statistics else None,
                    "data_quality_issues": json.dumps(profile.data_quality_issues, default=str) if profile.data_quality_issues else None,
                    "correlations": json.dumps(profile.correlations, default=str) if profile.correlations else None,
                    "entity_relationships": json.dumps(profile.entity_relationships.__dict__, default=str) if profile.entity_relationships else None,
                    "semantic_summary": json.dumps(profile.semantic_summary, default=str) if profile.semantic_summary else None,
                    "created_date": datetime.now().isoformat(),
                    "created_by": "data_profiling_processor"
                }
                profile_records.append(profile_record)
                
                # Create history record
                history_record = {
                    "history_id": str(uuid.uuid4()),
                    "table_name": result["table_name"],
                    "profile_timestamp": result["profile_timestamp"],
                    "row_count": profile.row_count,
                    "column_count": profile.column_count,
                    "data_quality_score": profile.data_quality_score,
                    "profile_type": result["profile_type"],
                    "execution_id": result["execution_id"]
                }
                history_records.append(history_record)
        
        # Write to profile results table
        if profile_records:
            from pyspark.sql.types import *
            
            # Define schema for profile results (enhanced with relationship fields)
            profile_schema = StructType([
                StructField("profile_id", StringType(), False),
                StructField("execution_id", StringType(), False),
                StructField("table_name", StringType(), False),
                StructField("profile_timestamp", StringType(), False),
                StructField("profile_type", StringType(), False),
                StructField("row_count", LongType(), True),
                StructField("column_count", IntegerType(), True),
                StructField("null_count", LongType(), True),
                StructField("duplicate_count", LongType(), True),
                StructField("data_quality_score", DoubleType(), True),
                StructField("column_profiles", StringType(), True),  # JSON with enhanced relationship data
                StructField("statistics", StringType(), True),
                StructField("data_quality_issues", StringType(), True),
                StructField("correlations", StringType(), True),  # JSON correlation matrix
                StructField("entity_relationships", StringType(), True),  # JSON relationship graph
                StructField("semantic_summary", StringType(), True),  # JSON semantic analysis summary
                StructField("created_date", StringType(), False),
                StructField("created_by", StringType(), False)
            ])
            
            profile_df = config_lakehouse.spark.createDataFrame(profile_records, profile_schema)
            config_lakehouse.write_to_table(
                df=profile_df,
                table_name="profile_results",
                mode="append"
            )
            print(f"âœ… Saved {len(profile_records)} profile results")
        
        # Write to profile history table
        if history_records:
            history_schema = StructType([
                StructField("history_id", StringType(), False),
                StructField("table_name", StringType(), False),
                StructField("profile_timestamp", StringType(), False),
                StructField("row_count", LongType(), True),
                StructField("column_count", IntegerType(), True),
                StructField("data_quality_score", DoubleType(), True),
                StructField("profile_type", StringType(), False),
                StructField("execution_id", StringType(), False)
            ])
            
            history_df = config_lakehouse.spark.createDataFrame(history_records, history_schema)
            config_lakehouse.write_to_table(
                df=history_df,
                table_name="profile_history",
                mode="append"
            )
            print(f"âœ… Saved {len(history_records)} history records")
            
    except Exception as e:
        import traceback
        print(f"âŒ Error saving results to catalog: {e}")
        print("Stack trace:")
        traceback.print_exc()

# Display final summary
print(f"\n{'='*80}")
print("Data Profiling Execution Summary")
print(f"{'='*80}")
print(f"Execution ID: {execution_id}")
print(f"Total tables processed: {len(profiling_results)}")
successful = sum(1 for r in profiling_results if r.get("status") == "success")
failed = sum(1 for r in profiling_results if r.get("status") == "failed")
print(f"Successful: {successful}")
print(f"Failed: {failed}")

if successful > 0:
    avg_quality = sum(r.get("quality_score", 0) for r in profiling_results if r.get("quality_score")) / successful
    print(f"Average Quality Score: {avg_quality:.2%}")

print(f"\nâœ… Data profiling completed successfully!")
{% endblock %}