{%- import 'shared/notebook/macros/notebook_macros.py.jinja' as macros -%}
{%- include "shared/notebook/headers/pyspark.py.jinja" %}

{% include "shared/notebook/environment/library_loader.py.jinja" %}

{{macros.python_cell_with_heading("## 🗂️ Load Custom Python Libraries")}}

if run_mode == "local":
    from ingen_fab.python_libs.common.config_utils import *
    from ingen_fab.python_libs.pyspark.lakehouse_utils import lakehouse_utils
    from ingen_fab.python_libs.pyspark.ddl_utils import ddl_utils
    from ingen_fab.python_libs.pyspark.notebook_utils_abstraction import NotebookUtilsFactory
    from ingen_fab.python_libs.pyspark.synthetic_data_utils import PySparkSyntheticDataGenerator, PySparkDatasetBuilder
    from ingen_fab.python_libs.pyspark.incremental_synthetic_data_utils import IncrementalSyntheticDataGenerator
    notebookutils = NotebookUtilsFactory.create_instance() 
else:
    files_to_load = [
        "ingen_fab/python_libs/common/config_utils.py",
        "ingen_fab/python_libs/pyspark/lakehouse_utils.py",
        "ingen_fab/python_libs/pyspark/ddl_utils.py",
        "ingen_fab/python_libs/pyspark/notebook_utils_abstraction.py",
        "ingen_fab/python_libs/pyspark/synthetic_data_utils.py",
        "ingen_fab/python_libs/pyspark/incremental_synthetic_data_utils.py"
    ]

    load_python_modules_from_path(mount_path, files_to_load)

{{macros.python_cell_with_heading("## ⚙️ Configuration Settings")}}

# Incremental Dataset Configuration
dataset_config = {{dataset_config | tojson | replace('null', 'None') | replace('true', 'True') | replace('false', 'False')}}
generation_mode = "{{generation_mode}}"
generation_date = "{{generation_date}}"
path_format = "{{path_format}}"  # "nested" or "flat"
state_management = {{state_management | string | lower | replace('false', 'False') | replace('true', 'True')}}

# Parse generation date
from datetime import datetime, date
generation_date_obj = datetime.strptime(generation_date, "%Y-%m-%d").date()

# Incremental configuration
incremental_config = {{incremental_config | tojson | replace('null', 'None') | replace('true', 'True') | replace('false', 'False')}}
table_configs = {{table_configs | tojson | replace('null', 'None') | replace('true', 'True') | replace('false', 'False')}}

# variableLibraryInjectionStart: var_lib
# variableLibraryInjectionEnd: var_lib

{{macros.python_cell_with_heading("## 🆕 Initialize Components")}}

# Get configurations
configs: ConfigsObject = get_configs_as_object()

# Initialize lakehouse utils first (this will create/get the Spark session)
target_lakehouse_id = get_config_value("{{target_lakehouse_config_prefix | default('config')}}_lakehouse_id")
target_workspace_id = get_config_value("{{target_lakehouse_config_prefix | default('config')}}_workspace_id")

lh_utils = lakehouse_utils(
    target_workspace_id=target_workspace_id,
    target_lakehouse_id=target_lakehouse_id,
    spark=spark
)

# Initialize incremental synthetic data generator
incremental_generator = IncrementalSyntheticDataGenerator(
    lakehouse_utils_instance=lh_utils,
    seed=incremental_config.get('seed_value'),
    state_table_name=incremental_config.get('state_table_name', 'synthetic_data_state')
)

{{macros.python_cell_with_heading("## 📅 Incremental Data Generation")}}

print(f"🎲 Generating incremental synthetic data for dataset: {dataset_config['dataset_id']}")
print(f"📅 Generation date: {generation_date}")
print(f"🔧 Generation mode: {generation_mode}")
print(f"📁 Path format: {path_format}")
print(f"🔄 State management: {state_management}")

# Display table configuration summary
print("\n📋 Table Configuration Summary:")
for table_name, table_config in table_configs.items():
    table_type = table_config.get('type', 'incremental')
    frequency = table_config.get('frequency', 'daily')
    
    if table_type == 'snapshot':
        base_rows = table_config.get('base_rows', 'N/A')
        print(f"  📊 {table_name}: {table_type} ({frequency}) - Base: {base_rows:,} rows")
    else:
        daily_rows = table_config.get('base_rows_per_day', 'N/A')
        print(f"  📈 {table_name}: {table_type} ({frequency}) - Daily: {daily_rows:,} rows")

{{macros.python_cell_with_heading("## 🚀 Execute Incremental Generation")}}

# Generate the incremental dataset
try:
    results = incremental_generator.generate_incremental_dataset(
        dataset_config=dataset_config,
        generation_date=generation_date,
        path_format=path_format,
        output_mode="parquet"  # Always use parquet for incremental data
    )
    
    print("✅ Incremental data generation completed successfully!")
    print(f"\n📊 Generation Results for {generation_date}:")
    print(f"   Total tables processed: {len(results['generated_tables'])}")
    print(f"   Total rows generated: {results['total_rows']:,}")
    
    # Display detailed results per table
    print("\n📋 Detailed Results:")
    for table_name, table_result in results['generated_tables'].items():
        rows = table_result['rows']
        table_type = table_result['type']
        file_path = table_result['file_path']
        print(f"  ✅ {table_name} ({table_type}): {rows:,} rows -> {file_path}")
    
    # Display file paths
    if path_format == "nested":
        date_path = generation_date_obj.strftime("%Y/%m/%d")
        print(f"\n📁 Files saved to: Files/synthetic_data/{dataset_config['dataset_id']}/{date_path}/")
    else:
        date_str = generation_date_obj.strftime("%Y%m%d")
        print(f"\n📁 Files saved to: Files/synthetic_data/{dataset_config['dataset_id']}/")
        print(f"   File naming pattern: {date_str}_[table_name].parquet")
    
except Exception as e:
    print(f"❌ Error during incremental data generation: {str(e)}")
    import traceback
    traceback.print_exc()
    raise

{{macros.python_cell_with_heading("## 🔍 Data Quality Validation")}}

print("🔍 Performing data quality validation...")

# Validate generated files exist and have expected structure
dataset_dir = f"synthetic_data/{dataset_config['dataset_id']}"
validation_results = {}

for table_name, table_result in results['generated_tables'].items():
    file_path = table_result['file_path']
    expected_rows = table_result['rows']
    
    try:
        # Read the file back to validate
        if path_format == "nested":
            full_path = f"{dataset_dir}/{generation_date_obj.strftime('%Y/%m/%d')}/{table_name}"
        else:
            date_str = generation_date_obj.strftime('%Y%m%d')
            full_path = f"{dataset_dir}/{date_str}_{table_name}"
        
        # Attempt to read the parquet file
        validation_df = lh_utils.read_file(full_path, "parquet")
        actual_rows = validation_df.count()
        
        validation_results[table_name] = {
            "expected_rows": expected_rows,
            "actual_rows": actual_rows,
            "validation_passed": expected_rows == actual_rows,
            "file_path": full_path
        }
        
        status = "✅" if expected_rows == actual_rows else "⚠️"
        print(f"{status} {table_name}: {actual_rows:,} rows (expected: {expected_rows:,})")
        
    except Exception as e:
        validation_results[table_name] = {
            "expected_rows": expected_rows,
            "actual_rows": 0,
            "validation_passed": False,
            "error": str(e),
            "file_path": file_path
        }
        print(f"❌ {table_name}: Validation failed - {str(e)}")

# Summary
passed_validations = sum(1 for result in validation_results.values() if result.get('validation_passed', False))
total_validations = len(validation_results)

print(f"\n📊 Validation Summary: {passed_validations}/{total_validations} tables passed validation")

if passed_validations == total_validations:
    print("🎉 All data quality validations passed!")
else:
    print("⚠️ Some validations failed. Please review the results above.")

{{macros.python_cell_with_heading("## 📈 Generation Statistics")}}

print("📈 Generation Statistics:")
print(f"   Dataset ID: {dataset_config['dataset_id']}")
print(f"   Generation Date: {generation_date}")
print(f"   Total Tables: {len(results['generated_tables'])}")
print(f"   Total Rows: {results['total_rows']:,}")

# Calculate statistics by table type
snapshot_tables = [name for name, config in table_configs.items() if config.get('type') == 'snapshot']
incremental_tables = [name for name, config in table_configs.items() if config.get('type') == 'incremental']

snapshot_rows = sum(
    results['generated_tables'][name]['rows'] 
    for name in snapshot_tables 
    if name in results['generated_tables']
)

incremental_rows = sum(
    results['generated_tables'][name]['rows'] 
    for name in incremental_tables 
    if name in results['generated_tables']
)

print(f"\n📊 By Table Type:")
if snapshot_tables:
    generated_snapshots = [name for name in snapshot_tables if name in results['generated_tables']]
    print(f"   Snapshot tables: {len(generated_snapshots)}/{len(snapshot_tables)} generated ({snapshot_rows:,} rows)")
    for name in generated_snapshots:
        print(f"     • {name}: {results['generated_tables'][name]['rows']:,} rows")

if incremental_tables:
    generated_incrementals = [name for name in incremental_tables if name in results['generated_tables']]
    print(f"   Incremental tables: {len(generated_incrementals)}/{len(incremental_tables)} generated ({incremental_rows:,} rows)")
    for name in generated_incrementals:
        print(f"     • {name}: {results['generated_tables'][name]['rows']:,} rows")

{{macros.python_cell_with_heading("## 🎯 Next Steps")}}

print("🎯 Next Steps:")
print("1. ✅ Incremental data generation completed")
print("2. 📁 Files are organized by date using the selected path format")
print("3. 🔄 State has been updated for future incremental generations")

if path_format == "nested":
    print("\n📂 Directory Structure (Nested):")
    print(f"   Files/synthetic_data/{dataset_config['dataset_id']}/")
    print(f"   ├── {generation_date_obj.strftime('%Y')}/")
    print(f"   │   ├── {generation_date_obj.strftime('%m')}/")
    print(f"   │   │   ├── {generation_date_obj.strftime('%d')}/")
    for table_name in results['generated_tables'].keys():
        print(f"   │   │   │   ├── {table_name}.parquet")
else:
    print("\n📂 Directory Structure (Flat):")
    print(f"   Files/synthetic_data/{dataset_config['dataset_id']}/")
    date_str = generation_date_obj.strftime('%Y%m%d')
    for table_name in results['generated_tables'].keys():
        print(f"   ├── {date_str}_{table_name}.parquet")

print("\n💡 To generate data for additional dates:")
print("   • Run this notebook with different generation_date values")
print("   • Use the series generation notebook for bulk date ranges")
print("   • Snapshot tables will only regenerate based on their frequency settings")
print("   • Incremental tables will generate new data for each date")

print(f"\n🔗 Generated files can be found at:")
if results['file_paths']:
    for table_name, file_path in results['file_paths'].items():
        print(f"   • {table_name}: Files/{file_path}.parquet")
