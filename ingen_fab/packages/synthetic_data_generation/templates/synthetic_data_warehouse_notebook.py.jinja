{%- import 'shared/notebook/macros/notebook_macros.py.jinja' as macros -%}
{%- include "shared/notebook/headers/python.py.jinja" %}

{% include "shared/notebook/environment/library_loader.py.jinja" %}

{{macros.python_cell_with_heading("## 🗂️ Load Custom Python Libraries")}}

if run_mode == "local":
    from ingen_fab.python_libs.common.config_utils import *
    from ingen_fab.python_libs.python.lakehouse_utils import lakehouse_utils
    from ingen_fab.python_libs.python.ddl_utils import ddl_utils
    from ingen_fab.python_libs.python.notebook_utils_abstraction import NotebookUtilsFactory
    from ingen_fab.python_libs.python.warehouse_utils import warehouse_utils
    from ingen_fab.python_libs.python.synthetic_data_utils import SyntheticDataGenerator, DatasetBuilder
    notebookutils = NotebookUtilsFactory.create_instance() 
else:
    files_to_load = [
        "ingen_fab/python_libs/common/config_utils.py",
        "ingen_fab/python_libs/python/lakehouse_utils.py",
        "ingen_fab/python_libs/python/ddl_utils.py",
        "ingen_fab/python_libs/python/notebook_utils_abstraction.py",
        "ingen_fab/python_libs/python/warehouse_utils.py",
        "ingen_fab/python_libs/python/synthetic_data_utils.py"
    ]

    load_python_modules_from_path(mount_path, files_to_load)

{{macros.python_cell_with_heading("## ⚙️ Configuration Settings")}}

# Dataset Configuration
dataset_config = {{dataset_config | tojson | replace('null', 'None') | replace('true', 'True') | replace('false', 'False')}}
generation_mode = "{{generation_mode}}"
target_rows = {{target_rows}}
{% if seed_value %}seed_value = {{seed_value}}{% else %}seed_value = None{% endif %}

# variableLibraryInjectionStart: var_lib
# variableLibraryInjectionEnd: var_lib

{{macros.python_cell_with_heading("## 🆕 Initialize Components")}}

# Get configurations
configs: ConfigsObject = get_configs_as_object()

# Initialize synthetic data generator
generator = SyntheticDataGenerator(seed=seed_value)
dataset_builder = DatasetBuilder(generator)

# Initialize warehouse utils
target_warehouse_id = get_config_value("{{target_lakehouse_config_prefix | default('config')}}_warehouse_id")
target_workspace_id = get_config_value("{{target_lakehouse_config_prefix | default('config')}}_workspace_id")

wu = warehouse_utils(
    target_workspace_id=target_workspace_id,
    target_warehouse_id=target_warehouse_id
)

{{macros.python_cell_with_heading("## 🎲 Generate Synthetic Data")}}

print(f"🎲 Generating synthetic data for dataset: {dataset_config['dataset_id']}")
print(f"📊 Target rows: {target_rows:,}")
print(f"🔧 Generation mode: {generation_mode}")
print(f"🌱 Seed value: {seed_value}")

{% if dataset_config.schema_pattern == "oltp" %}
# Generate OLTP (Transactional) Dataset
print("📋 Generating OLTP dataset...")

# Determine scale based on target rows
if target_rows <= 10000:
    scale = "small"
elif target_rows <= 100000:
    scale = "medium"
else:
    scale = "large"

print(f"📏 Using scale: {scale}")

# Generate complete OLTP dataset
dataset = dataset_builder.build_retail_oltp_dataset(scale=scale)

print("✅ OLTP dataset generation completed!")

{% elif dataset_config.schema_pattern == "star_schema" %}
# Generate Star Schema (Analytical) Dataset
print("⭐ Generating Star Schema dataset...")

# Generate complete star schema dataset
dataset = dataset_builder.build_retail_star_schema_dataset(fact_rows=target_rows)

print("✅ Star Schema dataset generation completed!")

{% else %}
# Custom dataset generation
print("🛠️ Generating custom dataset...")

# For custom datasets, generate basic tables
customers_df = generator.generate_customers_table(min(target_rows // 10, 10000))
products_df = generator.generate_products_table(min(target_rows // 100, 1000))

dataset = {
    "customers": customers_df,
    "products": products_df
}

print("✅ Custom dataset generation completed!")
{% endif %}

{{macros.python_cell_with_heading("## 💾 Save Data to Warehouse")}}

print("💾 Saving generated data to warehouse...")

# Create schema if it doesn't exist
schema_name = "{{dataset_config.dataset_id}}"
wu.create_schema_if_not_exists(schema_name)

# Save each table to the warehouse
total_rows_inserted = 0
for table_name, df in dataset.items():
    print(f"📊 Saving {table_name} with {len(df):,} rows...")
    
    try:
        # Use warehouse_utils abstract method to write data
        wu.write_to_table(
            df=df,
            table_name=table_name,
            schema_name=schema_name,
            mode="overwrite"
        )
        
        total_rows_inserted += len(df)
        print(f"✅ {table_name}: {len(df):,} rows saved")
        
    except Exception as e:
        print(f"❌ Error saving {table_name}: {e}")
        print(f"🔄 Skipping {table_name} due to error...")
        continue

print(f"📊 Total rows inserted: {total_rows_inserted:,}")

{{macros.python_cell_with_heading("## 📊 Data Quality Validation")}}

print("🔍 Performing data quality validation...")

# Validate data in warehouse
schema_name = "{{dataset_config.dataset_id}}"
validation_results = {}

for table_name in dataset.keys():
    full_table_name = f"[{schema_name}].[{table_name}]"
    
    try:
        # Get row count from warehouse using abstract method
        warehouse_count = wu.get_table_row_count(table_name=table_name, schema_name=schema_name)
        
        # Compare with generated DataFrame
        generated_count = len(dataset[table_name])
        
        validation_results[table_name] = {
            'generated': generated_count,
            'warehouse': warehouse_count,
            'match': generated_count == warehouse_count
        }
        
        status = "✅" if generated_count == warehouse_count else "⚠️"
        print(f"{status} {table_name}: Generated {generated_count:,}, Warehouse {warehouse_count:,}")
        
        # Show sample data using abstract method
        try:
            sample_result = wu.read_table(table_name=table_name, schema_name=schema_name, limit=5)
            if sample_result is not None and len(sample_result) > 0:
                print(f"📝 Sample data from {table_name}:")
                for i, row in sample_result.head(3).iterrows():  # Show first 3 rows
                    print(f"   {row.to_dict()}")
        except Exception as sample_error:
            print(f"📝 Could not retrieve sample data from {table_name}: {sample_error}")
        
    except Exception as e:
        print(f"❌ Validation error for {table_name}: {e}")
        validation_results[table_name] = {'error': str(e)}

# Overall validation
all_matched = all(
    result.get('match', False) for result in validation_results.values()
)

if all_matched:
    print("✅ All data validation checks passed!")
else:
    print("⚠️ Some data validation checks failed!")

{{macros.python_cell_with_heading("## 📈 Performance Metrics")}}

print("📈 Performance and storage metrics:")

schema_name = "{{dataset_config.dataset_id}}"
total_rows_validated = 0
total_size_mb = 0

# Get storage information for each table
for table_name in dataset.keys():
    try:
        # Get row count using abstract method
        row_count = wu.get_table_row_count(table_name=table_name, schema_name=schema_name)
        
        # Try to get table metadata (size info may not be available in all environments)
        try:
            metadata = wu.get_table_metadata(table_name=table_name, schema_name=schema_name)
            size_mb = metadata.get('size_mb', 0) if metadata else 0
        except Exception:
            # Size information may not be available in local/dev environments
            size_mb = 0
        
        total_rows_validated += row_count
        total_size_mb += size_mb
        
        if size_mb > 0:
            print(f"💾 {table_name}: {size_mb:.2f} MB, {row_count:,} rows")
        else:
            print(f"💾 {table_name}: {row_count:,} rows (size info unavailable)")
        
    except Exception as e:
        print(f"⚠️ Could not get metrics for {table_name}: {e}")

print(f"📊 Total storage used: {total_size_mb:.2f} MB")
print(f"📊 Total rows validated: {total_rows_validated:,}")

if total_rows_validated > 0 and total_size_mb > 0:
    print(f"⚡ Storage efficiency: {total_rows_validated / total_size_mb:.0f} rows/MB")

{{macros.python_cell_with_heading("## ✅ Completion Summary")}}

print("🎉 Synthetic Data Generation Completed Successfully!")
print(f"📊 Dataset: {dataset_config['dataset_name']}")
print(f"🏷️ Dataset ID: {dataset_config['dataset_id']}")
print(f"📈 Schema Pattern: {dataset_config['schema_pattern']}")
print(f"🎯 Rows Generated: {total_rows_validated:,}")
print(f"📋 Tables Created: {len(dataset)}")
print(f"💾 Total Storage: {total_size_mb:.2f} MB")
print(f"🏛️ Schema: [{schema_name}]")

# List created tables
print("📋 Created tables:")
for table_name in dataset.keys():
    print(f"   - [{schema_name}].[{table_name}]")

# Return success
notebookutils.exit_notebook("success")

{%include "shared/notebook/cells/footer.py.jinja" %}