{%- import 'shared/notebook/macros/notebook_macros.py.jinja' as macros -%}
{%- include "shared/notebook/headers/python.py.jinja" %}

{% include "shared/notebook/environment/library_loader.py.jinja" %}

{{macros.python_cell_with_heading("## ğŸ—‚ï¸ Load Custom Python Libraries")}}

if run_mode == "local":
    from ingen_fab.python_libs.common.config_utils import *
    from ingen_fab.python_libs.python.lakehouse_utils import lakehouse_utils
    from ingen_fab.python_libs.python.ddl_utils import ddl_utils
    from ingen_fab.python_libs.python.notebook_utils_abstraction import NotebookUtilsFactory
    from ingen_fab.python_libs.python.warehouse_utils import warehouse_utils
    from ingen_fab.python_libs.python.synthetic_data_utils import SyntheticDataGenerator, DatasetBuilder
    notebookutils = NotebookUtilsFactory.create_instance() 
else:
    files_to_load = [
        "ingen_fab/python_libs/common/config_utils.py",
        "ingen_fab/python_libs/python/lakehouse_utils.py",
        "ingen_fab/python_libs/python/ddl_utils.py",
        "ingen_fab/python_libs/python/notebook_utils_abstraction.py",
        "ingen_fab/python_libs/python/warehouse_utils.py",
        "ingen_fab/python_libs/python/synthetic_data_utils.py"
    ]

    load_python_modules_from_path(mount_path, files_to_load)

{{macros.python_cell_with_heading("## âš™ï¸ Configuration Settings")}}

# Dataset Configuration
dataset_config = {{dataset_config | tojson | replace('null', 'None') | replace('true', 'True') | replace('false', 'False')}}
generation_mode = "{{generation_mode}}"
target_rows = {{target_rows}}
{% if seed_value %}seed_value = {{seed_value}}{% else %}seed_value = None{% endif %}

# variableLibraryInjectionStart: var_lib
# variableLibraryInjectionEnd: var_lib

{{macros.python_cell_with_heading("## ğŸ†• Initialize Components")}}

# Get configurations
configs: ConfigsObject = get_configs_as_object()

# Initialize synthetic data generator
generator = SyntheticDataGenerator(seed=seed_value)
dataset_builder = DatasetBuilder(generator)

# Initialize warehouse utils
target_warehouse_id = get_config_value("{{target_lakehouse_config_prefix | default('config')}}_warehouse_id")
target_workspace_id = get_config_value("{{target_lakehouse_config_prefix | default('config')}}_workspace_id")

wu = warehouse_utils(
    target_workspace_id=target_workspace_id,
    target_warehouse_id=target_warehouse_id
)

{{macros.python_cell_with_heading("## ğŸ² Generate Synthetic Data")}}

print(f"ğŸ² Generating synthetic data for dataset: {dataset_config['dataset_id']}")
print(f"ğŸ“Š Target rows: {target_rows:,}")
print(f"ğŸ”§ Generation mode: {generation_mode}")
print(f"ğŸŒ± Seed value: {seed_value}")

{% if dataset_config.schema_pattern == "oltp" %}
# Generate OLTP (Transactional) Dataset
print("ğŸ“‹ Generating OLTP dataset...")

# Determine scale based on target rows
if target_rows <= 10000:
    scale = "small"
elif target_rows <= 100000:
    scale = "medium"
else:
    scale = "large"

print(f"ğŸ“ Using scale: {scale}")

# Generate complete OLTP dataset
dataset = dataset_builder.build_retail_oltp_dataset(scale=scale)

print("âœ… OLTP dataset generation completed!")

{% elif dataset_config.schema_pattern == "star_schema" %}
# Generate Star Schema (Analytical) Dataset
print("â­ Generating Star Schema dataset...")

# Generate complete star schema dataset
dataset = dataset_builder.build_retail_star_schema_dataset(fact_rows=target_rows)

print("âœ… Star Schema dataset generation completed!")

{% else %}
# Custom dataset generation
print("ğŸ› ï¸ Generating custom dataset...")

# For custom datasets, generate basic tables
customers_df = generator.generate_customers_table(min(target_rows // 10, 10000))
products_df = generator.generate_products_table(min(target_rows // 100, 1000))

dataset = {
    "customers": customers_df,
    "products": products_df
}

print("âœ… Custom dataset generation completed!")
{% endif %}

{{macros.python_cell_with_heading("## ğŸ’¾ Save Data to Warehouse")}}

print("ğŸ’¾ Saving generated data to warehouse...")

# Create schema if it doesn't exist
schema_name = "{{dataset_config.dataset_id}}"
wu.create_schema_if_not_exists(schema_name)

# Save each table to the warehouse
total_rows_inserted = 0
for table_name, df in dataset.items():
    print(f"ğŸ“Š Saving {table_name} with {len(df):,} rows...")
    
    try:
        # Use warehouse_utils abstract method to write data
        wu.write_to_table(
            df=df,
            table_name=table_name,
            schema_name=schema_name,
            mode="overwrite"
        )
        
        total_rows_inserted += len(df)
        print(f"âœ… {table_name}: {len(df):,} rows saved")
        
    except Exception as e:
        print(f"âŒ Error saving {table_name}: {e}")
        print(f"ğŸ”„ Skipping {table_name} due to error...")
        continue

print(f"ğŸ“Š Total rows inserted: {total_rows_inserted:,}")

{{macros.python_cell_with_heading("## ğŸ“Š Data Quality Validation")}}

print("ğŸ” Performing data quality validation...")

# Validate data in warehouse
schema_name = "{{dataset_config.dataset_id}}"
validation_results = {}

for table_name in dataset.keys():
    full_table_name = f"[{schema_name}].[{table_name}]"
    
    try:
        # Get row count from warehouse using abstract method
        warehouse_count = wu.get_table_row_count(table_name=table_name, schema_name=schema_name)
        
        # Compare with generated DataFrame
        generated_count = len(dataset[table_name])
        
        validation_results[table_name] = {
            'generated': generated_count,
            'warehouse': warehouse_count,
            'match': generated_count == warehouse_count
        }
        
        status = "âœ…" if generated_count == warehouse_count else "âš ï¸"
        print(f"{status} {table_name}: Generated {generated_count:,}, Warehouse {warehouse_count:,}")
        
        # Show sample data using abstract method
        try:
            sample_result = wu.read_table(table_name=table_name, schema_name=schema_name, limit=5)
            if sample_result is not None and len(sample_result) > 0:
                print(f"ğŸ“ Sample data from {table_name}:")
                for i, row in sample_result.head(3).iterrows():  # Show first 3 rows
                    print(f"   {row.to_dict()}")
        except Exception as sample_error:
            print(f"ğŸ“ Could not retrieve sample data from {table_name}: {sample_error}")
        
    except Exception as e:
        print(f"âŒ Validation error for {table_name}: {e}")
        validation_results[table_name] = {'error': str(e)}

# Overall validation
all_matched = all(
    result.get('match', False) for result in validation_results.values()
)

if all_matched:
    print("âœ… All data validation checks passed!")
else:
    print("âš ï¸ Some data validation checks failed!")

{{macros.python_cell_with_heading("## ğŸ“ˆ Performance Metrics")}}

print("ğŸ“ˆ Performance and storage metrics:")

schema_name = "{{dataset_config.dataset_id}}"
total_rows_validated = 0
total_size_mb = 0

# Get storage information for each table
for table_name in dataset.keys():
    try:
        # Get row count using abstract method
        row_count = wu.get_table_row_count(table_name=table_name, schema_name=schema_name)
        
        # Try to get table metadata (size info may not be available in all environments)
        try:
            metadata = wu.get_table_metadata(table_name=table_name, schema_name=schema_name)
            size_mb = metadata.get('size_mb', 0) if metadata else 0
        except Exception:
            # Size information may not be available in local/dev environments
            size_mb = 0
        
        total_rows_validated += row_count
        total_size_mb += size_mb
        
        if size_mb > 0:
            print(f"ğŸ’¾ {table_name}: {size_mb:.2f} MB, {row_count:,} rows")
        else:
            print(f"ğŸ’¾ {table_name}: {row_count:,} rows (size info unavailable)")
        
    except Exception as e:
        print(f"âš ï¸ Could not get metrics for {table_name}: {e}")

print(f"ğŸ“Š Total storage used: {total_size_mb:.2f} MB")
print(f"ğŸ“Š Total rows validated: {total_rows_validated:,}")

if total_rows_validated > 0 and total_size_mb > 0:
    print(f"âš¡ Storage efficiency: {total_rows_validated / total_size_mb:.0f} rows/MB")

{{macros.python_cell_with_heading("## âœ… Completion Summary")}}

print("ğŸ‰ Synthetic Data Generation Completed Successfully!")
print(f"ğŸ“Š Dataset: {dataset_config['dataset_name']}")
print(f"ğŸ·ï¸ Dataset ID: {dataset_config['dataset_id']}")
print(f"ğŸ“ˆ Schema Pattern: {dataset_config['schema_pattern']}")
print(f"ğŸ¯ Rows Generated: {total_rows_validated:,}")
print(f"ğŸ“‹ Tables Created: {len(dataset)}")
print(f"ğŸ’¾ Total Storage: {total_size_mb:.2f} MB")
print(f"ğŸ›ï¸ Schema: [{schema_name}]")

# List created tables
print("ğŸ“‹ Created tables:")
for table_name in dataset.keys():
    print(f"   - [{schema_name}].[{table_name}]")

# Return success
notebookutils.exit_notebook("success")

{%include "shared/notebook/cells/footer.py.jinja" %}