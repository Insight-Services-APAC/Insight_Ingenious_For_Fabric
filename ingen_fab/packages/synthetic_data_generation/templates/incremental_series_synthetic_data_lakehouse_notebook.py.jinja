{%- import 'shared/notebook/macros/notebook_macros.py.jinja' as macros -%}
{%- include "shared/notebook/headers/pyspark.py.jinja" %}

{% include "shared/notebook/environment/library_loader.py.jinja" %}

{{macros.python_cell_with_heading("## ğŸ—‚ï¸ Load Custom Python Libraries")}}

if run_mode == "local":
    from ingen_fab.python_libs.common.config_utils import *
    from ingen_fab.python_libs.pyspark.lakehouse_utils import lakehouse_utils
    from ingen_fab.python_libs.pyspark.ddl_utils import ddl_utils
    from ingen_fab.python_libs.pyspark.notebook_utils_abstraction import NotebookUtilsFactory
    from ingen_fab.python_libs.pyspark.synthetic_data_utils import PySparkSyntheticDataGenerator, PySparkDatasetBuilder
    from ingen_fab.python_libs.pyspark.incremental_synthetic_data_utils import IncrementalSyntheticDataGenerator
    notebookutils = NotebookUtilsFactory.create_instance() 
else:
    files_to_load = [
        "ingen_fab/python_libs/common/config_utils.py",
        "ingen_fab/python_libs/pyspark/lakehouse_utils.py",
        "ingen_fab/python_libs/pyspark/ddl_utils.py",
        "ingen_fab/python_libs/pyspark/notebook_utils_abstraction.py",
        "ingen_fab/python_libs/pyspark/synthetic_data_utils.py",
        "ingen_fab/python_libs/pyspark/incremental_synthetic_data_utils.py"
    ]

    load_python_modules_from_path(mount_path, files_to_load)

{{macros.python_cell_with_heading("## âš™ï¸ Configuration Settings")}}

# Incremental Dataset Series Configuration
dataset_config = {{dataset_config | tojson | replace('null', 'None') | replace('true', 'True') | replace('false', 'False')}}
generation_mode = "{{generation_mode}}"
path_format = "{{path_format}}"  # "nested" or "flat"

# Date range configuration
date_range = {{date_range | tojson | replace('null', 'None') | replace('true', 'True') | replace('false', 'False')}}
start_date = date_range['start_date']
end_date = date_range['end_date']
total_days = date_range['total_days']
batch_size = date_range['batch_size']

# Parse dates
from datetime import datetime, date, timedelta
start_date_obj = datetime.strptime(start_date, "%Y-%m-%d").date()
end_date_obj = datetime.strptime(end_date, "%Y-%m-%d").date()

# Configuration
incremental_config = {{incremental_config | tojson | replace('null', 'None') | replace('true', 'True') | replace('false', 'False')}}
table_configs = {{table_configs | tojson | replace('null', 'None') | replace('true', 'True') | replace('false', 'False')}}

# variableLibraryInjectionStart: var_lib
# variableLibraryInjectionEnd: var_lib

{{macros.python_cell_with_heading("## ğŸ†• Initialize Components")}}

# Get configurations
configs: ConfigsObject = get_configs_as_object()

# Initialize lakehouse utils first (this will create/get the Spark session)
target_lakehouse_id = get_config_value("{{target_lakehouse_config_prefix | default('config')}}_lakehouse_id")
target_workspace_id = get_config_value("{{target_lakehouse_config_prefix | default('config')}}_workspace_id")

lh_utils = lakehouse_utils(
    target_workspace_id=target_workspace_id,
    target_lakehouse_id=target_lakehouse_id,
    spark=spark
)

# Initialize incremental synthetic data generator
incremental_generator = IncrementalSyntheticDataGenerator(
    lakehouse_utils_instance=lh_utils,
    seed=incremental_config.get('seed_value'),
    state_table_name=incremental_config.get('state_table_name', 'synthetic_data_state')
)

{{macros.python_cell_with_heading("## ğŸ“… Date Range Summary")}}

print(f"ğŸ² Generating incremental synthetic data series for dataset: {dataset_config['dataset_id']}")
print(f"ğŸ“… Date range: {start_date} to {end_date}")
print(f"ğŸ“Š Total days: {total_days}")
print(f"ğŸ”§ Generation mode: {generation_mode}")
print(f"ğŸ“ Path format: {path_format}")
print(f"ğŸ“¦ Batch size: {batch_size} days")

# Estimate total work
estimated_total_rows = 0
incremental_tables = [name for name, config in table_configs.items() if config.get('type') == 'incremental']
for table_name in incremental_tables:
    daily_rows = table_configs[table_name].get('base_rows_per_day', 1000)
    estimated_total_rows += daily_rows * total_days

print(f"\nğŸ“ˆ Estimated total incremental rows: {estimated_total_rows:,}")

# Display table configuration summary
print("\nğŸ“‹ Table Configuration Summary:")
for table_name, table_config in table_configs.items():
    table_type = table_config.get('type', 'incremental')
    frequency = table_config.get('frequency', 'daily')
    
    if table_type == 'snapshot':
        base_rows = table_config.get('base_rows', 'N/A')
        print(f"  ğŸ“Š {table_name}: {table_type} ({frequency}) - Base: {base_rows:,} rows")
    else:
        daily_rows = table_config.get('base_rows_per_day', 'N/A')
        total_table_rows = daily_rows * total_days if isinstance(daily_rows, int) else 'N/A'
        print(f"  ğŸ“ˆ {table_name}: {table_type} ({frequency}) - Daily: {daily_rows:,}, Total est: {total_table_rows:,}")

{{macros.python_cell_with_heading("## ğŸš€ Execute Series Generation")}}

# Generate the incremental dataset series
try:
    print("ğŸš€ Starting incremental data series generation...")
    print(f"â±ï¸ Processing {total_days} days in batches of {batch_size}")
    
    series_results = incremental_generator.generate_incremental_dataset_series(
        dataset_config=dataset_config,
        start_date=start_date,
        end_date=end_date,
        path_format=path_format,
        output_mode="parquet",
        batch_size=batch_size
    )
    
    print("âœ… Incremental data series generation completed successfully!")
    
    # Display overall results
    print(f"\nğŸ“Š Overall Generation Results:")
    print(f"   Dataset ID: {series_results['dataset_id']}")
    print(f"   Date Range: {series_results['start_date']} to {series_results['end_date']}")
    print(f"   Total Days Processed: {series_results['total_days']}")
    print(f"   Total Rows Generated: {series_results['total_rows']:,}")
    
    # Display results by table
    print(f"\nğŸ“‹ Results by Table:")
    for table_name, table_stats in series_results['generated_tables'].items():
        total_rows = table_stats['total_rows']
        generation_count = table_stats['generation_count']
        avg_rows_per_generation = total_rows / generation_count if generation_count > 0 else 0
        
        print(f"  âœ… {table_name}:")
        print(f"      Total rows: {total_rows:,}")
        print(f"      Generations: {generation_count}")
        print(f"      Avg per generation: {avg_rows_per_generation:,.0f}")
    
except Exception as e:
    print(f"âŒ Error during series generation: {str(e)}")
    import traceback
    traceback.print_exc()
    raise

{{macros.python_cell_with_heading("## ğŸ“Š Daily Generation Summary")}}

print("ğŸ“Š Daily Generation Summary:")

# Display summary of daily results
daily_totals = []
for daily_result in series_results['daily_results']:
    gen_date = daily_result['generation_date']
    total_rows = daily_result['total_rows']
    table_count = len(daily_result['generated_tables'])
    
    daily_totals.append({
        'date': gen_date,
        'rows': total_rows,
        'tables': table_count
    })

# Show first few and last few days, plus some statistics
print(f"\nFirst 5 days:")
for daily in daily_totals[:5]:
    print(f"  {daily['date']}: {daily['rows']:,} rows across {daily['tables']} tables")

if len(daily_totals) > 10:
    print(f"\n... ({len(daily_totals) - 10} days omitted) ...")

print(f"\nLast 5 days:")
for daily in daily_totals[-5:]:
    print(f"  {daily['date']}: {daily['rows']:,} rows across {daily['tables']} tables")

# Calculate statistics
if daily_totals:
    daily_row_counts = [d['rows'] for d in daily_totals]
    avg_daily_rows = sum(daily_row_counts) / len(daily_row_counts)
    max_daily_rows = max(daily_row_counts)
    min_daily_rows = min(daily_row_counts)
    
    print(f"\nğŸ“ˆ Daily Generation Statistics:")
    print(f"   Average daily rows: {avg_daily_rows:,.0f}")
    print(f"   Maximum daily rows: {max_daily_rows:,}")
    print(f"   Minimum daily rows: {min_daily_rows:,}")

{{macros.python_cell_with_heading("## ğŸ” Data Quality Validation")}}

print("ğŸ” Performing data quality validation on series...")

# Sample a few dates for validation
import random
validation_dates = []

if len(series_results['daily_results']) <= 5:
    # Validate all dates if we have 5 or fewer
    validation_dates = [dr['generation_date'] for dr in series_results['daily_results']]
else:
    # Sample first, middle, and last dates plus a couple random ones
    all_dates = [dr['generation_date'] for dr in series_results['daily_results']]
    validation_dates = [
        all_dates[0],  # First
        all_dates[len(all_dates)//2],  # Middle
        all_dates[-1]  # Last
    ]
    
    # Add a couple random dates
    remaining_dates = [d for d in all_dates if d not in validation_dates]
    if remaining_dates:
        sample_size = min(2, len(remaining_dates))
        validation_dates.extend(random.sample(remaining_dates, sample_size))

print(f"Validating {len(validation_dates)} sample dates...")

validation_summary = {
    'dates_validated': 0,
    'dates_passed': 0,
    'total_files_checked': 0,
    'files_passed': 0
}

for val_date in validation_dates:
    print(f"\nğŸ” Validating {val_date}...")
    
    # Find the daily result for this date
    daily_result = next((dr for dr in series_results['daily_results'] if dr['generation_date'] == val_date), None)
    
    if not daily_result:
        print(f"   âŒ No generation result found for {val_date}")
        continue
    
    validation_summary['dates_validated'] += 1
    date_validation_passed = True
    
    # Validate each table for this date
    for table_name, table_result in daily_result['generated_tables'].items():
        file_path = table_result['file_path']
        expected_rows = table_result['rows']
        
        try:
            # Attempt to read the file
            validation_df = lh_utils.read_file(file_path, "parquet")
            actual_rows = validation_df.count()
            
            validation_summary['total_files_checked'] += 1
            
            if actual_rows == expected_rows:
                validation_summary['files_passed'] += 1
                print(f"   âœ… {table_name}: {actual_rows:,} rows")
            else:
                date_validation_passed = False
                print(f"   âš ï¸ {table_name}: {actual_rows:,} rows (expected {expected_rows:,})")
        
        except Exception as e:
            date_validation_passed = False
            print(f"   âŒ {table_name}: Validation failed - {str(e)}")
    
    if date_validation_passed:
        validation_summary['dates_passed'] += 1

# Summary
print(f"\nğŸ“Š Validation Summary:")
print(f"   Dates validated: {validation_summary['dates_validated']}")
print(f"   Dates passed: {validation_summary['dates_passed']}")
print(f"   Files checked: {validation_summary['total_files_checked']}")
print(f"   Files passed: {validation_summary['files_passed']}")

validation_rate = (validation_summary['files_passed'] / validation_summary['total_files_checked'] * 100) if validation_summary['total_files_checked'] > 0 else 0
print(f"   Overall validation rate: {validation_rate:.1f}%")

if validation_rate >= 95:
    print("ğŸ‰ Excellent validation results!")
elif validation_rate >= 80:
    print("âœ… Good validation results!")
else:
    print("âš ï¸ Some validation issues detected. Please review.")

{{macros.python_cell_with_heading("## ğŸ“ File Organization Summary")}}

print("ğŸ“ File Organization Summary:")

dataset_base_path = f"Files/synthetic_data/{dataset_config['dataset_id']}"

if path_format == "nested":
    print(f"\nğŸ“‚ Nested Directory Structure:")
    print(f"   {dataset_base_path}/")
    
    # Group by year/month for display
    date_paths = set()
    for daily_result in series_results['daily_results']:
        gen_date_obj = datetime.strptime(daily_result['generation_date'], "%Y-%m-%d").date()
        year_month = gen_date_obj.strftime("%Y/%m")
        date_paths.add(year_month)
    
    for year_month in sorted(date_paths):
        print(f"   â”œâ”€â”€ {year_month}/")
        
        # Show a few sample days for this month
        month_dates = []
        for daily_result in series_results['daily_results']:
            gen_date_obj = datetime.strptime(daily_result['generation_date'], "%Y-%m-%d").date()
            if gen_date_obj.strftime("%Y/%m") == year_month:
                month_dates.append(gen_date_obj.strftime("%d"))
        
        sample_days = sorted(month_dates)[:3]  # Show first 3 days
        for day in sample_days:
            print(f"   â”‚   â”œâ”€â”€ {day}/")
            for table_name in series_results['generated_tables'].keys():
                print(f"   â”‚   â”‚   â”œâ”€â”€ {table_name}.parquet")
        
        if len(month_dates) > 3:
            print(f"   â”‚   â””â”€â”€ ... ({len(month_dates) - 3} more days)")

else:
    print(f"\nğŸ“‚ Flat Directory Structure:")
    print(f"   {dataset_base_path}/")
    
    # Show sample files
    sample_results = series_results['daily_results'][:3]  # First 3 days
    for daily_result in sample_results:
        gen_date_obj = datetime.strptime(daily_result['generation_date'], "%Y-%m-%d").date()
        date_str = gen_date_obj.strftime("%Y%m%d")
        
        for table_name in daily_result['generated_tables'].keys():
            print(f"   â”œâ”€â”€ {date_str}_{table_name}.parquet")
    
    if len(series_results['daily_results']) > 3:
        remaining_days = len(series_results['daily_results']) - 3
        remaining_files = remaining_days * len(series_results['generated_tables'])
        print(f"   â””â”€â”€ ... ({remaining_files} more files for {remaining_days} days)")

{{macros.python_cell_with_heading("## ğŸ¯ Completion Summary")}}

print("ğŸ¯ Incremental Data Series Generation Complete!")

print(f"\nğŸ“ˆ Final Statistics:")
print(f"   Dataset: {dataset_config['dataset_id']}")
print(f"   Date Range: {start_date} to {end_date} ({total_days} days)")
print(f"   Total Rows Generated: {series_results['total_rows']:,}")
print(f"   Generation Mode: {generation_mode}")
print(f"   Path Format: {path_format}")

print(f"\nğŸ“Š Table Statistics:")
for table_name, table_stats in series_results['generated_tables'].items():
    table_config = table_configs[table_name]
    table_type = table_config.get('type', 'incremental')
    total_rows = table_stats['total_rows']
    generation_count = table_stats['generation_count']
    
    print(f"   {table_name} ({table_type}):")
    print(f"      Total rows: {total_rows:,}")
    print(f"      Generated: {generation_count} times")

print(f"\nğŸ’¡ Usage Notes:")
print(f"   â€¢ Snapshot tables were generated based on their frequency settings")
print(f"   â€¢ Incremental tables contain daily data for the entire date range")
print(f"   â€¢ State management ensures consistent incremental generation")
print(f"   â€¢ Files can be processed individually by date or as a complete dataset")

if path_format == "nested":
    print(f"\nğŸ“‚ Access Pattern Examples:")
    print(f"   â€¢ Single day: Files/synthetic_data/{dataset_config['dataset_id']}/YYYY/MM/DD/")
    print(f"   â€¢ Full month: Files/synthetic_data/{dataset_config['dataset_id']}/YYYY/MM/")
    print(f"   â€¢ Full year: Files/synthetic_data/{dataset_config['dataset_id']}/YYYY/")
else:
    print(f"\nğŸ“‚ Access Pattern Examples:")
    print(f"   â€¢ Single day: Files/synthetic_data/{dataset_config['dataset_id']}/YYYYMMDD_*")
    print(f"   â€¢ Date range: Use pattern matching on YYYYMMDD prefix")

print(f"\nâœ… All files are ready for analysis and further processing!")
