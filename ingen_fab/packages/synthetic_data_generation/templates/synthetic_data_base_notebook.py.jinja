{%- import 'shared/notebook/macros/notebook_macros.py.jinja' as macros -%}
{%- if language_group == "synapse_pyspark" -%}
{%- include "shared/notebook/headers/pyspark.py.jinja" %}
{%- else -%}
{%- include "shared/notebook/headers/python.py.jinja" %}
{%- endif %}

{% include "shared/notebook/environment/library_loader.py.jinja" %}

{{macros.python_cell_with_heading("## ğŸ—‚ï¸ Load Custom Python Libraries")}}

if run_mode == "local":
    from ingen_fab.python_libs.common.config_utils import *
{%- if language_group == "synapse_pyspark" %}
    from ingen_fab.python_libs.pyspark.lakehouse_utils import lakehouse_utils
    from ingen_fab.python_libs.pyspark.ddl_utils import ddl_utils
    from ingen_fab.python_libs.pyspark.notebook_utils_abstraction import NotebookUtilsFactory
    from ingen_fab.python_libs.pyspark.synthetic_data_utils import PySparkSyntheticDataGenerator, PySparkDatasetBuilder
{%- else %}
    from ingen_fab.python_libs.python.lakehouse_utils import lakehouse_utils
    from ingen_fab.python_libs.python.ddl_utils import ddl_utils
    from ingen_fab.python_libs.python.notebook_utils_abstraction import NotebookUtilsFactory
    from ingen_fab.python_libs.python.warehouse_utils import warehouse_utils
    from ingen_fab.python_libs.python.synthetic_data_utils import SyntheticDataGenerator, DatasetBuilder
{%- endif %}
    notebookutils = NotebookUtilsFactory.create_instance() 
else:
    files_to_load = [
        "ingen_fab/python_libs/common/config_utils.py",
{%- if language_group == "synapse_pyspark" %}
        "ingen_fab/python_libs/pyspark/lakehouse_utils.py",
        "ingen_fab/python_libs/pyspark/ddl_utils.py",
        "ingen_fab/python_libs/pyspark/notebook_utils_abstraction.py",
        "ingen_fab/python_libs/pyspark/synthetic_data_utils.py"
{%- else %}
        "ingen_fab/python_libs/python/lakehouse_utils.py",
        "ingen_fab/python_libs/python/ddl_utils.py",
        "ingen_fab/python_libs/python/notebook_utils_abstraction.py",
        "ingen_fab/python_libs/python/warehouse_utils.py",
        "ingen_fab/python_libs/python/synthetic_data_utils.py"
{%- endif %}
    ]

    load_python_modules_from_path(mount_path, files_to_load)

{{macros.python_cell_with_heading("## âš™ï¸ Configuration Settings")}}

# Dataset Configuration
dataset_config = {{dataset_config | tojson | replace('null', 'None') | replace('true', 'True') | replace('false', 'False')}}
generation_mode = "{{generation_mode}}"
{% if target_rows is defined %}target_rows = {{target_rows}}{% endif %}
{% if chunk_size is defined %}chunk_size = {{chunk_size}}{% endif %}
{% if seed_value %}seed_value = {{seed_value}}{% else %}seed_value = None{% endif %}
{% if output_mode is defined %}output_mode = "{{output_mode}}"{% endif %}
{% if generation_date is defined %}generation_date = "{{generation_date}}"{% endif %}
{% if path_format is defined %}path_format = "{{path_format}}"{% endif %}
{% if state_management is defined %}state_management = {{state_management | lower}}{% endif %}

# variableLibraryInjectionStart: var_lib
# variableLibraryInjectionEnd: var_lib

{{macros.python_cell_with_heading("## ğŸ†• Initialize Components")}}

# Get configurations
configs: ConfigsObject = get_configs_as_object()

# Initialize target datastore configuration
target_datastore_config_prefix = "{{target_lakehouse_config_prefix | default('config')}}"

{%- if target_environment == "warehouse" %}
# Initialize synthetic data generator
generator = SyntheticDataGenerator(seed=seed_value)
dataset_builder = DatasetBuilder(generator)

# Initialize warehouse utils
target_warehouse_id = get_config_value(f"{target_datastore_config_prefix}_warehouse_id")
target_workspace_id = get_config_value(f"{target_datastore_config_prefix}_workspace_id")

wh_utils = warehouse_utils(
    target_workspace_id=target_workspace_id,
    target_warehouse_id=target_warehouse_id,
    mount_path=mount_path
)

# SQL client for DDL operations
sql_client = wh_utils.get_sql_client()
{%- else %}
# Initialize lakehouse utils first (this will create/get the Spark session)
target_lakehouse_id = get_config_value(f"{target_datastore_config_prefix}_lakehouse_id")
target_workspace_id = get_config_value(f"{target_datastore_config_prefix}_workspace_id")

lh_utils = lakehouse_utils(
    target_workspace_id=target_workspace_id,
    target_lakehouse_id=target_lakehouse_id,
    mount_path=mount_path
)

# Now initialize PySpark synthetic data generator with the lakehouse utils instance
generator = PySparkSyntheticDataGenerator(
    lakehouse_utils_instance=lh_utils,
    seed=seed_value
)
dataset_builder = PySparkDatasetBuilder(generator, lh_utils)
{%- endif %}

{{macros.python_cell_with_heading("## ğŸ“Š Dataset Information")}}

# Display dataset information
dataset_info = {
    "Dataset ID": dataset_config.get("dataset_id", "custom"),
    "Dataset Name": dataset_config.get("dataset_name", "Custom Dataset"),
    "Domain": dataset_config.get("domain", "generic"),
    "Schema Pattern": dataset_config.get("schema_pattern", "custom"),
    "Description": dataset_config.get("description", "No description provided")
}

print("=" * 60)
print("SYNTHETIC DATA GENERATION - DATASET INFORMATION")
print("=" * 60)
for key, value in dataset_info.items():
    print(f"{key:20}: {value}")
print("=" * 60)

{{macros.python_cell_with_heading("## ğŸ”„ Generate Synthetic Data")}}

import time

# Track generation progress
generated_tables = {}
total_rows_generated = 0
execution_start_time = time.time()

# Get dataset configuration
dataset_id = dataset_config.get("dataset_id", "custom_dataset")
schema_pattern = dataset_config.get("schema_pattern", "oltp")

print(f"ğŸš€ Starting synthetic data generation for: {dataset_id}")
print(f"ğŸ“Š Schema pattern: {schema_pattern}")
print(f"ğŸ¯ Target environment: {{target_environment}}")
print(f"âš™ï¸ Generation mode: {generation_mode}")

{%- if generation_date is defined %}
print(f"ğŸ“… Generation date: {generation_date}")
{%- endif %}

{%- if start_date is defined and end_date is defined %}
print(f"ğŸ“… Date range: {start_date} to {end_date}")
{%- endif %}

{% if language_group == "synapse_pyspark" %}
# PySpark generation logic
if schema_pattern == "oltp":
    # Generate OLTP tables
    if "customers" in dataset_config.get("tables", []):
        print("ğŸ“‹ Generating customers table...")
        customers_df = generator.generate_customers_table(
            dataset_config.get("customer_rows", {{target_rows if target_rows is defined else 10000}})
        )
        generated_tables["customers"] = customers_df
        total_rows_generated += customers_df.count()
        
    if "products" in dataset_config.get("tables", []):
        print("ğŸ·ï¸ Generating products table...")
        products_df = generator.generate_products_table(
            dataset_config.get("product_rows", {{target_rows // 10 if target_rows is defined else 1000}})
        )
        generated_tables["products"] = products_df
        total_rows_generated += products_df.count()
        
    if "orders" in dataset_config.get("tables", []) and "customers" in generated_tables:
        print("ğŸ›’ Generating orders table...")
        orders_df = generator.generate_orders_table(
            dataset_config.get("order_rows", {{target_rows if target_rows is defined else 10000}}),
            generated_tables["customers"],
            generated_tables.get("products")
        )
        generated_tables["orders"] = orders_df
        total_rows_generated += orders_df.count()
        
    if "order_items" in dataset_config.get("tables", []) and "orders" in generated_tables:
        print("ğŸ“¦ Generating order items table...")
        order_items_df = generator.generate_order_items_table(
            generated_tables["orders"],
            generated_tables.get("products")
        )
        generated_tables["order_items"] = order_items_df
        total_rows_generated += order_items_df.count()

elif schema_pattern == "star_schema":
    # Generate star schema tables
    dimensions = {}
    
    for dim in dataset_config.get("dimensions", []):
        print(f"ğŸ“ Generating {dim}...")
        if dim == "dim_date":
            dim_df = generator.generate_date_dimension("2020-01-01", "2025-12-31")
        elif dim == "dim_customer":
            dim_df = generator.generate_customers_table(50000)
        elif dim == "dim_product":
            dim_df = generator.generate_products_table(5000)
        else:
            continue
            
        dimensions[dim] = dim_df
        generated_tables[dim] = dim_df
        total_rows_generated += dim_df.count()
    
    # Generate fact tables
    for fact in dataset_config.get("fact_tables", []):
        print(f"ğŸ“Š Generating {fact}...")
        if fact == "fact_sales":
            fact_df = generator.generate_fact_sales({{target_rows if target_rows is defined else 100000}}, dimensions)
            generated_tables[fact] = fact_df
            total_rows_generated += fact_df.count()

{% else %}
# Python generation logic
if schema_pattern == "oltp":
    # Generate OLTP tables using Python
    if "customers" in dataset_config.get("tables", []):
        print("ğŸ“‹ Generating customers table...")
        customers_df = generator.generate_customers_table(
            dataset_config.get("customer_rows", {{target_rows if target_rows is defined else 10000}})
        )
        generated_tables["customers"] = customers_df
        total_rows_generated += len(customers_df)
        
    if "products" in dataset_config.get("tables", []):
        print("ğŸ·ï¸ Generating products table...")
        products_df = generator.generate_products_table(
            dataset_config.get("product_rows", {{target_rows // 10 if target_rows is defined else 1000}})
        )
        generated_tables["products"] = products_df
        total_rows_generated += len(products_df)
        
    if "orders" in dataset_config.get("tables", []) and "customers" in generated_tables:
        print("ğŸ›’ Generating orders table...")
        orders_df = generator.generate_orders_table(
            dataset_config.get("order_rows", {{target_rows if target_rows is defined else 10000}}),
            generated_tables["customers"],
            generated_tables.get("products")
        )
        generated_tables["orders"] = orders_df
        total_rows_generated += len(orders_df)
        
    if "order_items" in dataset_config.get("tables", []) and "orders" in generated_tables:
        print("ğŸ“¦ Generating order items table...")
        order_items_df = generator.generate_order_items_table(
            generated_tables["orders"],
            generated_tables.get("products")
        )
        generated_tables["order_items"] = order_items_df
        total_rows_generated += len(order_items_df)

elif schema_pattern == "star_schema":
    # Generate star schema using dataset builder
    dataset_builder = DatasetBuilder(generator)
    dataset_tables = dataset_builder.build_retail_star_schema(scale_factor=1.0)
    
    for table_name, table_df in dataset_tables.items():
        generated_tables[table_name] = table_df
        total_rows_generated += len(table_df)
        print(f"âœ… Generated {table_name}: {len(table_df):,} rows")

{% endif %}

# Save generated tables
{% if output_mode is defined and output_mode == "parquet" %}
print("ğŸ’¾ Saving tables as Parquet files...")
output_path = "synthetic_data_output"
generator.export_to_parquet(generated_tables, output_path)
{% elif output_mode is defined and output_mode == "csv" %}
print("ğŸ’¾ Saving tables as CSV files...")
output_path = "synthetic_data_output"
generator.export_to_csv(generated_tables, output_path)
{% else %}
# Save to target environment tables
print("ğŸ’¾ Saving tables to target environment...")
{% if target_environment == "warehouse" %}
for table_name, table_df in generated_tables.items():
    sql_client.create_table_from_dataframe(table_df, table_name, overwrite=True)
    print(f"âœ… Saved {table_name} to warehouse with {len(table_df):,} rows")
{% else %}
for table_name, table_df in generated_tables.items():
    lh_utils.write_dataframe_to_table(table_df, table_name, mode="overwrite")
    print(f"âœ… Saved {table_name} to lakehouse with {table_df.count() if hasattr(table_df, 'count') else len(table_df):,} rows")
{% endif %}
{% endif %}

execution_time = time.time() - execution_start_time
print(f"â±ï¸ Total execution time: {execution_time:.2f} seconds")

{{macros.python_cell_with_heading("## âœ… Generation Complete")}}

# Display summary
print("=" * 60)
print("SYNTHETIC DATA GENERATION COMPLETE")
print("=" * 60)
print(f"Dataset: {dataset_config.get('dataset_name', 'Custom Dataset')}")
print(f"Generation Mode: {generation_mode}")
{% if generation_date is defined %}
print(f"Generation Date: {generation_date}")
{% endif %}
print(f"Tables Generated: {len(generated_tables)}")
print(f"Total Rows: {total_rows_generated:,}")
print(f"Execution Time: {execution_time:.2f} seconds")
print("=" * 60)

# Log completion
if 'notebookutils' in globals() and hasattr(notebookutils, 'log'):
    notebookutils.log(f"Synthetic data generation completed for {dataset_config.get('dataset_id')}")