#!/usr/bin/env python3
"""
Test the compiled notebooks by running them in the proper sequence
"""

import os
import sys
from pathlib import Path

# Set up environment - use absolute path for consistency
workspace_dir = os.path.abspath("./sample_project")
os.environ["FABRIC_WORKSPACE_REPO_DIR"] = workspace_dir
os.environ["FABRIC_ENVIRONMENT"] = "local"

# Add the project to Python path
sys.path.insert(0, "/workspaces/ingen_fab")


def insert_sample_config_data():
    """Insert sample configuration data by calling the compiled DDL notebook"""
    print("=== Running Compiled DDL Notebooks ===")

    import subprocess

    try:
        # Step 1: Run the initial creation notebook (creates config and log tables)
        print("Step 1: Creating config and log tables...")
        initial_creation_notebook = "./sample_project/fabric_workspace_items/ddl_scripts/Lakehouses/Config/001_Initial_Creation_Ingestion_Config_Lakehouses.Notebook/notebook-content.py"

        if not Path(initial_creation_notebook).exists():
            print(f"‚úó Initial creation notebook not found: {initial_creation_notebook}")
            return False

        result = subprocess.run(
            [sys.executable, initial_creation_notebook],
            capture_output=True,
            text=True,
            cwd=os.getcwd(),
        )

        if result.returncode == 0:
            print("‚úì Config and log tables created successfully")
        else:
            print(
                "‚ö†Ô∏è Initial creation notebook completed with warnings (may already exist)"
            )
            print("Output:", result.stdout[-500:] if result.stdout else "No output")

        # Step 2: Run the sample data insertion notebook
        print("\nStep 2: Inserting sample configuration data...")
        sample_data_notebook = "./sample_project/fabric_workspace_items/ddl_scripts/Lakehouses/Config/002_Sample_Data_Ingestion_Config_Lakehouses.Notebook/notebook-content.py"

        if not Path(sample_data_notebook).exists():
            print(f"‚úó Sample data notebook not found: {sample_data_notebook}")
            return False

        result = subprocess.run(
            [sys.executable, sample_data_notebook],
            capture_output=True,
            text=True,
            cwd=os.getcwd(),
        )

        if result.returncode == 0:
            print("‚úì Sample configuration data inserted successfully")
            print("Output:", result.stdout[-500:] if result.stdout else "No output")
            return True
        else:
            print("‚úó Sample data insertion failed")
            print(
                "Error output:",
                result.stderr[-500:] if result.stderr else "No error output",
            )
            print(
                "Standard output:",
                result.stdout[-500:] if result.stdout else "No output",
            )

            # Try to continue anyway - the manual approach as fallback
            print("\n‚ö†Ô∏è Falling back to manual sample data insertion...")
            return _insert_sample_data_fallback()

    except Exception as e:
        print(f"‚úó Error running compiled notebooks: {e}")
        print("\n‚ö†Ô∏è Falling back to manual sample data insertion...")
        return _insert_sample_data_fallback()


def _insert_sample_data_fallback():
    """Fallback method to insert sample data manually if notebook execution fails"""
    print("=== Manual Sample Data Insertion (Fallback) ===")

    from pyspark.sql import SparkSession

    # Create a simple Spark session
    spark = SparkSession.builder.appName("SampleDataFallback").getOrCreate()

    try:
        # Create sample configurations as a simple list of dictionaries
        sample_configs = [
            {
                "config_id": "csv_test_001",
                "config_name": "CSV Sales Data Test - Simple Archive",
                "source_file_path": "Files/sample_data/sales_data.csv",
                "source_file_format": "csv",
                "target_lakehouse_workspace_id": "test_workspace",
                "target_lakehouse_id": "test_lakehouse",
                "target_schema_name": "raw",
                "target_table_name": "sales_data",
                "file_delimiter": ",",
                "has_header": True,
                "encoding": "utf-8",
                "schema_inference": True,
                "write_mode": "overwrite",
                "execution_group": 1,
                "active_yn": "Y",
                # Archive: Simple flat structure
                "enable_archive": True,
                "archive_path": "archive/{filename}",
            },
            {
                "config_id": "csv_test_002",
                "config_name": "CSV Orders Data Test - Date-Based Archive with Cleanup",
                "source_file_path": "Files/sample_data/orders.csv",
                "source_file_format": "csv",
                "target_lakehouse_workspace_id": "test_workspace",
                "target_lakehouse_id": "test_lakehouse",
                "target_schema_name": "raw",
                "target_table_name": "orders",
                "file_delimiter": ",",
                "has_header": True,
                "encoding": "utf-8",
                "schema_inference": True,
                "write_mode": "overwrite",
                "execution_group": 1,
                "active_yn": "Y",
                # Archive: Date-partitioned folders (process timestamp) with cleanup
                "enable_archive": True,
                "archive_path": "archive/{YYYY}/{MM}/{DD}/{filename}",
                "cleanup_empty_dirs": True,  # Clean up empty directories after archiving
            },
            {
                "config_id": "json_test_003",
                "config_name": "JSON Products Data Test - Context-Based Archive",
                "source_file_path": "Files/sample_data/products.json",
                "source_file_format": "json",
                "target_lakehouse_workspace_id": "test_workspace",
                "target_lakehouse_id": "test_lakehouse",
                "target_schema_name": "raw",
                "target_table_name": "products",
                "encoding": "utf-8",
                "schema_inference": True,
                "write_mode": "overwrite",
                "execution_group": 1,
                "active_yn": "Y",
                # Archive: Using context variables
                "enable_archive": True,
                "archive_path": "archive/{source_name}/{target_table_name}/{YYYY}-{MM}-{DD}/{filename}",
            },
            {
                "config_id": "csv_test_004",
                "config_name": "CSV Customers Data Test - No Archive",
                "source_file_path": "Files/sample_data/customers.csv",
                "source_file_format": "csv",
                "target_lakehouse_workspace_id": "test_workspace",
                "target_lakehouse_id": "test_lakehouse",
                "target_schema_name": "raw",
                "target_table_name": "customers",
                "file_delimiter": ",",
                "has_header": True,
                "encoding": "utf-8",
                "schema_inference": True,
                "write_mode": "overwrite",
                "execution_group": 1,
                "active_yn": "Y",
                # Archive: Disabled
                "enable_archive": False,
            },
        ]

        # Create a simple DataFrame and temp view for testing
        import pandas as pd

        config_df = pd.DataFrame(sample_configs)

        # Convert to Spark DataFrame and create temp view
        spark_df = spark.createDataFrame(config_df)
        spark_df.createOrReplaceTempView("config_flat_file_ingestion")

        print(f"‚úì Created temporary config table with {len(sample_configs)} records")
        print("Sample configurations ready for processing")

        return True

    except Exception as e:
        print(f"‚úó Fallback insertion failed: {e}")
        return False
    finally:
        spark.stop()


def run_flat_file_processor():
    """Run the compiled flat file processor notebook"""
    print("\n=== Running Flat File Processor ===")

    processor_path = "./sample_project/fabric_workspace_items/flat_file_ingestion/flat_file_ingestion_processor.Notebook/notebook-content.py"

    if not Path(processor_path).exists():
        print(f"‚úó Processor notebook not found: {processor_path}")
        return False

    try:
        # Set parameters for the notebook
        os.environ["config_id"] = ""  # Process all active configs
        os.environ["execution_group"] = "1"
        os.environ["environment"] = "development"
        # Ensure consistent workspace directory
        os.environ["FABRIC_WORKSPACE_REPO_DIR"] = workspace_dir

        print(f"‚úì Running processor: {processor_path}")

        # Execute the notebook from root directory so table paths are consistent
        import subprocess

        result = subprocess.run(
            [sys.executable, processor_path],
            capture_output=True,
            text=True,
            cwd=os.getcwd(),
        )

        if result.returncode == 0:
            print("‚úì Flat file processor completed successfully!")
            print("\nProcessor output:")
            print(result.stdout)
            return True
        else:
            print("‚úó Flat file processor failed!")
            print("Error output:")
            print(result.stderr)
            return False

    except Exception as e:
        print(f"‚úó Error running processor: {e}")
        return False


def main():
    """Run the complete compiled notebook workflow"""
    print("üöÄ Testing Compiled Notebooks Workflow")
    print("=" * 60)

    try:
        # Step 0: Compile notebooks and DDL scripts first
        print("Step 0: Compiling notebooks and DDL scripts...")

        import subprocess

        # Compile flat file ingestion package with samples
        print("  - Compiling flat file ingestion package...")
        result = subprocess.run(
            [
                sys.executable,
                "-m",
                "ingen_fab.cli",
                "package",
                "ingest",
                "compile",
                "--include-samples",
            ],
            capture_output=True,
            text=True,
            cwd=os.getcwd(),
        )

        if result.returncode != 0:
            print("‚úó Failed to compile flat file ingestion package")
            print("Error:", result.stderr)
            return False
        else:
            print("‚úì Flat file ingestion package compiled successfully")

        # Compile DDL scripts
        print("  - Compiling DDL scripts...")
        result = subprocess.run(
            [sys.executable, "-m", "ingen_fab.cli", "ddl", "compile"],
            capture_output=True,
            text=True,
            cwd=os.getcwd(),
        )

        if result.returncode != 0:
            print("‚úó Failed to compile DDL scripts")
            print("Error:", result.stderr)
            return False
        else:
            print("‚úì DDL scripts compiled successfully")

        print("\n" + "-" * 40)

        # Step 1: Insert sample configuration data
        if not insert_sample_config_data():
            print("‚ùå Failed to insert sample data")
            return False

        # Step 2: Run flat file processor
        if not run_flat_file_processor():
            print("‚ùå Failed to run flat file processor")
            return False

        print("\n" + "=" * 60)
        print("‚úÖ COMPILED NOTEBOOK WORKFLOW COMPLETE")
        print("‚úÖ Sample configurations inserted")
        print("‚úÖ Flat file processor executed")
        print("‚úÖ Sample data files processed")

        return True

    except Exception as e:
        print(f"‚ùå Workflow failed: {e}")
        return False


if __name__ == "__main__":
    main()
