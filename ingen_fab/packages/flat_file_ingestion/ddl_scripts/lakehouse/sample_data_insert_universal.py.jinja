# Sample configuration data for flat file ingestion testing - Universal schema (Lakehouse version)
from pyspark.sql import Row

# Import the universal schema definition
{% include 'ddl/schemas/flat_file_config_schema_universal.py.jinja' %}

# Sample configuration records for testing - Using synthetic data generator files
sample_configs = [
    Row(
        config_id="synthetic_customers_folder_001",
        config_name="Synthetic Data - Customers Folder (Retail OLTP Small)",
        source_file_path="synthetic_data/csv/series/retail_oltp_small/test_data_feb_small/flat/snapshot_customers/snapshot_customers_20240201.csv",
        source_file_format="csv",
        source_workspace_id="{% raw %}{{varlib:sample_lh_workspace_id}}{% endraw %}",
        source_datastore_id="{% raw %}{{varlib:sample_lh_lakehouse_id}}{% endraw %}",
        source_datastore_type="lakehouse",
        source_file_root_path=None,  # No root path override needed
        target_workspace_id="{% raw %}{{varlib:sample_lh_workspace_id}}{% endraw %}",
        target_datastore_id="{% raw %}{{varlib:sample_lh_lakehouse_id}}{% endraw %}",
        target_datastore_type="lakehouse",
        target_schema_name="raw",
        target_table_name="synthetic_customers",
        staging_table_name=None,
        file_delimiter=",",
        has_header=True,
        encoding="utf-8",
        date_format="yyyy-MM-dd",
        timestamp_format="yyyy-MM-dd HH:mm:ss",
        schema_inference=True,
        custom_schema_json=None,
        partition_columns="",
        sort_columns="customer_id",
        write_mode="overwrite",
        merge_keys="",
        data_validation_rules=None,
        error_handling_strategy="log",
        execution_group=1,  # Folder-based snapshot processing (Group 1)
        active_yn="Y",
        created_date="2024-01-15",
        modified_date=None,
        created_by="system",
        modified_by=None,
        quote_character='"',
        escape_character='"',
        multiline_values=True,
        ignore_leading_whitespace=False,
        ignore_trailing_whitespace=False,
        null_value="",
        empty_value="",
        comment_character=None,
        max_columns=100,
        max_chars_per_column=50000,
        # New fields for incremental synthetic data import support
        import_pattern="single_file",
        date_partition_format=None,
        table_relationship_group="retail_oltp_single",
        batch_import_enabled=False,
        file_discovery_pattern=None,
        import_sequence_order=1,
        date_range_start=None,
        date_range_end=None,
        skip_existing_dates=None,
        source_is_folder=True,  # Read all part files from the folder
    ),
    Row(
        config_id="synthetic_orders_folder_002",
        config_name="Synthetic Data - Orders Folder (Retail OLTP Small)",
        source_file_path="synthetic_data/parquet/series/retail_oltp_small/high_volume_summer_data/flat/orders/",
        source_file_format="parquet",
        source_workspace_id="{% raw %}{{varlib:sample_lh_workspace_id}}{% endraw %}",
        source_datastore_id="{% raw %}{{varlib:sample_lh_lakehouse_id}}{% endraw %}",
        source_datastore_type="lakehouse",
        source_file_root_path=None,  # No root path override needed
        target_workspace_id="{% raw %}{{varlib:sample_lh_workspace_id}}{% endraw %}",
        target_datastore_id="{% raw %}{{varlib:sample_lh_lakehouse_id}}{% endraw %}",
        target_datastore_type="lakehouse",
        target_schema_name="raw",
        target_table_name="synthetic_orders",
        staging_table_name=None,
        file_delimiter=",",
        has_header=True,
        encoding="utf-8",
        date_format="yyyyMMdd",
        timestamp_format="yyyy-MM-dd HH:mm:ss",
        schema_inference=True,
        custom_schema_json=None,
        partition_columns="",
        sort_columns="order_id",
        write_mode="overwrite",
        merge_keys="",
        data_validation_rules=None,
        error_handling_strategy="log",
        execution_group=1,
        active_yn="Y",
        created_date="2024-01-15",
        modified_date=None,
        created_by="system",
        modified_by=None,
        quote_character='"',
        escape_character='"',
        multiline_values=True,
        ignore_leading_whitespace=False,
        ignore_trailing_whitespace=False,
        null_value="",
        empty_value="",
        comment_character=None,
        max_columns=100,
        max_chars_per_column=50000,
        # New fields for incremental synthetic data import support
        import_pattern="date_partitioned",
        date_partition_format="yyyyMMdd",
        table_relationship_group="retail_oltp_single",
        batch_import_enabled=False,
        file_discovery_pattern="orders_*.parquet",
        import_sequence_order=1,
        date_range_start=None,
        date_range_end=None,
        skip_existing_dates=None,
        source_is_folder=True,  # Read all part files from the folder
    ),
]

# Create DataFrame and insert records
df = target_lakehouse.get_connection.createDataFrame(sample_configs, schema)
target_lakehouse.write_to_table(
    df=df,
    table_name="config_flat_file_ingestion",
    mode="append"
)

print("✓ Inserted " + str(len(sample_configs)) + " sample configuration records")
print("✓ Active configurations:")
print("  - Customers Folder: Read all part files from a Spark-generated CSV folder")
print("  - Customers Single File: Read a specific CSV part file")
print("  - Orders Incremental: Date-partitioned CSV orders with wildcard pattern")
print("✓ Inactive configurations (enable when data is available):")
print("  - Large dataset support: Configuration for retail_oltp_large dataset")
print("  - Parquet format: Customers data in Parquet format")
print("  - Cross-workspace and Delta table examples")
print("✓ Files use synthetic data from test_data_feb_small directory")