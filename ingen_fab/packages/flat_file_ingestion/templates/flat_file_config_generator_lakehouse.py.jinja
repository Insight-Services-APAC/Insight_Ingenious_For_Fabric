{%- import 'shared/notebook/macros/notebook_macros.py.jinja' as macros -%}
{%- include "shared/notebook/headers/pyspark.py.jinja" %}

{% include "shared/notebook/environment/library_loader.py.jinja" %}

{{macros.python_cell_with_heading("## üóÇÔ∏è Load Custom Python Libraries")}}

if run_mode == "local":
    from ingen_fab.python_libs.common.config_utils import *
    from ingen_fab.python_libs.pyspark.lakehouse_utils import lakehouse_utils
    from ingen_fab.python_libs.pyspark.notebook_utils_abstraction import NotebookUtilsFactory
    notebookutils = NotebookUtilsFactory.create_instance() 
else:
    files_to_load = [
        "ingen_fab/python_libs/common/config_utils.py",
        "ingen_fab/python_libs/pyspark/lakehouse_utils.py",
        "ingen_fab/python_libs/pyspark/notebook_utils_abstraction.py"
    ]
    load_python_modules_from_path(mount_path, files_to_load)

{{macros.python_cell_with_heading("## ‚öôÔ∏è Configuration Settings")}}

# Configuration for file discovery
scan_folder_path = "{{scan_folder_path | default('synthetic_data')}}"  # Folder to scan
recursive_scan = {{recursive_scan | default('True')}}  # Scan subfolders
max_files_to_sample = {{max_files_to_sample | default(5)}}  # Max files to sample per pattern
target_schema = "{{target_schema | default('raw')}}"  # Default target schema
execution_group_start = {{execution_group_start | default(100)}}  # Starting execution group number

{{macros.python_cell_with_heading("## üÜï Initialize Components")}}

import os
import glob
import json
import uuid
from datetime import datetime, date
from pathlib import Path
from typing import Dict, List, Tuple, Optional

# Get configurations
configs: ConfigsObject = get_configs_as_object()

# Initialize lakehouse utils
target_lakehouse_id = get_config_value("config_lakehouse_id")
target_workspace_id = get_config_value("config_workspace_id")

lh_utils = lakehouse_utils(
    target_workspace_id=target_workspace_id,
    target_lakehouse_id=target_lakehouse_id,
    spark=spark
)

{{macros.python_cell_with_heading("## üîç File Discovery Functions")}}

def discover_files(base_path: str, recursive: bool = True) -> Dict[str, List[str]]:
    """Discover files in the specified path and group by pattern
    
    This function understands Spark output conventions where files are actually folders
    containing part files (e.g., customers.csv/ contains part-00000-*.csv files)
    """
    file_groups = {}
    
    # Prepare full path for scanning
    full_base_path = f"{base_path}"
    
    try:
        # Get all directories and files in the base path using the new list_all method
        all_items = lh_utils.list_all(full_base_path, recursive=recursive)
        
        # Process each item to identify file types
        for item_path in all_items:
            # Extract relative path from full path - handle absolute paths properly
            # item_path looks like: /workspaces/ingen_fab/tmp/spark/Files/synthetic_data/csv/single/customers.csv
            # We want: csv/single/customers.csv
            
            # Find the part after synthetic_data (or the base path)
            synthetic_data_marker = f"/tmp/spark/Files/{base_path}/"
            if synthetic_data_marker in item_path:
                rel_path = item_path.split(synthetic_data_marker)[1]
            else:
                # Fallback - try other patterns
                rel_path = item_path.replace(full_base_path + "/", "").replace(full_base_path, "")
                if rel_path.startswith("/"):
                    rel_path = rel_path[1:]
            
            # Check if this looks like a file format folder (e.g., customers.csv, data.parquet)
            path_parts = rel_path.split('/')
            for part in path_parts:
                if '.' in part:
                    file_ext = part.split('.')[-1].lower()
                    if file_ext in ['csv', 'parquet', 'json', 'avro']:
                        # Check if this is actually a folder containing part files
                        # Build the relative path to the folder
                        folder_rel_path_parts = path_parts[:path_parts.index(part)+1]
                        folder_rel_path = '/'.join(folder_rel_path_parts)
                        
                        # Convert back to absolute path for checking
                        absolute_folder_path = item_path.replace('/' + '/'.join(path_parts), '/' + folder_rel_path)
                        
                        try:
                            # Check if this absolute path exists and contains part files
                            from pathlib import Path as PathLib
                            abs_path = PathLib(absolute_folder_path)
                            if abs_path.exists() and abs_path.is_dir():
                                part_files = list(abs_path.glob("part-*"))
                                has_part_files = len(part_files) > 0
                                
                                if has_part_files:
                                    if file_ext not in file_groups:
                                        file_groups[file_ext] = []
                                    if folder_rel_path not in file_groups[file_ext]:
                                        file_groups[file_ext].append(folder_rel_path)
                        except:
                            # If we can't list contents, it might be a regular file
                            pass
                        break
        
    except Exception as e:
        print(f"Warning: Error discovering files in {full_base_path}: {str(e)}")
        # Fallback to glob for local testing
        if run_mode == "local":
            file_patterns = {
                'csv': ['**/*.csv'] if recursive else ['*.csv'],
                'parquet': ['**/*.parquet'] if recursive else ['*.parquet'],
                'json': ['**/*.json'] if recursive else ['*.json'],
                'avro': ['**/*.avro'] if recursive else ['*.avro'],
                'delta': ['**/*/_delta_log'] if recursive else ['*/_delta_log']
            }
            
            for file_type, patterns in file_patterns.items():
                found_files = []
                for pattern in patterns:
                    local_path = Path(base_path) / pattern.replace('**/', '')
                    found_files.extend(glob.glob(str(local_path), recursive=recursive))
                
                if found_files:
                    # Convert to relative paths
                    rel_files = [str(Path(f).relative_to(base_path)) for f in found_files]
                    file_groups[file_type] = list(set(rel_files))
    
    return file_groups

def analyze_file_structure(file_path: str) -> Dict[str, any]:
    """Analyze a file to determine its structure and properties"""
    file_info = {
        'path': file_path,
        'format': Path(file_path).suffix.lstrip('.'),
        'size_mb': 0,
        'is_folder': False,
        'has_header': None,
        'delimiter': None,
        'encoding': 'utf-8',
        'columns': [],
        'sample_data': None,
        'date_pattern': None,
        'is_partitioned': False
    }
    
    try:
        # Check if it's a folder with part files (Spark output)
        full_file_path = f"Files/{file_path}" if not file_path.startswith("Files/") else file_path
        try:
            folder_contents = lh_utils.list_files(full_file_path)
            part_files = [f for f in folder_contents if 'part-' in f]
            if part_files:
                file_info['is_folder'] = True
                file_info['part_file_count'] = len(part_files)
            else:
                file_info['is_folder'] = False
        except:
            file_info['is_folder'] = False
        
        # Set default properties based on file type without reading the actual file
        if file_info['format'] in ['csv', 'parquet', 'json', 'avro']:
            # Set reasonable defaults based on file format
            if file_info['format'] == 'csv':
                file_info['has_header'] = True  # Assume CSV files have headers
                file_info['delimiter'] = ','  # Default delimiter
                file_info['columns'] = []  # Will be inferred during actual ingestion
            elif file_info['format'] == 'parquet':
                file_info['columns'] = []  # Parquet has embedded schema
            elif file_info['format'] == 'json':
                file_info['columns'] = []  # JSON schema will be inferred
            elif file_info['format'] == 'avro':
                file_info['columns'] = []  # Avro has embedded schema
            
            # Don't read sample data during discovery - this is just for configuration generation
            file_info['sample_data'] = None
        
        # Check for date partitioning patterns - improved detection
        path_parts = file_path.split('/')
        date_parts = []
        date_indices = []
        
        # Look for consecutive numeric parts that could be year/month/day
        for i, part in enumerate(path_parts):
            if part.isdigit():
                if len(part) == 4 and 2020 <= int(part) <= 2030:  # Year
                    date_parts.append(('year', part, i))
                    date_indices.append(i)
                elif len(part) == 2 and 1 <= int(part) <= 12:  # Month
                    date_parts.append(('month', part, i))
                    date_indices.append(i)
                elif len(part) == 2 and 1 <= int(part) <= 31:  # Day
                    date_parts.append(('day', part, i))
                    date_indices.append(i)
        
        # Check if we have a date hierarchy (year/month/day pattern)
        if len(date_parts) >= 2:
            # Sort by index to check if they're consecutive
            date_indices.sort()
            is_consecutive = all(date_indices[i] + 1 == date_indices[i + 1] for i in range(len(date_indices) - 1))
            
            if is_consecutive and len(date_parts) >= 2:
                file_info['is_partitioned'] = True
                file_info['date_pattern'] = 'YYYY/MM/DD'
                file_info['partition_depth'] = len(date_parts)
                
                # Extract date range information
                year_parts = [p[1] for p in date_parts if p[0] == 'year']
                month_parts = [p[1] for p in date_parts if p[0] == 'month']
                day_parts = [p[1] for p in date_parts if p[0] == 'day']
                
                file_info['date_components'] = {
                    'year': year_parts[0] if year_parts else None,
                    'month': month_parts[0] if month_parts else None,
                    'day': day_parts[0] if day_parts else None
                }
                
    except Exception as e:
        file_info['error'] = str(e)
    
    return file_info

def infer_table_relationships(file_groups: Dict[str, List[Dict]]) -> Dict[str, str]:
    """Infer relationships between tables based on naming patterns"""
    relationships = {}
    
    # Common relationship patterns
    fact_patterns = ['fact_', 'f_', 'sales', 'transactions', 'events', 'orders']
    dimension_patterns = ['dim_', 'd_', 'customer', 'product', 'location', 'date']
    
    for group_name, files in file_groups.items():
        table_names = [f['inferred_table_name'] for f in files]
        
        # Check if it's a fact or dimension table group
        if any(pattern in group_name.lower() for pattern in fact_patterns):
            relationships[group_name] = 'fact_tables'
        elif any(pattern in group_name.lower() for pattern in dimension_patterns):
            relationships[group_name] = 'dimension_tables'
        else:
            relationships[group_name] = 'general_tables'
    
    return relationships

{{macros.python_cell_with_heading("## üìä Discover Files in Target Folder")}}

print(f"üîç Scanning folder: {scan_folder_path}")
print(f"üìÅ Recursive scan: {recursive_scan}")
print("=" * 60)

# Discover files
discovered_files = discover_files(scan_folder_path, recursive_scan)

# Display summary
total_files = sum(len(files) for files in discovered_files.values())
print(f"‚úÖ Found {total_files} files across {len(discovered_files)} file types")
print("")
print("File Type Summary:")
for file_type, files in discovered_files.items():
    print(f"  - {file_type.upper()}: {len(files)} files")

{{macros.python_cell_with_heading("## üî¨ Analyze File Structures")}}

# Analyze sample files from each group
analyzed_files = {}

for file_type, file_list in discovered_files.items():
    print("")
    print(f"üìã Analyzing {file_type.upper()} files...")
    analyzed_files[file_type] = []
    
    # Analyze all files to properly group by table name and detect date ranges
    # For large datasets, we still need to see all files to create proper incremental configs
    sample_files = file_list
    
    for file_path in sample_files:
        print(f"  - Analyzing: {file_path}")
        file_info = analyze_file_structure(file_path)
        
        # Infer table name from file path - extract meaningful table name from folder structure
        path_parts = file_path.split('/')
        
        # Look for meaningful table names in the path, skipping date folders and common prefixes
        table_name_candidates = []
        for part in path_parts:
            # Skip date patterns (YYYY, MM, DD, or date-like folders)
            if part.isdigit() and (len(part) == 4 or len(part) == 2):
                continue
            # Skip common folder names and file extensions
            if part.lower() in ['csv', 'parquet', 'json', 'avro', 'data', 'files', 'series', 'nested']:
                continue
            # Skip file names that are just 'data' with extension
            if part.startswith('data.'):
                continue
            # Add potential table name
            if '.' not in part or file_info['is_folder']:
                table_name_candidates.append(part)
        
        # Use the most specific meaningful part (usually the last meaningful folder)
        if table_name_candidates:
            # Prefer the last meaningful folder name before date folders
            table_name = table_name_candidates[-1]
            # Add prefix for synthetic data
            if 'synthetic_data' in file_path or any('retail_oltp' in part for part in path_parts):
                table_name = f"synth_data_{table_name}"
        else:
            # Fallback to file name
            file_name = path_parts[-1].replace(f'.{file_type}', '')
            table_name = file_name
        
        file_info['inferred_table_name'] = table_name.replace('-', '_').replace(' ', '_').lower()
        file_info['path_parts'] = path_parts  # Store for date range detection
        analyzed_files[file_type].append(file_info)

{{macros.python_cell_with_heading("## üîß Generate Ingestion Configurations")}}

def generate_config_id(table_name: str, file_type: str) -> str:
    """Generate a unique config ID"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    return f"auto_{table_name}_{file_type}_{timestamp}"

def determine_execution_group(file_info: Dict, base_group: int) -> int:
    """Determine execution group based on file characteristics"""
    # Assign groups based on dependencies and file types
    if 'dim_' in file_info['inferred_table_name'] or 'dimension' in file_info['inferred_table_name']:
        return base_group  # Dimensions first
    elif 'fact_' in file_info['inferred_table_name']:
        return base_group + 10  # Facts after dimensions
    elif file_info['is_partitioned']:
        return base_group + 20  # Partitioned data later
    else:
        return base_group + 5  # General tables in between

def generate_ingestion_config(file_info: Dict, config_counter: int) -> Dict:
    """Generate a flat file ingestion configuration from file analysis"""
    
    config = {
        "config_id": generate_config_id(file_info['inferred_table_name'], file_info['format']),
        "config_name": f"Auto-discovered: {file_info['inferred_table_name']} ({file_info['format']})",
        "source_file_path": file_info['path'],
        "source_file_format": file_info['format'],
        "source_workspace_id": "{% raw %}{{varlib:config_workspace_id}}{% endraw %}",
        "source_datastore_id": "{% raw %}{{varlib:config_lakehouse_id}}{% endraw %}",
        "source_datastore_type": "lakehouse",
        "source_file_root_path": "Files",
        "target_workspace_id": "{% raw %}{{varlib:config_workspace_id}}{% endraw %}",
        "target_datastore_id": "{% raw %}{{varlib:config_lakehouse_id}}{% endraw %}",
        "target_datastore_type": "lakehouse",
        "target_schema_name": target_schema,
        "target_table_name": file_info['inferred_table_name'],
        "staging_table_name": None,
        "file_delimiter": file_info.get('delimiter', ',') if file_info['format'] == 'csv' else None,
        "has_header": file_info.get('has_header', True) if file_info['format'] == 'csv' else None,
        "encoding": file_info.get('encoding', 'utf-8') if file_info['format'] == 'csv' else None,
        "date_format": "yyyy-MM-dd",
        "timestamp_format": "yyyy-MM-dd HH:mm:ss",
        "schema_inference": True,
        "custom_schema_json": None,
        "partition_columns": "",
        "sort_columns": file_info['columns'][0] if file_info.get('columns') else "",
        "write_mode": "append" if file_info['is_partitioned'] else "overwrite",
        "merge_keys": "",
        "execution_group": determine_execution_group(file_info, execution_group_start),
        "active_yn": "Y",
        "created_date": datetime.now().strftime('%Y-%m-%d'),
        "modified_date": None,
        "created_by": "auto_discovery",
        "modified_by": None,
        "quote_character": '"' if file_info['format'] == 'csv' else None,
        "escape_character": '"' if file_info['format'] == 'csv' else None,
        "multiline_values": True if file_info['format'] == 'csv' else None,
        "ignore_leading_whitespace": False if file_info['format'] == 'csv' else None,
        "ignore_trailing_whitespace": False if file_info['format'] == 'csv' else None,
        "null_value": "" if file_info['format'] == 'csv' else None,
        "empty_value": "" if file_info['format'] == 'csv' else None,
        "comment_character": None,
        "max_columns": 100 if file_info['format'] == 'csv' else None,
        "max_chars_per_column": 50000 if file_info['format'] == 'csv' else None,
        "import_pattern": file_info.get('import_pattern', "date_partitioned" if file_info['is_partitioned'] else "single_file"),
        "date_partition_format": file_info.get('date_pattern'),
        "table_relationship_group": f"auto_discovered_{datetime.now().strftime('%Y%m%d')}",
        "batch_import_enabled": file_info.get('batch_import_enabled', file_info['is_partitioned']),
        "file_discovery_pattern": file_info.get('file_discovery_pattern', f"**/*.{file_info['format']}" if file_info['is_partitioned'] else None),
        "import_sequence_order": config_counter,
        "date_range_start": file_info.get('date_range_start'),
        "date_range_end": file_info.get('date_range_end'),
        "skip_existing_dates": True if file_info['is_partitioned'] else None,
        "source_is_folder": file_info['is_folder']
    }
    
    return config

# Group files by table name for incremental configs
grouped_configs = {}

for file_type, file_infos in analyzed_files.items():
    for file_info in file_infos:
        if 'error' not in file_info:
            table_name = file_info['inferred_table_name']
            
            if table_name not in grouped_configs:
                grouped_configs[table_name] = {
                    'base_info': file_info,
                    'files': [file_info],
                    'is_partitioned': file_info.get('is_partitioned', False),
                    'date_ranges': []
                }
            else:
                grouped_configs[table_name]['files'].append(file_info)
                if file_info.get('is_partitioned', False):
                    grouped_configs[table_name]['is_partitioned'] = True
            
            # Collect date components for range detection
            if file_info.get('date_components'):
                grouped_configs[table_name]['date_ranges'].append(file_info['date_components'])

# Generate configurations - one per table group
all_configs = []
config_counter = 1

for table_name, group_info in grouped_configs.items():
    base_file_info = group_info['base_info']
    files = group_info['files']
    is_partitioned = group_info['is_partitioned']
    
    if is_partitioned and len(files) > 1:
        # Create single incremental config for date-partitioned files
        # Calculate date range from all files
        date_ranges = group_info['date_ranges']
        if date_ranges:
            years = [d['year'] for d in date_ranges if d['year']]
            months = [d['month'] for d in date_ranges if d['month']]
            days = [d['day'] for d in date_ranges if d['day']]
            
            # Determine the base path (remove date components)
            base_path_parts = base_file_info['path_parts'][:]
            # Remove date parts from the end
            while base_path_parts and base_path_parts[-1].isdigit():
                base_path_parts.pop()
            # Remove data file name
            if base_path_parts and base_path_parts[-1].startswith('data.'):
                base_path_parts.pop()
            
            base_path = '/'.join(base_path_parts)
            
            # Update file info for incremental config
            incremental_info = base_file_info.copy()
            incremental_info['path'] = base_path
            incremental_info['import_pattern'] = 'date_partitioned'
            incremental_info['batch_import_enabled'] = True
            incremental_info['file_discovery_pattern'] = f"**/*.{base_file_info['format']}"
            
            # Set date ranges
            if years:
                min_year, max_year = min(years), max(years)
                incremental_info['date_range_start'] = f"{min_year}-01-01"
                incremental_info['date_range_end'] = f"{max_year}-12-31"
            
            config = generate_ingestion_config(incremental_info, config_counter)
            config['config_name'] = f"Auto-discovered incremental: {table_name} ({base_file_info['format']}) - {len(files)} date partitions"
            all_configs.append(config)
        else:
            # Fallback to regular config if no date ranges found
            config = generate_ingestion_config(base_file_info, config_counter)
            all_configs.append(config)
    else:
        # Create regular config for non-partitioned or single files
        config = generate_ingestion_config(base_file_info, config_counter)
        all_configs.append(config)
    
    config_counter += 1

print("")
print(f"‚úÖ Generated {len(all_configs)} ingestion configurations")

{{macros.python_cell_with_heading("## üíæ Save Configurations")}}

# Import the schema definition
{% include 'ddl/schemas/flat_file_config_schema_universal.py.jinja' %}

# Convert configs to Spark DataFrame
from pyspark.sql import Row

config_rows = []
for config in all_configs:
    # Convert dict to Row, ensuring all required fields are present
    row_data = {
        "config_id": config["config_id"],
        "config_name": config["config_name"],
        "source_file_path": config["source_file_path"],
        "source_file_format": config["source_file_format"],
        "source_workspace_id": config.get("source_workspace_id"),
        "source_datastore_id": config.get("source_datastore_id"),
        "source_datastore_type": config.get("source_datastore_type"),
        "source_file_root_path": config.get("source_file_root_path"),
        "target_workspace_id": config["target_workspace_id"],
        "target_datastore_id": config["target_datastore_id"],
        "target_datastore_type": config["target_datastore_type"],
        "target_schema_name": config["target_schema_name"],
        "target_table_name": config["target_table_name"],
        "staging_table_name": config.get("staging_table_name"),
        "file_delimiter": config.get("file_delimiter"),
        "has_header": config.get("has_header"),
        "encoding": config.get("encoding"),
        "date_format": config.get("date_format"),
        "timestamp_format": config.get("timestamp_format"),
        "schema_inference": config["schema_inference"],
        "custom_schema_json": config.get("custom_schema_json"),
        "partition_columns": config.get("partition_columns", ""),
        "sort_columns": config.get("sort_columns", ""),
        "write_mode": config["write_mode"],
        "merge_keys": config.get("merge_keys", ""),
        "execution_group": config["execution_group"],
        "active_yn": config["active_yn"],
        "created_date": config["created_date"],
        "modified_date": config.get("modified_date"),
        "created_by": config["created_by"],
        "modified_by": config.get("modified_by"),
        "quote_character": config.get("quote_character"),
        "escape_character": config.get("escape_character"),
        "multiline_values": config.get("multiline_values"),
        "ignore_leading_whitespace": config.get("ignore_leading_whitespace"),
        "ignore_trailing_whitespace": config.get("ignore_trailing_whitespace"),
        "null_value": config.get("null_value"),
        "empty_value": config.get("empty_value"),
        "comment_character": config.get("comment_character"),
        "max_columns": config.get("max_columns"),
        "max_chars_per_column": config.get("max_chars_per_column"),
        "import_pattern": config.get("import_pattern"),
        "date_partition_format": config.get("date_partition_format"),
        "table_relationship_group": config.get("table_relationship_group"),
        "batch_import_enabled": config.get("batch_import_enabled"),
        "file_discovery_pattern": config.get("file_discovery_pattern"),
        "import_sequence_order": config.get("import_sequence_order"),
        "date_range_start": config.get("date_range_start"),
        "date_range_end": config.get("date_range_end"),
        "skip_existing_dates": config.get("skip_existing_dates"),
        "source_is_folder": config.get("source_is_folder")
    }
    config_rows.append(Row(**row_data))

# Create DataFrame with the schema - use lakehouse_utils
configs_df = lh_utils.get_connection.createDataFrame(config_rows, schema)

# Option 1: Append to existing config table
print("")
print("üìù Option 1: Append configurations to existing table")
print("To append these configurations, uncomment and run:")
print("# lh_utils.save_dataframe_as_table(configs_df, 'config_flat_file_ingestion', mode='append')")

# Option 2: Save to a temporary table for review
temp_table_name = f"temp_auto_configs_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
lh_utils.save_dataframe_as_table(configs_df, temp_table_name, mode="overwrite")
print("")
print(f"‚úÖ Saved configurations to temporary table: {temp_table_name}")

# Display all configurations
print("")
print("üìã All Generated Configurations:")
configs_df.select("config_id", "config_name", "source_file_path", "target_table_name", "execution_group").show(truncate=False)

{{macros.python_cell_with_heading("## üìä Configuration Summary Report")}}

# Generate summary report
print("=" * 80)
print("FILE DISCOVERY AND CONFIGURATION GENERATION SUMMARY")
print("=" * 80)
print(f"Scan Path: {scan_folder_path}")
print(f"Total Files Discovered: {total_files}")
print(f"Configurations Generated: {len(all_configs)}")
print(f"Temporary Table: {temp_table_name}")
print("")
print("File Types:")
for file_type, count in [(ft, len(files)) for ft, files in discovered_files.items()]:
    print(f"  - {file_type.upper()}: {count} files")

print("")
print("Execution Groups:")
exec_groups = {}
for config in all_configs:
    group = config['execution_group']
    exec_groups[group] = exec_groups.get(group, 0) + 1

for group in sorted(exec_groups.keys()):
    print(f"  - Group {group}: {exec_groups[group]} configurations")

print("")
print("Table Relationship Groups:")
rel_groups = {}
for config in all_configs:
    group = config['table_relationship_group']
    rel_groups[group] = rel_groups.get(group, 0) + 1

for group, count in rel_groups.items():
    print(f"  - {group}: {count} tables")

print("")
print("üìù All Generated Configuration Details:")
print("=" * 120)
for i, config in enumerate(all_configs, 1):
    print(f"{i:2d}. Config ID: {config['config_id']}")
    print(f"    Name: {config['config_name']}")
    print(f"    Source: {config['source_file_path']} ({config['source_file_format']})")
    print(f"    Target: {config['target_schema_name']}.{config['target_table_name']}")
    print(f"    Execution Group: {config['execution_group']}")
    print(f"    Write Mode: {config['write_mode']}")
    print(f"    Import Pattern: {config['import_pattern']}")
    if config.get('date_range_start') and config.get('date_range_end'):
        print(f"    Date Range: {config['date_range_start']} to {config['date_range_end']}")
    if config.get('file_discovery_pattern'):
        print(f"    Discovery Pattern: {config['file_discovery_pattern']}")
    if config.get('file_delimiter'):
        print(f"    CSV Settings: delimiter='{config['file_delimiter']}', header={config['has_header']}")
    print("")

print("‚úÖ Configuration generation complete!")
print(f"Review the configurations in table: {temp_table_name}")
print("Once reviewed, append to config_flat_file_ingestion table to activate.")

{{ macros.exit_notebook("success") }}

{% include 'shared/notebook/cells/footer.py.jinja' %}