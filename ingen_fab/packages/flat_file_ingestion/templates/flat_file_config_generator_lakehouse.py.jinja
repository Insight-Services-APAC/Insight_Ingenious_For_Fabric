{%- import 'shared/notebook/macros/notebook_macros.py.jinja' as macros -%}
{%- include "shared/notebook/headers/pyspark.py.jinja" %}
{% include "shared/notebook/environment/library_loader.py.jinja" %}
{{ macros.python_cell_with_heading("## üóÇÔ∏è Load Custom Python Libraries") }}
if run_mode == "local":
from ingen_fab.python_libs.common.config_utils import *
from ingen_fab.python_libs.pyspark.lakehouse_utils import lakehouse_utils
from ingen_fab.python_libs.pyspark.notebook_utils_abstraction import NotebookUtilsFactory
notebookutils = NotebookUtilsFactory.create_instance()
else:
files_to_load = [
"ingen_fab/python_libs/common/config_utils.py",
"ingen_fab/python_libs/pyspark/lakehouse_utils.py",
"ingen_fab/python_libs/pyspark/notebook_utils_abstraction.py"
]
load_python_modules_from_path(mount_path, files_to_load)
{{ macros.python_cell_with_heading("## ‚öôÔ∏è Configuration Settings") }}
# Configuration for file discovery
scan_folder_path = "{{ scan_folder_path | default("synthetic_data") }}"  # Folder to scan
recursive_scan = {{ recursive_scan | default("True") }}  # Scan subfolders
max_files_to_sample = {{ max_files_to_sample | default(5) }}  # Max files to sample per pattern
target_schema = "{{ target_schema | default("raw") }}"  # Default target schema
execution_group_start = {{ execution_group_start | default(100) }}  # Starting execution group number
{{ macros.python_cell_with_heading("## üÜï Initialize Components") }}
import os
import glob
import json
import uuid
from datetime import datetime, date
from pathlib import Path
from typing import Dict, List, Tuple, Optional
# Get configurations
configs: ConfigsObject = get_configs_as_object()
# Initialize lakehouse utils
target_lakehouse_id = get_config_value("config_lakehouse_id")
target_workspace_id = get_config_value("config_workspace_id")
lh_utils = lakehouse_utils(
target_workspace_id=target_workspace_id,
target_lakehouse_id=target_lakehouse_id,
spark=spark
)
{{ macros.python_cell_with_heading("## üîç File Discovery Functions") }}
def discover_files(base_path: str, recursive: bool = True) -> Dict[str, List[str]]:
"""Discover files in the specified path and group by pattern
This function understands Spark output conventions where files are actually folders
containing part files (e.g., customers.csv/ contains part-00000-*.csv files)
"""
file_groups = {}
# Prepare full path for scanning
full_base_path = f"{base_path}"
try:
# Get all directories and files in the base path using the new list_all method
all_items = lh_utils.list_all(full_base_path, recursive=recursive)
# Process each item to identify file types
for item_path in all_items:
# Extract relative path from full path - handle absolute paths properly
# item_path looks like: /workspaces/ingen_fab/tmp/spark/Files/synthetic_data/csv/single/customers.csv
# We want: csv/single/customers.csv
# Find the part after synthetic_data (or the base path)
synthetic_data_marker = f"/tmp/spark/Files/{base_path}/"
if synthetic_data_marker in item_path:
rel_path = item_path.split(synthetic_data_marker)[1]
else:
# Fallback - try other patterns
rel_path = item_path.replace(full_base_path + "/", "").replace(full_base_path, "")
if rel_path.startswith("/"):
rel_path = rel_path[1:]
# Check if this looks like a file format folder (e.g., customers.csv, data.parquet)
path_parts = rel_path.split('/')
for part in path_parts:
if '.' in part:
file_ext = part.split('.')[-1].lower()
if file_ext in ['csv', 'parquet', 'json', 'avro']:
# Check if this is actually a folder containing part files
# Build the relative path to the folder
folder_rel_path_parts = path_parts[:path_parts.index(part)+1]
folder_rel_path = '/'.join(folder_rel_path_parts)
# Convert back to absolute path for checking
absolute_folder_path = item_path.replace('/' + '/'.join(path_parts), '/' + folder_rel_path)
try:
# Check if this absolute path exists and contains part files
from pathlib import Path as PathLib
abs_path = PathLib(absolute_folder_path)
if abs_path.exists() and abs_path.is_dir():
part_files = list(abs_path.glob("part-*"))
has_part_files = len(part_files) > 0
if has_part_files:
if file_ext not in file_groups:
file_groups[file_ext] = []
if folder_rel_path not in file_groups[file_ext]:
file_groups[file_ext].append(folder_rel_path)
except:
# If we can't list contents, it might be a regular file
pass
break
except Exception as e:
print(f"Warning: Error discovering files in {full_base_path}: {str(e)}")
# Fallback to glob for local testing
if run_mode == "local":
file_patterns = {
'csv': ['**/*.csv'] if recursive else ['*.csv'],
'parquet': ['**/*.parquet'] if recursive else ['*.parquet'],
'json': ['**/*.json'] if recursive else ['*.json'],
'avro': ['**/*.avro'] if recursive else ['*.avro'],
'delta': ['**/*/_delta_log'] if recursive else ['*/_delta_log']
}
for file_type, patterns in file_patterns.items():
found_files = []
for pattern in patterns:
local_path = Path(base_path) / pattern.replace('**/', '')
found_files.extend(glob.glob(str(local_path), recursive=recursive))
if found_files:
# Convert to relative paths
rel_files = [str(Path(f).relative_to(base_path)) for f in found_files]
file_groups[file_type] = list(set(rel_files))
return file_groups
def analyze_file_structure(file_path: str) -> Dict[str, any]:
"""Analyze a file to determine its structure and properties"""
file_info = {
'path': file_path,
'format': Path(file_path).suffix.lstrip('.'),
'size_mb': 0,
'is_folder': False,
'has_header': None,
'delimiter': None,
'encoding': 'utf-8',
'columns': [],
'sample_data': None,
'date_pattern': None,
'is_partitioned': False
}
try:
# Check if it's a folder with part files (Spark output)
full_file_path = f"Files/{file_path}" if not file_path.startswith("Files/") else file_path
try:
folder_contents = lh_utils.list_files(full_file_path)
part_files = [f for f in folder_contents if 'part-' in f]
if part_files:
file_info['is_folder'] = True
file_info['part_file_count'] = len(part_files)
else:
file_info['is_folder'] = False
except:
file_info['is_folder'] = False
# Set default properties based on file type without reading the actual file
if file_info['format'] in ['csv', 'parquet', 'json', 'avro']:
# Set reasonable defaults based on file format
if file_info['format'] == 'csv':
file_info['has_header'] = True  # Assume CSV files have headers
file_info['delimiter'] = ','  # Default delimiter
file_info['columns'] = []  # Will be inferred during actual ingestion
elif file_info['format'] == 'parquet':
file_info['columns'] = []  # Parquet has embedded schema
elif file_info['format'] == 'json':
file_info['columns'] = []  # JSON schema will be inferred
elif file_info['format'] == 'avro':
file_info['columns'] = []  # Avro has embedded schema
# Don't read sample data during discovery - this is just for configuration generation
file_info['sample_data'] = None
# Check for date partitioning patterns - improved detection
path_parts = file_path.split('/')
date_parts = []
date_indices = []
# Look for consecutive numeric parts that could be year/month/day
for i, part in enumerate(path_parts):
if part.isdigit():
if len(part) == 4 and 2020 <= int(part) <= 2030:  # Year
date_parts.append(('year', part, i))
date_indices.append(i)
elif len(part) == 2 and 1 <= int(part) <= 12:  # Month
date_parts.append(('month', part, i))
date_indices.append(i)
elif len(part) == 2 and 1 <= int(part) <= 31:  # Day
date_parts.append(('day', part, i))
date_indices.append(i)
# Check if we have a date hierarchy (year/month/day pattern)
if len(date_parts) >= 2:
# Sort by index to check if they're consecutive
date_indices.sort()
is_consecutive = all(date_indices[i] + 1 == date_indices[i + 1] for i in range(len(date_indices) - 1))
if is_consecutive and len(date_parts) >= 2:
file_info['is_partitioned'] = True
file_info['date_pattern'] = 'YYYY/MM/DD'
file_info['partition_depth'] = len(date_parts)
# Extract date range information
year_parts = [p[1] for p in date_parts if p[0] == 'year']
month_parts = [p[1] for p in date_parts if p[0] == 'month']
day_parts = [p[1] for p in date_parts if p[0] == 'day']
file_info['date_components'] = {
'year': year_parts[0] if year_parts else None,
'month': month_parts[0] if month_parts else None,
'day': day_parts[0] if day_parts else None
}
except Exception as e:
file_info['error'] = str(e)
return file_info
def infer_table_relationships(file_groups: Dict[str, List[Dict]]) -> Dict[str, str]:
"""Infer relationships between tables based on naming patterns"""
relationships = {}
# Common relationship patterns
fact_patterns = ['fact_', 'f_', 'sales', 'transactions', 'events', 'orders']
dimension_patterns = ['dim_', 'd_', 'customer', 'product', 'location', 'date']
for group_name, files in file_groups.items():
table_names = [f['inferred_table_name'] for f in files]
# Check if it's a fact or dimension table group
if any(pattern in group_name.lower() for pattern in fact_patterns):
relationships[group_name] = 'fact_tables'
elif any(pattern in group_name.lower() for pattern in dimension_patterns):
relationships[group_name] = 'dimension_tables'
else:
relationships[group_name] = 'general_tables'
return relationships
{{ macros.python_cell_with_heading("## üìä Discover Files in Target Folder") }}
print(f"üîç Scanning folder: {scan_folder_path}")
print(f"üìÅ Recursive scan: {recursive_scan}")
print("=" * 60)
# Discover files
discovered_files = discover_files(scan_folder_path, recursive_scan)
# Display summary
total_files = sum(len(files) for files in discovered_files.values())
print(f"‚úÖ Found {total_files} files across {len(discovered_files)} file types")
print("")
print("File Type Summary:")
for file_type, files in discovered_files.items():
print(f"  - {file_type.upper()}: {len(files)} files")
{{ macros.python_cell_with_heading("## üî¨ Analyze File Structures") }}
# Analyze sample files from each group
analyzed_files = {}
for file_type, file_list in discovered_files.items():
print("")
print(f"üìã Analyzing {file_type.upper()} files...")
analyzed_files[file_type] = []
# Analyze all files to properly group by table name and detect date ranges
# For large datasets, we still need to see all files to create proper incremental configs
sample_files = file_list
for file_path in sample_files:
print(f"  - Analyzing: {file_path}")
file_info = analyze_file_structure(file_path)
# Infer table name from file path - extract meaningful table name from folder structure
path_parts = file_path.split('/')
# Look for meaningful table names in the path, skipping date folders and common prefixes
table_name_candidates = []
for part in path_parts:
# Skip date patterns (YYYY, MM, DD, or date-like folders)
if part.isdigit() and (len(part) == 4 or len(part) == 2):
continue
# Skip common folder names and file extensions
if part.lower() in ['csv', 'parquet', 'json', 'avro', 'data', 'files', 'series', 'nested']:
continue
# Skip file names that are just 'data' with extension
if part.startswith('data.'):
continue
# Add potential table name
if '.' not in part or file_info['is_folder']:
table_name_candidates.append(part)
# Use the most specific meaningful part (usually the last meaningful folder)
if table_name_candidates:
# Prefer the last meaningful folder name before date folders
table_name = table_name_candidates[-1]
# Add prefix for synthetic data
if 'synthetic_data' in file_path or any('retail_oltp' in part for part in path_parts):
table_name = f"synth_data_{table_name}"
else:
# Fallback to file name
file_name = path_parts[-1].replace(f'.{file_type}', '')
table_name = file_name
file_info['inferred_table_name'] = table_name.replace('-', '_').replace(' ', '_').lower()
file_info['path_parts'] = path_parts  # Store for date range detection
analyzed_files[file_type].append(file_info)
{{ macros.python_cell_with_heading("## üîß Generate Ingestion Configurations") }}
def generate_config_id(table_name: str, file_type: str) -> str:
"""Generate a unique config ID"""
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
return f"auto_{table_name}_{file_type}_{timestamp}"
def determine_execution_group(file_info: Dict, base_group: int) -> int:
"""Determine execution group based on file characteristics"""
# Assign groups based on dependencies and file types
if 'dim_' in file_info['inferred_table_name'] or 'dimension' in file_info['inferred_table_name']:
return base_group  # Dimensions first
elif 'fact_' in file_info['inferred_table_name']:
return base_group + 10  # Facts after dimensions
elif file_info['is_partitioned']:
return base_group + 20  # Partitioned data later
else:
return base_group + 5  # General tables in between
def generate_ingestion_config(file_info: Dict, config_counter: int) -> Dict:
"""Generate a flat file ingestion configuration from file analysis"""
config = {
"config_id": generate_config_id(file_info['inferred_table_name'], file_info['format']),
"config_name": f"Auto-discovered: {file_info['inferred_table_name']} ({file_info['format']})",
"source_file_path": file_info['path'],
"source_file_format": file_info['format'],
"source_workspace_id": "
{% raw %}
    {{ varlib:config_workspace_id }}
{% endraw %}
",
"source_datastore_id": "
{% raw %}
    {{ varlib:config_lakehouse_id }}
{% endraw %}
",
"source_datastore_type": "lakehouse",
"source_file_root_path": "Files",
"target_workspace_id": "
{% raw %}
    {{ varlib:config_workspace_id }}
{% endraw %}
",
"target_datastore_id": "
{% raw %}
    {{ varlib:config_lakehouse_id }}
{% endraw %}
",
"target_datastore_type": "lakehouse",
"target_schema_name": target_schema,
"target_table_name": file_info['inferred_table_name'],
"staging_table_name": None,
"file_delimiter": file_info.get('delimiter', ',') if file_info['format'] == 'csv' else None,
"has_header": file_info.get('has_header', True) if file_info['format'] == 'csv' else None,
"encoding": file_info.get('encoding', 'utf-8') if file_info['format'] == 'csv' else None,
"date_format": "yyyy-MM-dd",
"timestamp_format": "yyyy-MM-dd HH:mm:ss",
"schema_inference": True,
"custom_schema_json": None,
"partition_columns": "",
"sort_columns": file_info['columns'][0] if file_info.get('columns') else "",
"write_mode": "append" if file_info['is_partitioned'] else "overwrite",
"merge_keys": "",
"data_validation_rules": None,
"error_handling_strategy": "log",
"execution_group": determine_execution_group(file_info, execution_group_start),
"active_yn": "Y",
"created_date": datetime.now().strftime('%Y-%m-%d'),
"modified_date": None,
"created_by": "auto_discovery",
"modified_by": None,
"quote_character": '"' if file_info['format'] == 'csv' else None,
"escape_character": '"' if file_info['format'] == 'csv' else None,
"multiline_values": True if file_info['format'] == 'csv' else None,
"ignore_leading_whitespace": False if file_info['format'] == 'csv' else None,
"ignore_trailing_whitespace": False if file_info['format'] == 'csv' else None,
"null_value": "" if file_info['format'] == 'csv' else None,
"empty_value": "" if file_info['format'] == 'csv' else None,
"comment_character": None,
"max_columns": 100 if file_info['format'] == 'csv' else None,
"max_chars_per_column": 50000 if file_info['format'] == 'csv' else None,
"import_pattern": file_info.get('import_pattern', "date_partitioned" if file_info['is_partitioned'] else "single_file"),
"date_partition_format": file_info.get('date_pattern'),
"table_relationship_group": f"auto_discovered_{datetime.now().strftime('%Y%m%d')}",
"batch_import_enabled": file_info.get('batch_import_enabled', file_info['is_partitioned']),
"file_discovery_pattern": file_info.get('file_discovery_pattern', f"**/*.{file_info['format']}" if file_info['is_partitioned'] else None),
"import_sequence_order": config_counter,
"date_range_start": file_info.get('date_range_start'),
"date_range_end": file_info.get('date_range_end'),
"skip_existing_dates": True if file_info['is_partitioned'] else None,
"source_is_folder": file_info['is_folder']
}
return config
# Group files by table name for incremental configs
grouped_configs = {}
for file_type, file_infos in analyzed_files.items():
for file_info in file_infos:
if 'error' not in file_info:
table_name = file_info['inferred_table_name']
if table_name not in grouped_configs:
grouped_configs[table_name] = {
'base_info': file_info,
'files': [file_info],
'is_partitioned': file_info.get('is_partitioned', False),
'date_ranges': []
}
else:
grouped_configs[table_name]['files'].append(file_info)
if file_info.get('is_partitioned', False):
grouped_configs[table_name]['is_partitioned'] = True
# Collect date components for range detection
if file_info.get('date_components'):
grouped_configs[table_name]['date_ranges'].append(file_info['date_components'])
# Generate configurations - one per table group
all_configs = []
config_counter = 1
for table_name, group_info in grouped_configs.items():
base_file_info = group_info['base_info']
files = group_info['files']
is_partitioned = group_info['is_partitioned']
if is_partitioned and len(files) > 1:
# Create single incremental config for date-partitioned files
# Calculate date range from all files
date_ranges = group_info['date_ranges']
if date_ranges:
years = [d['year'] for d in date_ranges if d['year']]
months = [d['month'] for d in date_ranges if d['month']]
days = [d['day'] for d in date_ranges if d['day']]
# Determine the base path (remove date components)
base_path_parts = base_file_info['path_parts'][:]
# Remove date parts from the end
while base_path_parts and base_path_parts[-1].isdigit():
base_path_parts.pop()
# Remove data file name
if base_path_parts and base_path_parts[-1].startswith('data.'):
base_path_parts.pop()
base_path = '/'.join(base_path_parts)
# Update file info for incremental config
incremental_info = base_file_info.copy()
incremental_info['path'] = base_path
incremental_info['import_pattern'] = 'date_partitioned'
incremental_info['batch_import_enabled'] = True
incremental_info['file_discovery_pattern'] = f"**/*.{base_file_info['format']}"
# Set date ranges
if years:
min_year, max_year = min(years), max(years)
incremental_info['date_range_start'] = f"{min_year}-01-01"
incremental_info['date_range_end'] = f"{max_year}-12-31"
config = generate_ingestion_config(incremental_info, config_counter)
config['config_name'] = f"Auto-discovered incremental: {table_name} ({base_file_info['format']}) - {len(files)} date partitions"
all_configs.append(config)
else:
# Fallback to regular config if no date ranges found
config = generate_ingestion_config(base_file_info, config_counter)
all_configs.append(config)
else:
# Create regular config for non-partitioned or single files
config = generate_ingestion_config(base_file_info, config_counter)
all_configs.append(config)
config_counter += 1
print("")
print(f"‚úÖ Generated {len(all_configs)} ingestion configurations")
{{ macros.python_cell_with_heading("## üíæ Save Configurations") }}
# Import the schema definition
{% include 'ddl/schemas/flat_file_config_schema_universal.py.jinja' %}
# Convert configs to Spark DataFrame
from pyspark.sql import Row
config_rows = []
for config in all_configs:
# Convert dict to Row, ensuring all required fields are present
row_data = {
"config_id": config["config_id"],
"config_name": config["config_name"],
"source_file_path": config["source_file_path"],
"source_file_format": config["source_file_format"],
"source_workspace_id": config.get("source_workspace_id"),
"source_datastore_id": config.get("source_datastore_id"),
"source_datastore_type": config.get("source_datastore_type"),
"source_file_root_path": config.get("source_file_root_path"),
"target_workspace_id": config["target_workspace_id"],
"target_datastore_id": config["target_datastore_id"],
"target_datastore_type": config["target_datastore_type"],
"target_schema_name": config["target_schema_name"],
"target_table_name": config["target_table_name"],
"staging_table_name": config.get("staging_table_name"),
"file_delimiter": config.get("file_delimiter"),
"has_header": config.get("has_header"),
"encoding": config.get("encoding"),
"date_format": config.get("date_format"),
"timestamp_format": config.get("timestamp_format"),
"schema_inference": config["schema_inference"],
"custom_schema_json": config.get("custom_schema_json"),
"partition_columns": config.get("partition_columns", ""),
"sort_columns": config.get("sort_columns", ""),
"write_mode": config["write_mode"],
"merge_keys": config.get("merge_keys", ""),
"data_validation_rules": config.get("data_validation_rules"),
"error_handling_strategy": config["error_handling_strategy"],
"execution_group": config["execution_group"],
"active_yn": config["active_yn"],
"created_date": config["created_date"],
"modified_date": config.get("modified_date"),
"created_by": config["created_by"],
"modified_by": config.get("modified_by"),
"quote_character": config.get("quote_character"),
"escape_character": config.get("escape_character"),
"multiline_values": config.get("multiline_values"),
"ignore_leading_whitespace": config.get("ignore_leading_whitespace"),
"ignore_trailing_whitespace": config.get("ignore_trailing_whitespace"),
"null_value": config.get("null_value"),
"empty_value": config.get("empty_value"),
"comment_character": config.get("comment_character"),
"max_columns": config.get("max_columns"),
"max_chars_per_column": config.get("max_chars_per_column"),
"import_pattern": config.get("import_pattern"),
"date_partition_format": config.get("date_partition_format"),
"table_relationship_group": config.get("table_relationship_group"),
"batch_import_enabled": config.get("batch_import_enabled"),
"file_discovery_pattern": config.get("file_discovery_pattern"),
"import_sequence_order": config.get("import_sequence_order"),
"date_range_start": config.get("date_range_start"),
"date_range_end": config.get("date_range_end"),
"skip_existing_dates": config.get("skip_existing_dates"),
"source_is_folder": config.get("source_is_folder")
}
config_rows.append(Row(**row_data))
# Create DataFrame with the schema - use lakehouse_utils
configs_df = lh_utils.get_connection.createDataFrame(config_rows, schema)
# Option 1: Append to existing config table
print("")
print("üìù Option 1: Append configurations to existing table")
print("To append these configurations, uncomment and run:")
print("# lh_utils.save_dataframe_as_table(configs_df, 'config_flat_file_ingestion', mode='append')")
# Option 2: Save to a temporary table for review
temp_table_name = f"temp_auto_configs_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
lh_utils.save_dataframe_as_table(configs_df, temp_table_name, mode="overwrite")
print("")
print(f"‚úÖ Saved configurations to temporary table: {temp_table_name}")
# Display all configurations
print("")
print("üìã All Generated Configurations:")
configs_df.select("config_id", "config_name", "source_file_path", "target_table_name", "execution_group").show(truncate=False)
{{ macros.python_cell_with_heading("## üìä Configuration Summary Report") }}
# Generate summary report
print("=" * 80)
print("FILE DISCOVERY AND CONFIGURATION GENERATION SUMMARY")
print("=" * 80)
print(f"Scan Path: {scan_folder_path}")
print(f"Total Files Discovered: {total_files}")
print(f"Configurations Generated: {len(all_configs)}")
print(f"Temporary Table: {temp_table_name}")
print("")
print("File Types:")
for file_type, count in [(ft, len(files)) for ft, files in discovered_files.items()]:
print(f"  - {file_type.upper()}: {count} files")
print("")
print("Execution Groups:")
exec_groups = {}
for config in all_configs:
group = config['execution_group']
exec_groups[group] = exec_groups.get(group, 0) + 1
for group in sorted(exec_groups.keys()):
print(f"  - Group {group}: {exec_groups[group]} configurations")
print("")
print("Table Relationship Groups:")
rel_groups = {}
for config in all_configs:
group = config['table_relationship_group']
rel_groups[group] = rel_groups.get(group, 0) + 1
for group, count in rel_groups.items():
print(f"  - {group}: {count} tables")
print("")
print("üìù All Generated Configuration Details:")
print("=" * 120)
for i, config in enumerate(all_configs, 1):
print(f"{i:2d}. Config ID: {config['config_id']}")
print(f"    Name: {config['config_name']}")
print(f"    Source: {config['source_file_path']} ({config['source_file_format']})")
print(f"    Target: {config['target_schema_name']}.{config['target_table_name']}")
print(f"    Execution Group: {config['execution_group']}")
print(f"    Write Mode: {config['write_mode']}")
print(f"    Import Pattern: {config['import_pattern']}")
if config.get('date_range_start') and config.get('date_range_end'):
print(f"    Date Range: {config['date_range_start']} to {config['date_range_end']}")
if config.get('file_discovery_pattern'):
print(f"    Discovery Pattern: {config['file_discovery_pattern']}")
if config.get('file_delimiter'):
print(f"    CSV Settings: delimiter='{config['file_delimiter']}', header={config['has_header']}")
print("")
print("‚úÖ Configuration generation complete!")
print(f"Review the configurations in table: {temp_table_name}")
print("Once reviewed, append to config_flat_file_ingestion table to activate.")
{{ macros.exit_notebook("success") }}
{% include 'shared/notebook/cells/footer.py.jinja' %}
