{%- import 'shared/notebook/macros/notebook_macros.py.jinja' as macros -%}
{% if language_group == "synapse_pyspark" %}
{%- include "shared/notebook/headers/pyspark.py.jinja" %}
{% else %}
{%- include "shared/notebook/headers/python.py.jinja" %}
{% endif %}

{{ macros.parameters_cell() }}

# Default parameters
config_id = ""
execution_group = None
environment = "development"

{% if datastore_type == "lakehouse" %}
{{ macros.pyspark_cell_with_heading("## ðŸ“„ Flat File Ingestion Notebook (Lakehouse)") }}
{% else %}
{{ macros.python_cell_with_heading("## ðŸ“„ Flat File Ingestion Notebook (Warehouse)") }}
{% endif %}

# This notebook processes flat files (CSV, JSON, Parquet, Avro, XML) and loads them into {{ datastore_type }} tables based on configuration metadata.
# Uses modularized components from python_libs for maintainable and reusable code.

{% set runtime_type = runtime_type %}
{% set language_group = language_group %}
{% set include_ddl_utils = include_ddl_utils %}
{% include 'shared/notebook/environment/library_loader.py.jinja' %}

{% if datastore_type == "lakehouse" %}
{{ macros.pyspark_cell_with_heading("## ðŸ”§ Load Configuration and Initialize") }}
{% else %}
{{ macros.python_cell_with_heading("## ðŸ”§ Load Configuration and Initialize") }}
{% endif %}
{% include 'shared/notebook/environment/config_loader.py.jinja' %}

# Additional imports for flat file ingestion
import uuid
import json
import time
from datetime import datetime
from typing import Dict, List, Optional, Any

{% block datastore_specific_imports %}
# Datastore-specific imports will be defined in child templates
{% endblock %}

execution_id = str(uuid.uuid4())

print(f"Execution ID: {execution_id}")
print(f"Config ID: {config_id}")
print(f"Execution Group: {execution_group}")
print(f"Environment: {environment}")

{% if datastore_type == "lakehouse" %}
{{ macros.pyspark_cell_with_heading("## ðŸ“‹ Load Configuration Data") }}
{% else %}
{{ macros.python_cell_with_heading("## ðŸ“‹ Load Configuration Data") }}
{% endif %}

{% block configuration_setup %}
# Configuration setup will be defined in child templates
{% endblock %}

# Load configuration
{% block load_configuration %}
# Configuration loading will be defined in child templates
{% endblock %}

{% if add_debug_cells %}
{% if datastore_type == "lakehouse" %}
{{ macros.pyspark_cell_with_heading("## ðŸ› Debug Configuration Override") }}
{% else %}
{{ macros.python_cell_with_heading("## ðŸ› Debug Configuration Override") }}
{% endif %}

# Debug mode - override configurations with embedded test data
debug_mode = True  # Set to False to use normal database configurations

if debug_mode:
    import pandas as pd
    from datetime import datetime
    
    # Define debug configurations directly in the notebook
    debug_configs = [
        {
            "config_id": "debug_test_001",
            "source_file_path": "test_data/sample.csv",
            "source_file_format": "csv",
            "target_workspace_id": configs.{% if datastore_type == "lakehouse" %}raw_lh_workspace_id{% else %}raw_wh_workspace_id{% endif %},
            "target_datastore_id": configs.{% if datastore_type == "lakehouse" %}raw_lh_lakehouse_id{% else %}raw_wh_warehouse_id{% endif %},
            "target_datastore_type": "{{ datastore_type }}",
            "target_schema_name": "debug",
            "target_table_name": "debug_test_table",
            "staging_table_name": None,
            "file_delimiter": ",",
            "has_header": True,
            "encoding": "utf-8",
            "date_format": "yyyy-MM-dd",
            "timestamp_format": "yyyy-MM-dd HH:mm:ss",
            "schema_inference": True,
            "custom_schema_json": None,
            "partition_columns": "",
            "sort_columns": "",
            "write_mode": "overwrite",
            "merge_keys": "",
            "execution_group": 1,
            "active_yn": "Y",
            "created_date": datetime.now().strftime("%Y-%m-%d"),
            "modified_date": None,
            "created_by": "debug_user",
            "modified_by": None,
            "quote_character": '"',
            "escape_character": '"',
            "multiline_values": True,
            "ignore_leading_whitespace": False,
            "ignore_trailing_whitespace": False,
            "null_value": "",
            "empty_value": "",
            "comment_character": None,
            "max_columns": 100,
            "max_chars_per_column": 50000,
            "import_pattern": "single_file",
            "date_partition_format": None,
            "table_relationship_group": None,
            "batch_import_enabled": False,
            "file_discovery_pattern": None,
            "import_sequence_order": 1,
            "date_range_start": None,
            "date_range_end": None,
            "skip_existing_dates": None,
            "source_is_folder": False
        }
    ]
    
    # Override config_df with debug configurations
    config_df = pd.DataFrame(debug_configs)
    print("ðŸ› DEBUG MODE ACTIVE - Using embedded test configurations")
    print(f"Debug configurations loaded: {len(config_df)} items")
    
    # Display debug configurations
    display(config_df[["config_id", "source_file_path", "target_table_name"]])
else:
    print("ðŸ“‹ Using standard database configurations")
{% endif %}

# Filter configurations
if config_id:
    config_df = config_df[config_df["config_id"] == config_id]
else:
    # If execution_group is not set or is empty, process all execution groups
    if execution_group and str(execution_group).strip():
        config_df = config_df[
            (config_df["execution_group"] == execution_group) & 
            (config_df["active_yn"] == "Y")
        ]
    else:
        config_df = config_df[config_df["active_yn"] == "Y"]

if config_df.empty:
    raise ValueError(f"No active configurations found for config_id: {config_id}, execution_group: {execution_group}")

print(f"Found {len(config_df)} configurations to process")

{% if add_debug_cells %}
{% if datastore_type == "lakehouse" %}
{{ macros.pyspark_cell_with_heading("## ðŸ§ª Create Debug Test Data") }}
{% else %}
{{ macros.python_cell_with_heading("## ðŸ§ª Create Debug Test Data") }}
{% endif %}

# Create test data files for debug mode
if debug_mode:
    import os
    from pyspark.sql import Row
    
    # Create test data directory
    test_data_dir = "test_data"
    
    # Create sample CSV data
    sample_data = [
        Row(id=1, name="Test User 1", email="test1@example.com", created_date="2024-01-01"),
        Row(id=2, name="Test User 2", email="test2@example.com", created_date="2024-01-02"),
        Row(id=3, name="Test User 3", email="test3@example.com", created_date="2024-01-03"),
    ]
    
    # Create DataFrame and write to test location
    test_df = spark.createDataFrame(sample_data)
    
    {% if datastore_type == "lakehouse" %}
    # For lakehouse, write to the raw lakehouse Files location
    raw_lakehouse.write_file(
        df=test_df,
        file_path=test_data_dir + "/sample.csv",
        file_format="csv",
        options={"header": True}
    )
    print(f"âœ… Created test CSV file at: {test_data_dir}/sample.csv")
    
    # Also create a JSON test file
    json_data = [
        Row(product_id=101, product_name="Widget A", price=19.99, category="Electronics"),
        Row(product_id=102, product_name="Widget B", price=29.99, category="Electronics"),
        Row(product_id=103, product_name="Gadget X", price=39.99, category="Accessories"),
    ]
    json_df = spark.createDataFrame(json_data)
    raw_lakehouse.write_file(
        df=json_df,
        file_path=test_data_dir + "/products.json",
        file_format="json"
    )
    print(f"âœ… Created test JSON file at: {test_data_dir}/products.json")
    {% else %}
    # For warehouse, write to staging location
    staging_path = f"/lakehouse/default/{test_data_dir}/sample.csv"
    test_df.coalesce(1).write.mode("overwrite").option("header", True).csv(staging_path)
    print(f"âœ… Created test CSV file at: {staging_path}")
    {% endif %}
    
    # Display sample data
    print("\nðŸ“„ Sample test data:")
    test_df.show()
    
    # Update debug configurations to use multiple test files
    if len(debug_configs) == 1:
        # Add a second configuration for JSON file
        debug_configs.append({
            **debug_configs[0],  # Copy all fields from first config
            "config_id": "debug_test_002",
            "source_file_path": "test_data/products.json",
            "source_file_format": "json",
            "target_table_name": "debug_products_table",
            "file_delimiter": None,
            "has_header": None,
        })
        # Re-create config_df with updated configurations
        config_df = pd.DataFrame(debug_configs)
        print(f"\nðŸ”„ Updated debug configurations: {len(config_df)} items")
{% endif %}

{% if datastore_type == "lakehouse" %}
{{ macros.pyspark_cell_with_heading("## ðŸš€ Initialize Modular Services") }}
{% else %}
{{ macros.python_cell_with_heading("## ðŸš€ Initialize Modular Services") }}
{% endif %}

{% block initialize_services %}
# Service initialization will be defined in child templates
{% endblock %}

{% if datastore_type == "lakehouse" %}
{{ macros.pyspark_cell_with_heading("## ðŸ“Š Process Configurations") }}
{% else %}
{{ macros.python_cell_with_heading("## ðŸ“Š Process Configurations") }}
{% endif %}

# Convert pandas DataFrame rows to FlatFileIngestionConfig objects
configurations = []
for _, config_row in config_df.iterrows():
    config = FlatFileIngestionConfig.from_dict(config_row.to_dict())
    configurations.append(config)

# Process all configurations using the orchestrator
results = orchestrator.process_configurations(configurations, execution_id)

{% if datastore_type == "lakehouse" %}
{{ macros.pyspark_cell_with_heading("## âœ… Exit Notebook") }}
{% else %}
{{ macros.python_cell_with_heading("## âœ… Exit Notebook") }}
{% endif %}

# Determine exit status based on results
if results['failed'] > 0:
    exit_message = f"failed - {results['failed']} configuration(s) failed"
    {{ macros.exit_notebook("failed") }}
else:
    exit_message = "success"
    {{ macros.exit_notebook("success") }}

{% include 'shared/notebook/cells/footer.py.jinja' %}