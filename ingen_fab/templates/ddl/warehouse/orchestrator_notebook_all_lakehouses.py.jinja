{%- import 'shared/notebook/macros/notebook_macros.py.jinja' as macros -%}
{% if language_group == "synapse_pyspark" %}
{%- include "shared/notebook/headers/pyspark.py.jinja" %}
{% else %}
{%- include "shared/notebook/headers/python.py.jinja" %}
{% endif %}

{% include "shared/notebook/environment/library_loader.py.jinja" %}


{{macros.python_cell_with_heading("## 🗂️ Now Load the Custom Python Libraries")}}

if run_mode == "local":
    from ingen_fab.python_libs.common.config_utils import *
    from ingen_fab.python_libs.python.lakehouse_utils import lakehouse_utils
    from ingen_fab.python_libs.python.ddl_utils import ddl_utils
    from ingen_fab.python_libs.python.notebook_utils_abstraction import NotebookUtilsFactory
    from ingen_fab.python_libs.python.sql_templates import SQLTemplates
    from ingen_fab.python_libs.python.warehouse_utils import warehouse_utils
    from ingen_fab.python_libs.python.pipeline_utils import PipelineUtils
    notebookutils = NotebookUtilsFactory.create_instance() 
else:
    files_to_load = [
        "ingen_fab/python_libs/common/config_utils.py",
        "ingen_fab/python_libs/python/lakehouse_utils.py",
        "ingen_fab/python_libs/python/ddl_utils.py",
        "ingen_fab/python_libs/python/notebook_utils_abstraction.py",
        "ingen_fab/python_libs/python/sql_templates.py",
        "ingen_fab/python_libs/python/warehouse_utils.py",
        "ingen_fab/python_libs/python/pipeline_utils.py"
    ]

    load_python_modules_from_path(mount_path, files_to_load)
{{ macros.python_cell_with_heading("## 🆕 Instantiate Required Classes ")}}


configs: ConfigsObject = get_configs_as_object()


{{macros.python_cell_with_heading("## 🏃‍♂️‍➡️ Run All Lakehouse Orchestrators in Parallel")}}


# Define the lakehouses and their orchestrators
lakehouses_to_run = [
{%- for lakehouse in lakehouses %}
    {'name': '{{ lakehouse.name }}', 'orchestrator': '{{ lakehouse.orchestrator_name }}'},
{%- endfor %}
]


# Import required libraries
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
import time

# Initialize variables
start_time = datetime.now()
results = {}

print(f"Starting parallel orchestration for all lakehouses")
print(f"Start time: {start_time}")

print(f"Total lakehouses to process: {{ total_lakehouses }}")
print("="*60)


# Define function to run a single lakehouse orchestrator
def run_lakehouse_orchestrator(lakehouse_name, orchestrator_name, stagger_delay=0):
    """Run a single lakehouse orchestrator and return the result."""
    result = {
        'lakehouse': lakehouse_name,
        'orchestrator': orchestrator_name,
        'orchestrator_path': f"ddl_scripts/Warehouses/{lakehouse_name}/{orchestrator_name}.Notebook/notebook-content.py",
        'start_time': datetime.now(),
        'end_time': None,
        'duration': None,
        'status': 'Running',
        'error': None,
        'exit_value': None
    }
    
    try:
        # Add staggered delay to avoid concurrent update issues
        if stagger_delay > 0:
            print(f"[{result['start_time']}] Waiting {stagger_delay:.1f}s before starting {lakehouse_name} (staggered execution)")
            time.sleep(stagger_delay)
            
        print(f"[{result['start_time']}] Starting orchestrator for {lakehouse_name}")
        
        params = {}

        # Run the lakehouse orchestrator
        notebook_result = notebookutils.mssparkutils.notebook.run(
            f"{orchestrator_name}",
            timeout=7200,  # 2 hour timeout per lakehouse
            params=params
        )
        
        result['end_time'] = datetime.now()
        result['duration'] = result['end_time'] - result['start_time']
        if notebook_result == 'success':
            result['status'] = 'Success'
        else:
            raise Exception(f"Notebook returned unexpected result: {notebook_result}")
        
        print(f"[{result['end_time']}] ✓ Completed {lakehouse_name} in {result['duration']}")
        
    except Exception as e:
        result['end_time'] = datetime.now()
        result['duration'] = result['end_time'] - result['start_time']
        result['status'] = 'Failed'
        result['error'] = str(e)
        
        print(f"[{result['end_time']}] ✗ Failed {lakehouse_name} after {result['duration']}")
        print(f"  Error: {result['error']}")
    
    return result


# Execute all lakehouse orchestrators in parallel
print("\nStarting parallel execution of lakehouse orchestrators...")
print("="*60)


# Run orchestrators in parallel using ThreadPoolExecutor with staggered execution
with ThreadPoolExecutor(max_workers={{ total_lakehouses }}) as executor:
    # Submit all tasks with staggered delays (0.2 seconds apart)
    future_to_lakehouse = {}
    for i, lakehouse in enumerate(lakehouses_to_run):
        stagger_delay = i * 0.2  # 0.2-second stagger between executions
        future = executor.submit(
            run_lakehouse_orchestrator, 
            lakehouse['name'], 
            lakehouse['orchestrator'],
            stagger_delay
        )
        future_to_lakehouse[future] = lakehouse['name']
    
    # Process completed tasks as they finish
    for future in as_completed(future_to_lakehouse):
        lakehouse_name = future_to_lakehouse[future]
        try:
            result = future.result()
            results[lakehouse_name] = result
        except Exception as exc:
            print(f'Lakehouse {lakehouse_name} generated an exception: {exc}')
            results[lakehouse_name] = {
                'lakehouse': lakehouse_name,
                'status': 'Exception',
                'error': str(exc)
            }


# Generate detailed summary report
end_time = datetime.now()
total_duration = end_time - start_time

print("\n" + "="*60)
print("ORCHESTRATION SUMMARY REPORT")
print("="*60)
print(f"Total execution time: {total_duration}")
print(f"Total lakehouses: {{ total_lakehouses }}")

# Count results
success_count = sum(1 for r in results.values() if r['status'] == 'Success')
failed_count = sum(1 for r in results.values() if r['status'] == 'Failed')
exception_count = sum(1 for r in results.values() if r['status'] == 'Exception')

print(f"Successful: {success_count}")
print(f"Failed: {failed_count}")
print(f"Exceptions: {exception_count}")
print("\n" + "-"*60)


# Detailed results for each lakehouse
print("\nDETAILED RESULTS BY LAKEHOUSE:")
print("="*60)

for lakehouse_name in sorted(results.keys()):
    result = results[lakehouse_name]
    print(f"\n{lakehouse_name}:")
    print(f"  Status: {result['status']}")
    
    if 'duration' in result and result['duration']:
        print(f"  Duration: {result['duration']}")
    
    if result['status'] == 'Success' and 'exit_value' in result:
        print(f"  Exit value: {result['exit_value']}")
    elif result['status'] in ['Failed', 'Exception'] and 'error' in result:
        print(f"  Error: {result['error']}")


# Create a summary table using markdown
summary_data = []
for lakehouse_name in sorted(results.keys()):
    result = results[lakehouse_name]
    status_icon = "✓" if result['status'] == 'Success' else "✗"
    duration_str = str(result.get('duration', 'N/A'))
    orchestrator_path = result.get('orchestrator_path', 'N/A')
    summary_data.append(f"| {lakehouse_name} | {status_icon} {result['status']} | {duration_str} | {orchestrator_path} |")

markdown_table = f"""
## Execution Summary Table

| Lakehouse | Status | Duration | Orchestrator Path |
|-----------|--------|----------|-------------------|
{chr(10).join(summary_data)}

**Total Execution Time:** {total_duration}
"""

print(markdown_table)


# Final status and exit
if failed_count == 0 and exception_count == 0:
    final_message = f"✓ All {success_count} lakehouses processed successfully!"
    print(f"\n{final_message}")
    notebookutils.notebook.exit(final_message)
else:
    final_message = f"Completed with {failed_count + exception_count} failures out of {{ total_lakehouses }} lakehouses"
    print(f"\n✗ {final_message}")
    notebookutils.notebook.exit(final_message)
    raise Exception(final_message)

{%include "shared/notebook/cells/footer.py.jinja" %}